# Combined Python Codebase
# Generated on: 2025-04-07 11:34:34.377541
# Source directory: /home/admin/Plus91Backoffice/plus91_management


================================================================================
# File: codebase.py
================================================================================

import os
from pathlib import Path
import datetime

def combine_python_files(start_path: str, output_file: str) -> None:
    """
    Recursively find all Python files in the given directory and combine them into a single text file.
    Each file's content will be preceded by its path as a header.
    
    Args:
        start_path: The directory to start searching from
        output_file: The name of the output file
    """
    # Convert to absolute path
    start_path = os.path.abspath(start_path)
    
    # Initialize counter for files processed
    files_processed = 0
    
    try:
        with open(output_file, 'w', encoding='utf-8') as outfile:
            # Write header with timestamp
            outfile.write(f"# Combined Python Codebase\n")
            outfile.write(f"# Generated on: {datetime.datetime.now()}\n")
            outfile.write(f"# Source directory: {start_path}\n\n")
            
            # Walk through directory
            for root, dirs, files in os.walk(start_path):
                # Skip __pycache__ and virtual environment directories
                dirs[:] = [d for d in dirs if d not in ['__pycache__', 'venv', '.venv', '.git']]
                
                # Process Python files
                for file in files:
                    if file.endswith('.py'):
                        file_path = os.path.join(root, file)
                        relative_path = os.path.relpath(file_path, start_path)
                        
                        try:
                            with open(file_path, 'r', encoding='utf-8') as infile:
                                # Write file header
                                outfile.write(f"\n{'='*80}\n")
                                outfile.write(f"# File: {relative_path}\n")
                                outfile.write(f"{'='*80}\n\n")
                                
                                # Write file content
                                outfile.write(infile.read())
                                outfile.write("\n")
                                
                                files_processed += 1
                                
                        except Exception as e:
                            outfile.write(f"# Error reading file {relative_path}: {str(e)}\n")
            
            # Write summary at the end
            outfile.write(f"\n{'='*80}\n")
            outfile.write(f"# Summary: Processed {files_processed} Python files\n")
            outfile.write(f"# End of combined codebase\n")
            
        print(f"Successfully combined {files_processed} Python files into {output_file}")
        print(f"Output file size: {os.path.getsize(output_file) / 1024:.2f} KB")
        
    except Exception as e:
        print(f"Error writing to output file: {str(e)}")

if __name__ == "__main__":
    current_dir = os.getcwd()
    current_dir = "/home/admin/Plus91Backoffice/plus91_management"
    output_file = "combined_codebase.txt"
    
    combine_python_files(current_dir, output_file)


================================================================================
# File: app/config.py
================================================================================

from pydantic_settings import BaseSettings


class Settings(BaseSettings):
    DATABASE_URL: str
    SHAREPRO_WIZZER_API_KEY: str

    class Config:
        env_file = "/home/admin/Plus91Backoffice/plus91_management/.env"

settings = Settings()


================================================================================
# File: app/database.py
================================================================================

from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from .config import settings

DATABASE_URL = settings.DATABASE_URL

async_engine = create_async_engine(DATABASE_URL, echo=True)

AsyncSessionLocal = sessionmaker(
    bind=async_engine,
    class_=AsyncSession,
    expire_on_commit=False
)

async def get_db():
    async with AsyncSessionLocal() as session:
        yield session

================================================================================
# File: app/logger.py
================================================================================

import logging
import watchtower
import boto3
from functools import wraps

boto3.setup_default_session(region_name='ap-south-1')

logger = logging.getLogger("plus91_backend_ops")
logger.setLevel(logging.DEBUG)

handler = watchtower.CloudWatchLogHandler(
    log_group='PortfolioManagerLogs',
    stream_name='BackendStream'
)
handler.setFormatter(logging.Formatter(
    "%(asctime)s - %(levelname)s - %(filename)s - %(funcName)s - %(message)s"
))
logger.addHandler(handler)

def log_function_call(func):
    @wraps(func)
    async def wrapper(*args, **kwargs):
        logger.info(
            f"Function '{func.__name__}' called with args: {args}, kwargs: {kwargs}"
        )
        try:
            result = await func(*args, **kwargs)
            logger.info(f"Function '{func.__name__}' completed successfully")
            return result
        except Exception as e:
            logger.error(
                f"Function '{func.__name__}' failed with error: {str(e)}", exc_info=True
            )
            raise
    return wrapper

================================================================================
# File: app/__init__.py
================================================================================



================================================================================
# File: app/main.py
================================================================================

from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from app.routers.clients_router import (
    client_router, distributor_router, broker_router
)
from app.routers.accounts_router import (
    account_router, joint_account_router
)
from app.routers.portfolios_router import portfolio_router
from app.routers.report_router import report_router
from app.routers.accounts_data_router import accounts_data_router
import uvicorn
import logging
from app.logger import logger


app = FastAPI(
    title="Plus91 Client Management System API",
    description="API for managing clients, brokers, distributors, and types.",
    version="1.0.0"
)

@app.middleware("http")
async def log_requests(request: Request, call_next):
    logger.info(
        f"Incoming request: {request.method} {request.url} - Params: {request.query_params} - Body: {await request.body()}"
    )
    try:
        response = await call_next(request)
        logger.info(f"Request completed: {request.method} {request.url} - Status: {response.status_code}")
        return response
    except Exception as e:
        logger.error(f"Request failed: {request.method} {request.url} - Error: {str(e)}", exc_info=True)
        raise

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.include_router(portfolio_router)
app.include_router(broker_router)
app.include_router(distributor_router)
app.include_router(client_router)
app.include_router(joint_account_router)
app.include_router(account_router)
app.include_router(report_router)
app.include_router(accounts_data_router)


@app.get("/", summary="Root Endpoint")
async def root():
    return {"message": "Plus91 Client Management System API is running."}

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    uvicorn.run("app.main:app", host="0.0.0.0", port=8000, reload=True)


================================================================================
# File: app/models/non_tradable_logs.py
================================================================================

from sqlalchemy import Column, Integer, String, Text, Date, TIMESTAMP, func
from app.models.base import Base

class NonTradableLog(Base):
    __tablename__ = "non_tradable_logs"

    id = Column(Integer, primary_key=True)
    account_id = Column(String, nullable=False)
    trading_symbol = Column(String, nullable=False)
    reason = Column(Text)
    event_date = Column(Date, nullable=False)
    created_at = Column(TIMESTAMP, nullable=False, server_default=func.now())

    def __repr__(self):
        return f"<NonTradableLog(id={self.id}, account_id={self.account_id}, trading_symbol={self.trading_symbol})>"

================================================================================
# File: app/models/base.py
================================================================================

from sqlalchemy.orm import declarative_base

Base = declarative_base()

================================================================================
# File: app/models/report.py
================================================================================

from pydantic import BaseModel


class RequestData(BaseModel):
    broker_code: str
    pan_no: str

================================================================================
# File: app/models/__init__.py
================================================================================

from .accounts import (
    AccountActualPortfolio,
    AccountActualPortfolioException,
    AccountBracketBasketAllocation,
    AccountCashflow,
    AccountCashflowProgression,
    AccountIdealPortfolio,
    AccountPerformance,
    AccountTimePeriods,
    JointAccount,
    JointAccountMapping,
    SingleAccount,
)

from .clients import (
    Broker,
    Client,
    Distributor,
)

from .portfolio import (
    Basket,
    BasketStockMapping,
    Bracket,
    PfBracketBasketAllocation,
    PortfolioBasketMapping,
    PortfolioTemplate,
)

from .non_tradable_logs import NonTradableLog
from .report import RequestData
from .stock_exceptions import StockException
from .stock_ltps import StockLTP

================================================================================
# File: app/models/stock_ltps.py
================================================================================

from sqlalchemy import Column, Integer, String, Float, TIMESTAMP, func
from app.models.base import Base

class StockLTP(Base):
    __tablename__ = "stock_ltps"

    id = Column(Integer, primary_key=True)
    trading_symbol = Column(String, unique=True, nullable=False)
    ltp = Column(Float, nullable=False)
    updated_at = Column(TIMESTAMP, nullable=True, onupdate=func.now())

    def __repr__(self):
        return f"<StockLTP(id={self.id}, trading_symbol={self.trading_symbol}, ltp={self.ltp})>"

================================================================================
# File: app/models/stock_exceptions.py
================================================================================

from sqlalchemy import Column, Integer, String, TIMESTAMP, func
from app.models.base import Base

class StockException(Base):
    __tablename__ = "stock_exceptions"

    id = Column(Integer, primary_key=True)
    account_id = Column(String, nullable=False)
    trading_symbol = Column(String, nullable=False)
    created_at = Column(TIMESTAMP, nullable=False, server_default=func.now())
    updated_at = Column(TIMESTAMP, nullable=True, onupdate=func.now())

    def __repr__(self):
        return f"<StockException(id={self.id}, account_id={self.account_id}, trading_symbol={self.trading_symbol})>"

================================================================================
# File: app/models/portfolio/basket_stock_mapping.py
================================================================================

from sqlalchemy import Column, Integer, Text, Float, TIMESTAMP, ForeignKey
from sqlalchemy.sql import func
from sqlalchemy.orm import relationship
from app.models.base import Base


class BasketStockMapping(Base):
    __tablename__ = "basket_stock_mapping"

    basket_stock_mapping_id = Column(Integer, primary_key=True, autoincrement=True)
    basket_id = Column(Integer, ForeignKey("basket_details.basket_id", ondelete="CASCADE"), nullable=False)
    trading_symbol = Column(Text, nullable=False)
    multiplier = Column(Float, nullable=False)

    basket = relationship("Basket", back_populates="stock_mappings")

    def __repr__(self):
        return f"<BasketStockMapping(id={self.basket_stock_mapping_id}, symbol={self.trading_symbol})>"


================================================================================
# File: app/models/portfolio/portfolio_template_details.py
================================================================================

from sqlalchemy import Column, Integer, Text, TIMESTAMP, func
from sqlalchemy.orm import relationship
from app.models.base import Base

class PortfolioTemplate(Base):
    __tablename__ = "portfolio_template_details"

    portfolio_id = Column(Integer, primary_key=True, autoincrement=True)
    portfolio_name = Column(Text, nullable=False)
    description = Column(Text, nullable=True)
    created_at = Column(TIMESTAMP, nullable=False, server_default=func.now())

    accounts = relationship("SingleAccount", back_populates="portfolio_template")
    joint_accounts = relationship("JointAccount", back_populates="portfolio_template")
    pf_bracket_basket_allocations = relationship("PfBracketBasketAllocation", back_populates="portfolio_template")
    basket_mappings = relationship("PortfolioBasketMapping", back_populates="portfolio", cascade="all, delete-orphan")

    def __repr__(self):
        return f"<PortfolioTemplate(id={self.portfolio_id}, name={self.portfolio_name})>"

================================================================================
# File: app/models/portfolio/portfolio_basket_mapping.py
================================================================================

from sqlalchemy import Column, Integer, ForeignKey, Float, TIMESTAMP, func
from sqlalchemy.orm import relationship
from app.models.base import Base

class PortfolioBasketMapping(Base):
    __tablename__ = "portfolio_basket_mapping"

    portfolio_basket_mapping_id = Column(Integer, primary_key=True, autoincrement=True)
    portfolio_id = Column(Integer, ForeignKey("portfolio_template_details.portfolio_id", ondelete="CASCADE"), nullable=False)
    basket_id = Column(Integer, ForeignKey("basket_details.basket_id", ondelete="CASCADE"), nullable=False)
    allocation_pct = Column(Float, nullable=True)

    portfolio = relationship("PortfolioTemplate", back_populates="basket_mappings")
    basket = relationship("Basket", back_populates="portfolio_mappings")

    def __repr__(self):
        return f"<PortfolioBasketMapping(id={self.portfolio_basket_mapping_id}, portfolio_id={self.portfolio_id}, basket_id={self.basket_id})>"

================================================================================
# File: app/models/portfolio/__init__.py
================================================================================

from .basket_details import Basket
from .basket_stock_mapping import BasketStockMapping
from .bracket_details import Bracket
from .pf_bracket_basket_allocation import PfBracketBasketAllocation
from .portfolio_basket_mapping import PortfolioBasketMapping
from .portfolio_template_details import PortfolioTemplate

================================================================================
# File: app/models/portfolio/bracket_details.py
================================================================================

from sqlalchemy import Column, Integer, Float, Text, TIMESTAMP, func
from sqlalchemy.orm import relationship
from app.models.base import Base


class Bracket(Base):
    __tablename__ = "bracket_details"

    bracket_id = Column(Integer, primary_key=True, autoincrement=True)
    bracket_min = Column(Float, nullable=False)
    bracket_max = Column(Float, nullable=False)
    bracket_name = Column(Text, nullable=False)
    created_at = Column(TIMESTAMP, nullable=False, server_default=func.now())

    accounts = relationship("SingleAccount", back_populates="bracket")
    joint_accounts = relationship("JointAccount", back_populates="bracket")
    pf_bracket_basket_allocations = relationship("PfBracketBasketAllocation", back_populates="bracket", cascade="all, delete-orphan")

    def __repr__(self):
        return f"<Bracket(id={self.bracket_id}, name={self.bracket_name})>"

================================================================================
# File: app/models/portfolio/basket_details.py
================================================================================

from sqlalchemy import Column, Integer, Text, TIMESTAMP, CheckConstraint
from sqlalchemy.sql import func
from sqlalchemy.orm import relationship
from app.models.base import Base

class Basket(Base):
    __tablename__ = "basket_details"

    basket_id = Column(Integer, primary_key=True)
    basket_name = Column(Text, nullable=False)
    allocation_method = Column(Text, nullable=False)
    created_at = Column(TIMESTAMP, nullable=False, server_default=func.now())
    updated_at = Column(TIMESTAMP, nullable=True, onupdate=func.now())

    __table_args__ = (
        CheckConstraint(
            "allocation_method IN ('equal','manual')",
            name="basket_allocation_method_check"
        ),
    )

    stock_mappings = relationship("BasketStockMapping", back_populates="basket", cascade="all, delete-orphan")
    pf_bracket_basket_allocations = relationship("PfBracketBasketAllocation", back_populates="basket")
    portfolio_mappings = relationship("PortfolioBasketMapping", back_populates="basket", cascade="all, delete-orphan")

    def __repr__(self):
        return f"<Basket(basket_id={self.basket_id}, name={self.basket_name})>"

================================================================================
# File: app/models/portfolio/pf_bracket_basket_allocation.py
================================================================================

from sqlalchemy import Column, Integer, Float, TIMESTAMP, func, ForeignKey, CheckConstraint
from sqlalchemy.orm import relationship
from app.models.base import Base

class PfBracketBasketAllocation(Base):
    __tablename__ = "pf_bracket_basket_allocation"

    allocation_id = Column(Integer, primary_key=True, autoincrement=True)
    bracket_id = Column(Integer, ForeignKey("bracket_details.bracket_id"), nullable=True)
    basket_id = Column(Integer, ForeignKey("basket_details.basket_id"), nullable=True)
    portfolio_id = Column(Integer, ForeignKey("portfolio_template_details.portfolio_id"), nullable=True)
    allocation_pct = Column(Float, nullable=False, default=0)
    created_at = Column(TIMESTAMP, nullable=False, server_default=func.now())

    __table_args__ = (
        CheckConstraint("allocation_pct >= 0", name="allocation_pct_positive"),
    )

    bracket = relationship("Bracket", back_populates="pf_bracket_basket_allocations")
    basket = relationship("Basket", back_populates="pf_bracket_basket_allocations")
    portfolio_template = relationship("PortfolioTemplate", back_populates="pf_bracket_basket_allocations")

    def __repr__(self):
        return f"<PfBracketBasketAllocation(id={self.allocation_id}, allocation_pct={self.allocation_pct})>"

================================================================================
# File: app/models/clients/broker_details.py
================================================================================

from sqlalchemy import Column, String, Text, TIMESTAMP, func, text
from sqlalchemy.orm import relationship
from app.models.base import Base


class Broker(Base):
    __tablename__ = "broker_details"

    broker_id = Column(
        String,
        primary_key=True,
        server_default=text("CONCAT('BROKER_', LPAD(NEXTVAL('broker_seq')::TEXT, 4, '0'))")
    )
    broker_name = Column(Text, nullable=False)
    created_at = Column(TIMESTAMP, nullable=False, server_default=func.now())

    clients = relationship("Client", back_populates="broker")

    def __repr__(self):
        return f"<Broker(id={self.broker_id}, name={self.broker_name})>"



================================================================================
# File: app/models/clients/client_details.py
================================================================================

from sqlalchemy import (
    Column, String, Text, TIMESTAMP, ForeignKey, UniqueConstraint, text   
)
from sqlalchemy.orm import relationship
from sqlalchemy.sql import func
from app.models.base import Base

class Client(Base):
    __tablename__ = "client_details"

    client_id = Column(
        String,
        primary_key=True,
        server_default=text("CONCAT('CLIENT_', LPAD(NEXTVAL('client_seq')::TEXT, 6, '0'))")
    )
    account_id = Column(String, ForeignKey("single_account.single_account_id", ondelete="SET NULL"), nullable=True)
    client_name = Column(Text, nullable=False)
    broker_id = Column(String, ForeignKey("broker_details.broker_id", ondelete="SET NULL"), nullable=True)
    broker_code = Column(Text, nullable=True)
    broker_passwd = Column(Text, nullable=True)
    pan_no = Column(String, nullable=False)
    phone_no = Column(Text, nullable=True)
    country_code = Column(Text, nullable=True)
    email_id = Column(Text, nullable=True)
    addr = Column(Text, nullable=True)
    acc_start_date = Column(String, nullable=True)
    distributor_id = Column(String, ForeignKey("distributor_details.distributor_id", ondelete="SET NULL"), nullable=True)
    type = Column(Text, nullable=True)
    alias_name = Column(Text, nullable=True)
    alias_phone_no = Column(String, nullable=True)
    alias_addr = Column(Text, nullable=True)
    onboard_status = Column(Text, default="pending")
    created_at = Column(TIMESTAMP, nullable=False, server_default=func.now())
    updated_at = Column(TIMESTAMP, nullable=True, onupdate=func.now()) 

    __table_args__ = (
        UniqueConstraint('broker_id', 'pan_no', name='unique_broker_id_pan'),
    )

    distributor = relationship("Distributor", back_populates="clients")
    broker = relationship("Broker", back_populates="clients")
    account = relationship("SingleAccount", back_populates="client", uselist=False)

    @property
    def broker_name(self):
        return self.broker.broker_name if self.broker else None
    
    @property
    def distributor_name(self):
        return self.distributor.name if self.distributor else None

    def __repr__(self): 
        return f"<Client(client_id={self.client_id}, name={self.client_name})>"

================================================================================
# File: app/models/clients/__init__.py
================================================================================

from .broker_details import Broker
from .client_details import Client
from .distributor_details import Distributor

================================================================================
# File: app/models/clients/distributor_details.py
================================================================================

from sqlalchemy import Column, String, Text, TIMESTAMP, func, text
from sqlalchemy.orm import relationship
from app.models.base import Base


class Distributor(Base):
    __tablename__ = "distributor_details"

    distributor_id = Column(
        String,
        primary_key=True,
        server_default=text("CONCAT('DIST_', LPAD(NEXTVAL('distributor_seq')::TEXT, 5, '0'))")
    )
    name = Column(Text, nullable=False)
    created_at = Column(TIMESTAMP, nullable=False, server_default=func.now())

    clients = relationship("Client", back_populates="distributor")

    def __repr__(self):
        return f"<Distributor(id={self.distributor_id}, name={self.name})>"


================================================================================
# File: app/models/accounts/joint_account_mapping.py
================================================================================

from sqlalchemy import Column, String, Integer, ForeignKey
from sqlalchemy.orm import relationship
from app.models.base import Base


class JointAccountMapping(Base):
    __tablename__ = "joint_account_mapping"

    joint_account_mapping_id = Column(Integer, primary_key=True)
    joint_account_id = Column(
        String,
        ForeignKey("joint_account.joint_account_id", ondelete="CASCADE"),
        nullable=False
    )
    account_id = Column(
        String,
        ForeignKey("single_account.single_account_id", ondelete="CASCADE"),
        nullable=False
    )

    joint_account = relationship("JointAccount", back_populates="joint_account_mappings")

    def __repr__(self):
        return f"<JointAccountMapping(id={self.joint_account_mapping_id})>"

================================================================================
# File: app/models/accounts/joint_account.py
================================================================================

from sqlalchemy import (
    Column, String, Text, TIMESTAMP, Float, Integer, func, ForeignKey, text
)
from sqlalchemy.orm import relationship
from app.models.base import Base

class JointAccount(Base):
    __tablename__ = "joint_account"

    joint_account_id = Column(
        String,
        primary_key=True,
        server_default=text("CONCAT('JACC_', LPAD(NEXTVAL('joint_account_seq')::TEXT, 6, '0'))")
    )
    joint_account_name = Column(Text, nullable=False)
    account_type = Column(String, nullable=False, default="joint")
    portfolio_id = Column(Integer, ForeignKey("portfolio_template_details.portfolio_id"), nullable=True)
    bracket_id = Column(Integer, ForeignKey("bracket_details.bracket_id"), nullable=True)
    pf_value = Column(Float, default=0)
    cash_value = Column(Float, default=0)
    total_holdings = Column(Float, default=0)
    invested_amt = Column(Float, default=0)
    created_at = Column(TIMESTAMP, nullable=False, server_default=func.now())
    updated_at = Column(TIMESTAMP, nullable=False, server_default=func.now())

    portfolio_template = relationship("PortfolioTemplate", back_populates="joint_accounts")
    bracket = relationship("Bracket", back_populates="joint_accounts")
    
    performance = relationship(
        "AccountPerformance",
        uselist=False,
        back_populates="joint_account",
        primaryjoin="and_(JointAccount.joint_account_id == foreign(AccountPerformance.owner_id), "
                    "AccountPerformance.owner_type=='joint')",
        overlaps="performance,single_account"
    )

    actual_portfolios = relationship(
        "AccountActualPortfolio",
        back_populates="joint_account",
        primaryjoin="and_(JointAccount.joint_account_id == foreign(AccountActualPortfolio.owner_id), "
                    "AccountActualPortfolio.owner_type=='joint')",
        overlaps="actual_portfolios,single_account"
    )

    ideal_portfolios = relationship(
        "AccountIdealPortfolio",
        back_populates="joint_account",
        primaryjoin="and_(JointAccount.joint_account_id == foreign(AccountIdealPortfolio.owner_id), "
                    "AccountIdealPortfolio.owner_type=='joint')",
        overlaps="ideal_portfolios,single_account"
    )
    
    cashflow_details = relationship(
        "AccountCashflow",
        back_populates="joint_account",
        primaryjoin="and_(JointAccount.joint_account_id == foreign(AccountCashflow.owner_id), "
                    "AccountCashflow.owner_type=='joint')",
        overlaps="cashflow_details,single_account"
    )
    
    time_periods = relationship(
        "AccountTimePeriods",
        back_populates="joint_account",
        primaryjoin="and_(JointAccount.joint_account_id == foreign(AccountTimePeriods.owner_id), "
                    "AccountTimePeriods.owner_type=='joint')",
        overlaps="time_periods,single_account"
    )

    actual_portfolio_exceptions = relationship(
        "AccountActualPortfolioException",
        primaryjoin="and_(JointAccount.joint_account_id == foreign(AccountActualPortfolioException.owner_id), "
                    "AccountActualPortfolioException.owner_type == 'joint')",
        back_populates="joint_account"
    )

    joint_account_mappings = relationship("JointAccountMapping", back_populates="joint_account")

    def __repr__(self):
        return f"<JointAccount(joint_account_id={self.joint_account_id}, name={self.joint_account_name})>"

================================================================================
# File: app/models/accounts/account_bracket_basket_allocation.py
================================================================================

from sqlalchemy import Column, Integer, String, Float, Boolean, TIMESTAMP, func, ForeignKey, CheckConstraint
from app.models.base import Base

class AccountBracketBasketAllocation(Base):
    __tablename__ = "account_bracket_basket_allocation"

    id = Column(Integer, primary_key=True)
    account_id = Column(String, nullable=False)
    account_type = Column(String, nullable=False)
    bracket_id = Column(Integer, ForeignKey("bracket_details.bracket_id"), nullable=False)
    basket_id = Column(Integer, ForeignKey("basket_details.basket_id"), nullable=False)
    allocation_pct = Column(Float, nullable=False)
    is_custom = Column(Boolean, default=False)
    created_at = Column(TIMESTAMP, nullable=False, server_default=func.now())
    updated_at = Column(TIMESTAMP, nullable=True, onupdate=func.now())

    __table_args__ = (
        CheckConstraint("account_type IN ('single', 'joint')", name="account_type_check"),
    )

    def __repr__(self):
        return f"<AccountBracketBasketAllocation(id={self.id}, account_id={self.account_id}, allocation_pct={self.allocation_pct})>"

================================================================================
# File: app/models/accounts/account_performance.py
================================================================================

from sqlalchemy import Column, String, Float, TIMESTAMP, func
from app.models.accounts.owner_mixin import OwnerMixin
from sqlalchemy.orm import relationship
from app.models.base import Base

class AccountPerformance(Base, OwnerMixin):
    __tablename__ = "account_performance"

    performance_id = Column(String, primary_key=True)
    total_twrr = Column(Float)
    current_yr_twrr = Column(Float)
    cagr = Column(Float)
    created_at = Column(TIMESTAMP, nullable=False, server_default=func.now())
    updated_at = Column(TIMESTAMP, nullable=True, onupdate=func.now())  # Added

    single_account = relationship(
        "SingleAccount",
        back_populates="performance",
        primaryjoin="and_(SingleAccount.single_account_id == foreign(AccountPerformance.owner_id), "
                    "AccountPerformance.owner_type=='single')",
        overlaps="performance,joint_account"
    )

    joint_account = relationship(
        "JointAccount",
        back_populates="performance",
        primaryjoin="and_(JointAccount.joint_account_id == foreign(AccountPerformance.owner_id), "
                    "AccountPerformance.owner_type=='joint')",
        overlaps="performance,single_account"
    )

    def __repr__(self):
        return f"<AccountPerformance(owner_id={self.owner_id}, total_twrr={self.total_twrr})>" 

================================================================================
# File: app/models/accounts/account_cashflow_progression.py
================================================================================

from sqlalchemy import Column, Integer, String, Float, Date, TIMESTAMP, func, CheckConstraint
from app.models.base import Base

class AccountCashflowProgression(Base):
    __tablename__ = "account_cashflow_progression"

    id = Column(Integer, primary_key=True)
    owner_id = Column(String, nullable=False)
    owner_type = Column(String, nullable=False)
    event_date = Column(Date, nullable=False)
    cashflow = Column(Float, nullable=False)
    portfolio_value = Column(Float, nullable=False)
    portfolio_plus_cash = Column(Float, nullable=False)
    created_at = Column(TIMESTAMP, nullable=False, server_default=func.now())
    updated_at = Column(TIMESTAMP, nullable=True, onupdate=func.now())

    __table_args__ = (
        CheckConstraint("owner_type IN ('single', 'joint')", name="owner_type_check"),
    )

    def __repr__(self):
        return f"<AccountCashflowProgression(id={self.id}, owner_id={self.owner_id}, event_date={self.event_date})>"

================================================================================
# File: app/models/accounts/owner_mixin.py
================================================================================

from sqlalchemy import Column, String, CheckConstraint


class OwnerMixin:
    owner_id = Column(String, nullable=False)
    owner_type = Column(String, nullable=False)

    __table_args__ = (
        CheckConstraint("owner_type IN ('single','joint')", name="owner_type_check"),
    )
    

================================================================================
# File: app/models/accounts/account_actual_portfolio_exceptions.py
================================================================================

from sqlalchemy import (
    Column, Integer, String, Float, TIMESTAMP, func, CheckConstraint, UniqueConstraint
)
from sqlalchemy.orm import relationship
from app.models.base import Base

class AccountActualPortfolioException(Base):
    __tablename__ = "account_actual_portfolio_exceptions"

    id = Column(Integer, primary_key=True, autoincrement=True)
    owner_id = Column(String(255), nullable=False)
    owner_type = Column(String(10), nullable=False)
    trading_symbol = Column(String(50), nullable=False)
    quantity = Column(Float, nullable=False)
    created_at = Column(TIMESTAMP, nullable=False, server_default=func.now())
    updated_at = Column(TIMESTAMP, nullable=True, onupdate=func.now())

    __table_args__ = (
        CheckConstraint("owner_type IN ('single', 'joint')", name="owner_type_check"),
        CheckConstraint("quantity >= 0", name="quantity_positive"),
        UniqueConstraint("owner_id", "owner_type", "trading_symbol", name="unique_exception"),
    )
    single_account = relationship(
        "SingleAccount",
        primaryjoin="and_(SingleAccount.single_account_id == foreign(AccountActualPortfolioException.owner_id), "
                    "AccountActualPortfolioException.owner_type == 'single')",
        back_populates="actual_portfolio_exceptions",
        overlaps="actual_portfolio_exceptions,joint_account"
    )
    joint_account = relationship(
        "JointAccount",
        primaryjoin="and_(JointAccount.joint_account_id == foreign(AccountActualPortfolioException.owner_id), "
                    "AccountActualPortfolioException.owner_type == 'joint')",
        back_populates="actual_portfolio_exceptions",
        overlaps="actual_portfolio_exceptions,single_account"
    )

    def __repr__(self):
        return f"<AccountActualPortfolioException(id={self.id}, owner_id={self.owner_id}, trading_symbol={self.trading_symbol}, quantity={self.quantity})>"

================================================================================
# File: app/models/accounts/account_ideal_portfolio.py
================================================================================

from sqlalchemy import Column, Integer, String, Float, TIMESTAMP, Text, func, Date
from app.models.accounts.owner_mixin import OwnerMixin
from sqlalchemy.orm import relationship
from app.models.base import Base

class AccountIdealPortfolio(Base):

    __tablename__ = 'account_ideal_portfolio'

    owner_id = Column(String, primary_key=True)
    owner_type = Column(String, nullable=False)
    snapshot_date = Column(Date, primary_key=True)
    trading_symbol = Column(Text, primary_key=True)
    basket = Column(Text, nullable=False)
    allocation_pct = Column(Float, nullable=False)
    investment_amount = Column(Float, nullable=False)
    created_at = Column(TIMESTAMP, nullable=False, server_default=func.current_timestamp())
    updated_at = Column(TIMESTAMP, server_default=func.current_timestamp())

    single_account = relationship(
        "SingleAccount",
        back_populates="ideal_portfolios",
        primaryjoin="and_(SingleAccount.single_account_id == foreign(AccountIdealPortfolio.owner_id), "
                    "AccountIdealPortfolio.owner_type=='single')",
        overlaps="ideal_portfolios,joint_account"
    )
    joint_account = relationship(
        "JointAccount",
        back_populates="ideal_portfolios",
        primaryjoin="and_(JointAccount.joint_account_id == foreign(AccountIdealPortfolio.owner_id), "
                    "AccountIdealPortfolio.owner_type=='joint')",
        overlaps="ideal_portfolios,single_account"
    )

    def __repr__(self):
        return f"<AccountIdealPortfolio(owner_id={self.owner_id}, symbol={self.trading_symbol})>"

================================================================================
# File: app/models/accounts/account_actual_portfolio.py
================================================================================

from sqlalchemy import Column, String, Text, Float, TIMESTAMP, ForeignKey, Date
from sqlalchemy.sql import func
from sqlalchemy.orm import relationship
from app.models.accounts.owner_mixin import OwnerMixin
from app.models.base import Base

class AccountActualPortfolio(Base):

    __tablename__ = 'account_actual_portfolio'

    owner_id = Column(String, primary_key=True)
    owner_type = Column(String, nullable=False)
    snapshot_date = Column(Date, primary_key=True) 
    trading_symbol = Column(Text, primary_key=True)
    quantity = Column(Float, nullable=False)
    market_value = Column(Float, nullable=False)
    created_at = Column(TIMESTAMP, nullable=False, server_default=func.current_timestamp())
    updated_at = Column(TIMESTAMP, server_default=func.current_timestamp())

    single_account = relationship(
        "SingleAccount",
        back_populates="actual_portfolios",
        primaryjoin="and_(SingleAccount.single_account_id == foreign(AccountActualPortfolio.owner_id), "
                    "AccountActualPortfolio.owner_type=='single')",
        overlaps="actual_portfolios,joint_account"
    )
    joint_account = relationship(
        "JointAccount",
        back_populates="actual_portfolios",
        primaryjoin="and_(JointAccount.joint_account_id == foreign(AccountActualPortfolio.owner_id), "
                    "AccountActualPortfolio.owner_type=='joint')",
        overlaps="actual_portfolios,single_account"
    )

    def __repr__(self):
        return f"<AccountActualPortfolio(owner_id={self.owner_id}, symbol={self.trading_symbol})>"

================================================================================
# File: app/models/accounts/account_cashflow_details.py
================================================================================

from sqlalchemy import Column, Integer, Text, Float, Date, TIMESTAMP, func
from app.models.accounts.owner_mixin import OwnerMixin
from sqlalchemy.orm import relationship
from app.models.base import Base

class AccountCashflow(Base, OwnerMixin):
    __tablename__ = "account_cashflow_details"

    cashflow_id = Column(Integer, primary_key=True, autoincrement=True)
    event_date = Column(Date, nullable=False)
    cashflow = Column(Float, nullable=False)
    tag = Column(Text, nullable=True)
    created_at = Column(TIMESTAMP, nullable=False, server_default=func.now())
    updated_at = Column(TIMESTAMP, nullable=True, onupdate=func.now())

    single_account = relationship(
        "SingleAccount",
        back_populates="cashflow_details",
        primaryjoin="and_(SingleAccount.single_account_id == foreign(AccountCashflow.owner_id), "
                    "AccountCashflow.owner_type=='single')",
        overlaps="cashflow_details,joint_account"
    )

    joint_account = relationship(
        "JointAccount",
        back_populates="cashflow_details",
        primaryjoin="and_(JointAccount.joint_account_id == foreign(AccountCashflow.owner_id), "
                    "AccountCashflow.owner_type=='joint')",
        overlaps="cashflow_details,single_account"
    )
    
    def __repr__(self):
        return f"<AccountCashflow(id={self.cashflow_id}, owner_id={self.owner_id})>"

================================================================================
# File: app/models/accounts/__init__.py
================================================================================

from .account_actual_portfolio import AccountActualPortfolio
from .account_actual_portfolio_exceptions import AccountActualPortfolioException
from .account_bracket_basket_allocation import AccountBracketBasketAllocation
from .account_cashflow_details import AccountCashflow
from .account_cashflow_progression import AccountCashflowProgression
from .account_ideal_portfolio import AccountIdealPortfolio
from .account_performance import AccountPerformance
from .account_time_periods import AccountTimePeriods
from .joint_account import JointAccount
from .joint_account_mapping import JointAccountMapping
from .single_account import SingleAccount

================================================================================
# File: app/models/accounts/single_account.py
================================================================================

from sqlalchemy import Column, String, Text, TIMESTAMP, Float, func, ForeignKey, text, Integer
from sqlalchemy.orm import relationship
from app.models.base import Base

class SingleAccount(Base):
    __tablename__ = "single_account"
    
    single_account_id = Column(
        String,
        primary_key=True,
        server_default=text("CONCAT('ACC_', LPAD(NEXTVAL('single_account_seq')::TEXT, 6, '0'))")
    )
    account_name = Column(Text, nullable=False)
    account_type = Column(String, nullable=False, default='single')
    portfolio_id = Column(Integer, ForeignKey("portfolio_template_details.portfolio_id"), nullable=True)
    bracket_id = Column(Integer, ForeignKey("bracket_details.bracket_id"), nullable=True)
    pf_value = Column(Float, default=0)
    cash_value = Column(Float, default=0)
    total_holdings = Column(Float, default=0)
    invested_amt = Column(Float, default=0)
    created_at = Column(TIMESTAMP, nullable=False, server_default=func.now())
    updated_at = Column(TIMESTAMP, nullable=False, server_default=func.now())

    portfolio_template = relationship("PortfolioTemplate", back_populates="accounts")
    bracket = relationship("Bracket", back_populates="accounts")

    performance = relationship(
        "AccountPerformance",
        uselist=False,
        back_populates="single_account",
        primaryjoin="and_(SingleAccount.single_account_id == foreign(AccountPerformance.owner_id), "
                    "AccountPerformance.owner_type=='single')",
        overlaps="performance,joint_account"
    )

    actual_portfolios = relationship(
        "AccountActualPortfolio",
        back_populates="single_account",
        primaryjoin="and_(SingleAccount.single_account_id == foreign(AccountActualPortfolio.owner_id), "
                    "AccountActualPortfolio.owner_type=='single')",
        overlaps="actual_portfolios,joint_account"
    )
    ideal_portfolios = relationship(
        "AccountIdealPortfolio",
        back_populates="single_account",
        primaryjoin="and_(SingleAccount.single_account_id == foreign(AccountIdealPortfolio.owner_id), "
                    "AccountIdealPortfolio.owner_type=='single')",
        overlaps="ideal_portfolios,joint_account"
    )
    
    cashflow_details = relationship(
        "AccountCashflow",
        back_populates="single_account",
        primaryjoin="and_(SingleAccount.single_account_id == foreign(AccountCashflow.owner_id), "
                    "AccountCashflow.owner_type=='single')",
        overlaps="cashflow_details,joint_account"
    )
    
    time_periods = relationship(
        "AccountTimePeriods",
        back_populates="single_account",
        primaryjoin="and_(SingleAccount.single_account_id == foreign(AccountTimePeriods.owner_id), "
                    "AccountTimePeriods.owner_type=='single')",
        overlaps="time_periods,joint_account"
    )
    
    actual_portfolio_exceptions = relationship(
        "AccountActualPortfolioException",
        primaryjoin="and_(SingleAccount.single_account_id == foreign(AccountActualPortfolioException.owner_id), "
                    "AccountActualPortfolioException.owner_type == 'single')",
        back_populates="single_account"
    )

    client = relationship("Client", back_populates="account")

    def __repr__(self):
        return f"<Account(single_account_id={self.single_account_id}, name={self.account_name})>"

================================================================================
# File: app/models/accounts/account_time_periods.py
================================================================================

from sqlalchemy import Column, String, Integer, Float, Date, TIMESTAMP, func
from app.models.accounts.owner_mixin import OwnerMixin
from sqlalchemy.orm import relationship
from app.models.base import Base

class AccountTimePeriods(Base, OwnerMixin):
    __tablename__ = "account_time_periods"

    owner_id = Column(String, primary_key=True)
    owner_type = Column(String, nullable=False)
    time_period_id = Column(Integer, primary_key=True, autoincrement=True)
    start_date = Column(Date, nullable=False)
    start_value = Column(Float, nullable=False)
    end_date = Column(Date, nullable=False)
    end_value = Column(Float, nullable=False)
    returns = Column(Float, nullable=False)
    returns_1 = Column(Float)
    created_at = Column(TIMESTAMP, nullable=False, server_default=func.now())
    updated_at = Column(TIMESTAMP, nullable=True, onupdate=func.now())  # Added

    single_account = relationship(
        "SingleAccount",
        back_populates="time_periods",
        primaryjoin="and_(SingleAccount.single_account_id == foreign(AccountTimePeriods.owner_id), "
                    "AccountTimePeriods.owner_type=='single')",
        overlaps="time_periods,joint_account"
    )

    joint_account = relationship(
        "JointAccount",
        back_populates="time_periods",
        primaryjoin="and_(JointAccount.joint_account_id == foreign(AccountTimePeriods.owner_id), "
                    "AccountTimePeriods.owner_type=='joint')",
        overlaps="time_periods,single_account"
    )

    def __repr__(self):
        return f"<AccountTimePeriods(id={self.time_period_id}, owner_id={self.owner_id})>"

================================================================================
# File: app/routers/stock_exceptions_router.py
================================================================================

from fastapi import APIRouter, Depends
from sqlalchemy.orm import Session
from app.schemas.stock_exceptions import StockException, StockExceptionCreate
from app.models.stock_exceptions import StockException as StockExceptionModel
from app.database import get_db

router = APIRouter()

@router.post("/exceptions/", response_model=StockException)
def create_exception(exception: StockExceptionCreate, db: Session = Depends(get_db)):
    db_exception = StockExceptionModel(**exception.model_dump())
    db.add(db_exception)
    db.commit()
    db.refresh(db_exception)
    return db_exception

================================================================================
# File: app/routers/clients_router.py
================================================================================

from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.ext.asyncio import AsyncSession
from typing import List, Dict, Any
from app.database import get_db
from app.schemas.clients.client_details import (
    ClientCreateRequest,
    BulkClientResponse,
    ClientListResponse
)
from app.services.clients.clients_service import ClientService
from app.services.clients.distributors_service import DistributorService
from app.services.clients.brokers_service import BrokerService

client_router = APIRouter(prefix="/clients", tags=["Clients"])
distributor_router = APIRouter(prefix="/distributors", tags=["Distributors"])
broker_router = APIRouter(prefix="/brokers", tags=["Brokers"])

@client_router.get("/list", response_model=List[ClientListResponse])
async def get_all_clients_endpoint(
    db: AsyncSession = Depends(get_db)
):
    """Fetch complete client data for sheet updates"""
    try:
        result = await ClientService.get_all_clients(db)
        return result
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail="Failed to retrieve client list due to server error"
        )

@client_router.post("/add", response_model=BulkClientResponse)
async def add_clients(
    data_list: List[ClientCreateRequest],
    db: AsyncSession = Depends(get_db)
):
    """
    Bulk create clients with partial success. Each row that fails
    is skipped, others are committed.
    """
    try:
        result = await ClientService.bulk_create_clients(db, data_list)
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@client_router.post("/update", response_model=BulkClientResponse)
async def update_clients(
    data_list: List[ClientCreateRequest],
    db: AsyncSession = Depends(get_db)
):
    """
    Bulk update clients with partial success. Rows that fail
    don't block others from succeeding.
    """
    try:
        result = await ClientService.bulk_update_clients(db, data_list)
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@client_router.post("/delete", response_model=BulkClientResponse)
async def delete_clients(
    client_ids: List[str],
    db: AsyncSession = Depends(get_db)
):
    """
    Bulk delete clients. If a row fails, it's skipped, others succeed.
    The returned BulkClientResponse indicates row-by-row results.
    """
    try:
        result = await ClientService.bulk_delete_clients(db, client_ids)
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@distributor_router.get("/", response_model=List[Dict[str, Any]])
async def read_distributors(db: AsyncSession = Depends(get_db)):
    """Get data for all the distributors in the DB."""
    return await DistributorService.get_distributors(db)

@distributor_router.post("/add", response_model=Dict[str, Any])
async def create_distributor(payload: Dict[str, str], db: AsyncSession = Depends(get_db)):
    let_name = (payload.get("value") or "").strip()
    if not let_name:
        raise HTTPException(status_code=400, detail="Distributor name is required")
    try:
        return await DistributorService.add_distributor(db, let_name)
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

@distributor_router.put("/update", response_model=Dict[str, Any])
async def modify_distributor(payload: Dict[str, str], db: AsyncSession = Depends(get_db)):
    old_value = (payload.get("old_value") or "").strip()
    new_value = (payload.get("new_value") or "").strip()
    if not old_value or not new_value:
        raise HTTPException(status_code=400, detail="Both old and new distributor names are required")
    try:
        return await DistributorService.update_distributor(db, old_value, new_value)
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

@distributor_router.delete("/delete", response_model=Dict[str, Any])
async def remove_distributor(payload: Dict[str, str], db: AsyncSession = Depends(get_db)):
    name = (payload.get("value") or "").strip()
    if not name:
        raise HTTPException(status_code=400, detail="Distributor name is required")
    try:
        return await DistributorService.delete_distributor(db, name)
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

@broker_router.get("/", response_model=List[Dict[str, Any]])
async def read_brokers(db: AsyncSession = Depends(get_db)):
    """Get data for all the brokers in the DB."""
    return await BrokerService.get_brokers(db)

@broker_router.post("/add", response_model=Dict[str, Any])
async def create_broker(payload: Dict[str, str], db: AsyncSession = Depends(get_db)):
    broker_name = (payload.get("value") or "").strip()
    if not broker_name:
        raise HTTPException(status_code=400, detail="Broker name is required")
    try:
        return await BrokerService.add_broker(db, broker_name)
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

@broker_router.put("/update", response_model=Dict[str, Any])
async def modify_broker(payload: Dict[str, str], db: AsyncSession = Depends(get_db)):
    old_value = (payload.get("old_value") or "").strip()
    new_value = (payload.get("new_value") or "").strip()
    if not old_value or not new_value:
        raise HTTPException(status_code=400, detail="Both old and new broker names are required")
    try:
        return await BrokerService.update_broker(db, old_value, new_value)
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

@broker_router.delete("/delete", response_model=Dict[str, Any])
async def remove_broker(payload: Dict[str, str], db: AsyncSession = Depends(get_db)):
    broker_name = (payload.get("value") or "").strip()
    if not broker_name:
        raise HTTPException(status_code=400, detail="Broker name is required")
    try:
        return await BrokerService.delete_broker(db, broker_name)
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

================================================================================
# File: app/routers/portfolios_router.py
================================================================================

from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.ext.asyncio import AsyncSession
from typing import List, Dict, Any
from app.database import get_db
from app.services.portfolio.portfolio_service import PortfolioService 

portfolio_router = APIRouter(prefix="/portfolios", tags=["Portfolios"])

@portfolio_router.get("/", response_model=List[Dict[str, Any]])
async def list_portfolios(db: AsyncSession = Depends(get_db)):
    """Get data for all the portfolios in the DB."""
    return await PortfolioService.get_portfolios(db)  

@portfolio_router.get("/{portfolio_id}/structure", response_model=Dict[str, Any])
async def portfolio_structure(portfolio_id: int, db: AsyncSession = Depends(get_db)):
    data = await PortfolioService.get_portfolio_structure(db, portfolio_id)  
    if not data:
        raise HTTPException(404, "Portfolio not found")
    return data

@portfolio_router.post("/{portfolio_id}/structure/save", response_model=Dict[str, Any])
async def save_structure(portfolio_id: int, payload: Dict[str, Any], db: AsyncSession = Depends(get_db)):
    return await PortfolioService.save_portfolio_structure(db, portfolio_id, payload) 

================================================================================
# File: app/routers/__init__.py
================================================================================



================================================================================
# File: app/routers/accounts_router.py
================================================================================

from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.ext.asyncio import AsyncSession
from app.database import get_db
from app.schemas.accounts.account import (
    ViewAccountsResponse, AccountUpdateRequest, BulkAccountResponse
)
from app.schemas.accounts.joint_account import (
    JointAccountCreateRequest,
    JointAccountResponse,
    JointAccountDeleteRequest,
    JointAccountUpdateRequest
)
from app.services.accounts.joint_account_service import JointAccountService
from app.services.accounts.account_service import AccountService
from app.logger import logger
from typing import List

account_router = APIRouter(prefix="/accounts", tags=["Accounts"])
joint_account_router = APIRouter(prefix="/joint-accounts", tags=["Joint Accounts"])

@account_router.get("/list", response_model=ViewAccountsResponse)
async def get_view_accounts(db: AsyncSession = Depends(get_db)):
    """
    Retrieves a unified list of single and joint accounts with bracket, portfolio,
    and performance data, returned in a standardized format.
    """
    try:
        data = await AccountService.get_all_accounts_view(db)
        return ViewAccountsResponse(
            status="success",
            data=data
        )
    except Exception as e:
        logger.error(f"Error while fetching all accounts: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to retrieve accounts."
        )

@account_router.post("/update", response_model=BulkAccountResponse)
async def update_accounts(
    data_list: List[AccountUpdateRequest],
    db: AsyncSession = Depends(get_db)
):
    """
    Bulk update accounts for partial success:
    * single: update pf/cash/invested + optional TWRR
    * joint: only TWRR fields
    * Then recalc any linked joint for single changes
    """
    try:
        resp = await AccountService.bulk_update_accounts(db, data_list)
        return resp
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@joint_account_router.post("/", response_model=JointAccountResponse)
async def create_joint_account_endpoint(
    payload: JointAccountCreateRequest,
    db: AsyncSession = Depends(get_db)
):
    """
    Endpoint to create a new JointAccount.
    """
    logger.info("Endpoint '/joint-accounts' [POST] called with data: %s", payload.model_dump())
    try:
        result = await JointAccountService.create_joint_account(db, payload)
        if not result:
            logger.error("Unable to create joint account.")
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Unable to create joint account."
            )
        logger.info("Endpoint '/joint-accounts' [POST] completed successfully: %s", result)
        return result
    except ValueError as ve:
        logger.error("Validation error in '/joint-accounts' [POST]: %s", str(ve))
        raise HTTPException(
            status_code=400,
            detail=str(ve)
        )
    except Exception as e:
        logger.critical("Critical error in '/joint-accounts' [POST]: %s", str(e), exc_info=True, stack_info=True)
        raise HTTPException(
            status_code=500,
            detail="Failed to create joint account due to server error"
        )

@joint_account_router.put("/{joint_account_id}", response_model=JointAccountResponse)
async def update_joint_account_endpoint(
    joint_account_id: str,
    payload: JointAccountUpdateRequest,
    db: AsyncSession = Depends(get_db)
):
    """
    Endpoint to update an existing JointAccount.
    """
    logger.info(
        "Endpoint '/joint-accounts/%s' [PUT] called with data: %s",
        joint_account_id, payload.model_dump()
    )
    try:
        result = await JointAccountService.update_joint_account(db, joint_account_id, payload)
        if not result:
            logger.error("Joint account '%s' not found or could not be updated.", joint_account_id)
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"Joint account '{joint_account_id}' not found or could not be updated."
            )
        logger.info("Endpoint '/joint-accounts/%s' [PUT] completed successfully: %s", joint_account_id, result)
        return result
    except ValueError as ve:
        logger.error("Validation error in '/joint-accounts/%s' [PUT]: %s", joint_account_id, str(ve))
        raise HTTPException(status_code=400, detail=str(ve))
    except Exception as e:
        logger.critical("Critical error in '/joint-accounts/%s' [PUT]: %s", joint_account_id, str(e), exc_info=True, stack_info=True)
        raise HTTPException(
            status_code=500,
            detail="Failed to update joint account due to server error"
        )

@joint_account_router.delete("/{joint_account_id}", response_model=JointAccountResponse)
async def delete_joint_account_endpoint(
    joint_account_id: str,
    db: AsyncSession = Depends(get_db)
):
    """
    Endpoint to delete the specified JointAccount.
    """
    logger.info("Endpoint '/joint-accounts/%s' [DELETE] called", joint_account_id)
    try:
        result = await JointAccountService.delete_joint_account(db, joint_account_id)
        if not result:
            logger.warning("Joint account '%s' not found. Cannot delete.", joint_account_id)
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Joint account '{joint_account_id}' not found."
            )
        logger.info("Endpoint '/joint-accounts/%s' [DELETE] completed successfully: %s", joint_account_id, result)
        return result
    except ValueError as ve:
        logger.error("Validation error in '/joint-accounts/%s' [DELETE]: %s", joint_account_id, str(ve))
        raise HTTPException(status_code=400, detail=str(ve))
    except Exception as e:
        logger.critical("Critical error in '/joint-accounts/%s' [DELETE]: %s", joint_account_id, str(e), exc_info=True, stack_info=True)
        raise HTTPException(
            status_code=500,
            detail="Failed to delete joint account due to server error"
        )
    



================================================================================
# File: app/routers/accounts_data_router.py
================================================================================

from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.ext.asyncio import AsyncSession
from app.database import get_db
from app.services.accounts.accounts_data_service import AccountTimePeriodsService
from app.schemas.accounts.account_time_periods import AccountTimePeriodsResponse
from app.services.accounts.accounts_data_service import AccountCashflowProgressionService
from app.schemas.accounts.account_cashflow_progression import AccountCashflowProgressionResponse
from typing import List, Optional

accounts_data_router = APIRouter(prefix="/accounts-data", tags=["Accounts Data"])

@accounts_data_router.get("/time-periods/{owner_id}", response_model=List[AccountTimePeriodsResponse])
async def get_time_periods(
    owner_id: str,
    owner_type: Optional[str] = None,
    db: AsyncSession = Depends(get_db)
):
    """Fetch time periods for a given owner_id and optional owner_type."""
    try:
        time_periods = await AccountTimePeriodsService.get_time_periods_by_owner_id(
            db, owner_id, owner_type
        )
        if not time_periods:
            raise HTTPException(
                status_code=404,
                detail="No time periods found for this owner_id"
            )
        return time_periods
    except ValueError as ve:
        raise HTTPException(status_code=400, detail=str(ve))
    except HTTPException as http_exc:
        raise http_exc
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail="Failed to retrieve time periods due to server error"
        )

@accounts_data_router.get("/cashflow-progression/{owner_id}", response_model=List[AccountCashflowProgressionResponse])
async def get_cashflow_progression(
    owner_id: str,
    owner_type: Optional[str] = None,
    db: AsyncSession = Depends(get_db)
    ):
    """
    Fetch cashflow progression data for a given owner_id and optional owner_type.
    
    Args:
        owner_id (str): The ID of the account owner.
        owner_type (Optional[str]): The type of owner ("single" or "joint"), if specified.
        db (AsyncSession): The database session, injected via dependency.
    
    Returns:
        List[AccountCashflowProgressionResponse]: A list of cashflow progression records.
    
    Raises:
        HTTPException: If no data is found (404), input is invalid (400), or a server error occurs (500).
    """
    try:
        cashflow_progression = await AccountCashflowProgressionService.get_cashflow_progression_by_owner_id(
            db, owner_id, owner_type
        )
        if not cashflow_progression:
            raise HTTPException(
                status_code=404,
                detail=f"No cashflow progression found for owner_id: {owner_id}"
            )
        
        return cashflow_progression
    
    except ValueError as ve:
        raise HTTPException(status_code=400, detail=str(ve))
    except HTTPException as http_exc:
        raise http_exc
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail="Failed to retrieve cashflow progression due to an internal server error"
        )



================================================================================
# File: app/routers/report_router.py
================================================================================

import boto3
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession
from typing import List, Dict, Any
from app.database import get_db
from app.services.report_service import ReportService
from app.models.report import RequestData
from app.models.accounts.joint_account_mapping import JointAccountMapping
from app.models.clients.client_details import Client


report_router = APIRouter(prefix="/reports", tags=["Reports"])

@report_router.post("/send-report")
async def send_report(data: RequestData, db: AsyncSession = Depends(get_db)):
    """API endpoint to send reports based on broker_code and pan_no."""
    service = ReportService(db, boto3.client('s3'))
    try:
        client = await service.verify_user(data.broker_code, data.pan_no)
        accounts = await service.get_accounts(client)
        single_report = service.get_latest_report([client.broker_code], is_joint=False)

        joint_reports = []
        for joint_account in accounts["joint"]:
            stmt_mappings = select(JointAccountMapping).where(
                JointAccountMapping.joint_account_id == joint_account.joint_account_id
            )
            result_mappings = await db.execute(stmt_mappings)
            joint_mappings = result_mappings.scalars().all()
            joint_single_account_ids = [m.account_id for m in joint_mappings]
            
            stmt_clients = select(Client).where(
                Client.account_id.in_(joint_single_account_ids)
            )
            result_clients = await db.execute(stmt_clients)
            joint_clients = result_clients.scalars().all()
            joint_broker_codes = [c.broker_code for c in joint_clients]
            joint_report = service.get_latest_report(joint_broker_codes, is_joint=True)
            if joint_report:
                joint_reports.append((joint_report, f"JointAccount_{joint_account.joint_account_id}_report.pdf"))

        reports = []
        if single_report:
            reports.append((single_report, f"{client.broker_code}_report.pdf"))
        reports.extend(joint_reports)

        if reports:
            service.send_reports_email(client.email_id, reports)
            return {"status": "success", "message": "Reports sent successfully"}
        else:
            raise HTTPException(status_code=404, detail="No reports found")
    
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")
    
@report_router.post("/dummy-report")
async def dummy_report(data: RequestData, db: AsyncSession = Depends(get_db)):
    """API endpoint to send reports based on broker_code and pan_no."""
    service = ReportService(db, boto3.client('s3'))
    try:
        client = await service.verify_user(data.broker_code, data.pan_no)
        accounts = await service.get_accounts(client)
        single_report = service.get_latest_report([client.broker_code], is_joint=False)

        joint_reports = []
        for joint_account in accounts["joint"]:
            stmt_mappings = select(JointAccountMapping).where(
                JointAccountMapping.joint_account_id == joint_account.joint_account_id
            )
            result_mappings = await db.execute(stmt_mappings)
            joint_mappings = result_mappings.scalars().all()
            joint_single_account_ids = [m.account_id for m in joint_mappings]
            
            stmt_clients = select(Client).where(
                Client.account_id.in_(joint_single_account_ids)
            )
            result_clients = await db.execute(stmt_clients)
            joint_clients = result_clients.scalars().all()
            joint_broker_codes = [c.broker_code for c in joint_clients]
            joint_report = service.get_latest_report(joint_broker_codes, is_joint=True)
            if joint_report:
                joint_reports.append((joint_report, f"JointAccount_{joint_account.joint_account_id}_report.pdf"))

        reports = []
        if single_report:
            reports.append((single_report, f"{client.broker_code}_report.pdf"))
        reports.extend(joint_reports)

        if reports:
            # service.send_reports_email(client.email_id, reports)
            return {
                "status": "success",
                "message": "Reports sent successfully",
                "data": data
            }
        else:
            raise HTTPException(status_code=404, detail="No reports found")
    
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")

================================================================================
# File: app/scripts/bulk_pf_insertions.py
================================================================================

import os
import pandas as pd
from datetime import datetime
import psycopg2
from psycopg2.extras import execute_values

# Database connection parameters (replace with your actual values)
db_params = {
    'dbname': 'plus91_cms',
    'user': 'postgres',
    'password': '+91atplus91',
    'host': 'plus91-db-instance-instance-1.c128qeo8eoe3.ap-south-1.rds.amazonaws.com',
    'port': '5432'
}

# Path to the DATA_CLEANING_NEW directory (replace with your actual path)
data_cleaning_new_path = '/home/admin/Plus91Backoffice/plus91_management/app/scripts/keynote_clients_data'

# Function to retrieve account_id for a given broker_code
def get_account_id(broker_code, cursor):
    """Queries the client_details table to get the account_id for a broker_code."""
    cursor.execute("SELECT account_id FROM client_details WHERE broker_code = %s LIMIT 1", (broker_code,))
    result = cursor.fetchone()
    return result[0] if result else None

# Establish database connection
try:
    conn = psycopg2.connect(**db_params)
    cursor = conn.cursor()
    print("Database connection established.")
except Exception as e:
    print(f"Failed to connect to database: {e}")
    exit()

# Get list of broker_code folders
broker_code_folders = [
    f for f in os.listdir(data_cleaning_new_path)
    if os.path.isdir(os.path.join(data_cleaning_new_path, f))
]

# Process each broker_code folder
for broker_code in broker_code_folders:
    if broker_code == 'MK100':
        print(f"Processing broker_code: {broker_code}")
        
        # Get the associated account_id
        account_id = get_account_id(broker_code, cursor)
        if not account_id:
            print(f"No account found for broker_code {broker_code}. Skipping.")
            continue
        
        # Define the holdings folder path
        holdings_path = os.path.join(data_cleaning_new_path, broker_code, 'holdings')
        if not os.path.exists(holdings_path):
            print(f"Holdings folder not found for {broker_code}. Skipping.")
            continue
        
        # Get list of Excel files in the holdings folder
        excel_files = [f for f in os.listdir(holdings_path) if f.endswith('.xlsx')]
        
        for excel_file in excel_files:
            print(f"Processing file: {excel_file}")
            
            # Parse snapshot_date from the filename (e.g., 2024-05-31.xlsx -> 2024-05-31)
            date_str = os.path.splitext(excel_file)[0]
            try:
                snapshot_date = datetime.strptime(date_str, '%Y-%m-%d').date()
            except ValueError:
                print(f"Invalid date format in filename {excel_file}. Skipping.")
                continue
            
            # Read the Excel file
            file_path = os.path.join(holdings_path, excel_file)
            try:
                df = pd.read_excel(file_path)
            except Exception as e:
                print(f"Error reading {excel_file}: {e}")
                continue
            
            # Verify required columns are present
            required_columns = ['trading_symbol', 'quantity', 'market_value']
            if not all(col in df.columns for col in required_columns):
                print(f"Missing required columns in {excel_file}. Skipping.")
                continue
            
            # Aggregate data by trading_symbol, summing quantity and market_value
            df_grouped = df.groupby('trading_symbol', as_index=False).agg({
                'quantity': 'sum',
                'market_value': 'sum'
            })
            
            # Prepare data for insertion
            data = [
                (account_id, 'single', snapshot_date, row['trading_symbol'], row['quantity'], row['market_value'])
                for _, row in df_grouped.iterrows()
            ]
            
            # Define the SQL insert query
            insert_query = """
            INSERT INTO account_actual_portfolio (owner_id, owner_type, snapshot_date, trading_symbol, quantity, market_value)
            VALUES %s
            """
            
            # Perform the bulk insert
            try:
                execute_values(cursor, insert_query, data)
                print(f"Inserted {len(data)} aggregated rows from {excel_file}.")
            except Exception as e:
                print(f"Error inserting data from {excel_file}: {e}")
                conn.rollback()  # Roll back on error
                continue
        
        # Commit changes after processing all files for this broker_code
        conn.commit()
        print(f"Completed processing for broker_code {broker_code}.")

# Clean up
cursor.close()
conn.close()
print("Database connection closed.")

================================================================================
# File: app/scripts/test.py
================================================================================

import asyncio
import pandas as pd
from app.scripts.data_fetchers.data_transformer import KeynoteDataTransformer, ZerodhaDataTransformer

keynote_data_transformer = KeynoteDataTransformer()
 

# cashflow = asyncio.run(keynote_data_transformer.transform_ledger_to_cashflow(
#     broker_code="MK100",
#     from_date="2022-04-01",
#     to_date="2025-03-11"
#     ))

# cashflow_df = pd.DataFrame(cashflow)
# print(cashflow_df)

holdings = asyncio.run(keynote_data_transformer.transform_holdings_to_actual_portfolio(
    broker_code="MK100",
    for_date="2025-03-31"
))
holding_df = pd.DataFrame(holdings)
print(holding_df)


zerodha_data_transformer = ZerodhaDataTransformer()
# holdings = asyncio.run(zerodha_data_transformer.transform_holdings_to_actual_portfolio(
#     broker_code="TWV350",
#     year=2024,
#     month=4,
# ))


# cashflow = asyncio.run(zerodha_data_transformer.transform_ledger_to_cashflow(
#     broker_code="MM5525"
# ))

# cashflow_df = pd.DataFrame(cashflow)


================================================================================
# File: app/scripts/scripts_runner.py
================================================================================

import sys
import boto3
import asyncio
import logging
import calendar
import pandas as pd
from app.scripts.db_processors.db_runner import runner
from app.scripts.report_generation.report_generator import (
    generate_report,
    load_bse500_data,
    get_portfolio_summary,
    get_returns_table,
    get_bse500_twrr_cagr,
    get_portfolio_report,
)
from app.scripts.report_generation.data_feeder import report_datafeeder

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

s3 = boto3.client('s3')
bucket_name = 'plus91backoffice'

async def main() -> None:
    """Main function to update database and generate reports."""
    try:
        logger.info("Starting database update...")
        await runner()
        logger.info("Database update completed.")

        logger.info("Fetching report data...")
        data = await report_datafeeder()
        if not data:
            logger.warning("No data fetched for reports.")
            return

        report_df = pd.DataFrame(data)
        print(report_df)
        report_df.loc[:, 'broker_codes'] = report_df['broker_codes'].apply(sort_broker_codes)
        bse500_df = load_bse500_data()

        grouped_df = report_df.groupby('account_id')
        for account_id, group in grouped_df:
            # if account_id in []:
                logger.info(f"Generating report for account ID: {account_id}")
                account_name = group['account_name'].iloc[0]
                acc_start_date = group['acc_start_date'].iloc[0]
                snapshot_date = group['snapshot_date'].iloc[0]
                invested_amt = group['invested_amt'].iloc[0]
                pf_value = group['pf_value'].iloc[0]
                cash_value = group['cash_value'].iloc[0]
                total_holdings = group['total_holdings'].iloc[0]
                actual_portfolio = group[['trading_symbol', 'quantity', 'market_value']]
                total_twrr = group['total_twrr'].iloc[0]
                current_yr_twrr = group['current_yr_twrr'].iloc[0]
                cagr = group['cagr'].iloc[0]
                broker_code = group['broker_codes'].iloc[0]

                formatted_broker_code = format_broker_code(broker_code)

                portfolio_summary = get_portfolio_summary(str(acc_start_date), total_holdings, invested_amt)
                portfolio_report = get_portfolio_report(actual_portfolio, cash_value, total_holdings)
                bse500_current_yr_twrr, bse500_abs_twrr, bse500_abs_cagr = get_bse500_twrr_cagr(
                    str(acc_start_date), str(snapshot_date), bse500_df
                )
                returns_table = get_returns_table(
                    current_yr_twrr, total_twrr, cagr,
                    bse500_current_yr_twrr, bse500_abs_twrr, bse500_abs_cagr
                )

                # Generate the report
                logo_path = "/home/admin/Plus91Backoffice/plus91_management/app/scripts/report_generation/assets/Plus91_logo.jpeg"
                down_design_path = "/home/admin/Plus91Backoffice/plus91_management/app/scripts/report_generation/assets/Down_design.jpeg"
                pdf_bytes = generate_report(
                    portfolio_report,
                    portfolio_summary,
                    returns_table,
                    logo_path,
                    down_design_path,
                    account_name,
                    formatted_broker_code,
                    str(acc_start_date)
                )
                snapshot_date = pd.to_datetime(snapshot_date)
                year = snapshot_date.year
                month_abbr = calendar.month_abbr[snapshot_date.month].upper()

                filename = f"{formatted_broker_code} {month_abbr} {year} Report.pdf"
                s3_key = f"PLUS91_PMS/reports/{year}/{month_abbr}/{filename}"
                # s3.put_object(
                #         Body=pdf_bytes,
                #         Bucket=bucket_name,
                #         Key=s3_key,
                #         ContentType='application/pdf'
                #     )
                logger.info(f"Report generated for {account_name} in S3 at: {s3_key}.")
    except Exception as e:
        logger.error(f"Error in main process: {e}", exc_info=True)

def format_broker_code(broker_code: str) -> str:
    """Format broker code(s) into a string like '[CODE1 - CODE2]'."""
    if "," in broker_code:
        codes = broker_code.split(", ")
        sorted_codes = sorted(codes)
        return f"[{' - '.join(sorted_codes)}]"
    return f"[{broker_code}]"

def sort_broker_codes(broker_code: str) -> str:
    return ', '.join(sorted(broker_code.split(', ')))


if __name__ == "__main__":
    asyncio.run(main())

================================================================================
# File: app/scripts/__init__.py
================================================================================



================================================================================
# File: app/scripts/report_generation/__init__.py
================================================================================



================================================================================
# File: app/scripts/report_generation/report_generator.py
================================================================================

import os
import time
import base64
import pandas as pd
import plotly.graph_objects as go
from functools import reduce
from plotly.subplots import make_subplots
from datetime import datetime, timedelta, date 
from app.scripts.data_fetchers.broker_data import BrokerData

def load_bse500_data():
    day_time = []
    last_eod_date = datetime(2020, 1, 1).date()

    start_time_day = last_eod_date
    current_time = datetime.now().date()
    back_time_day = current_time - timedelta(days = 1000)

    if back_time_day < start_time_day:
        back_time_day = start_time_day

    i = 0
    while back_time_day >= start_time_day:
        append_time = [str(current_time), str(back_time_day), 'day']
        day_time.append(append_time)  
        if back_time_day == start_time_day:
            break   
        current_time = back_time_day
        back_time_day = current_time - timedelta(days = 2000)
        if current_time >= start_time_day >= back_time_day:
            back_time_day = start_time_day
        i += 1

    store_dfs = []
    for value in day_time:
        from_date = value[1]
        to_date = value[0]
        interval = value[2]
        exchange = "BSE"
        exchange_token = "4"
        instrument_type = "INDEX"
        instrument = {
            "from_date": str(from_date),
            "to_date": str(to_date),
            "interval": interval,
            "exchange": exchange,
            "exchange_token": exchange_token,
            "instrument_type": instrument_type
        }
        time.sleep(1)
        bse500_data = BrokerData.historical_data(instrument)
        bse500_data = bse500_data['data']
        bse500_df = pd.DataFrame(bse500_data)
        if not bse500_df.empty:
            bse500_df = bse500_df.fillna(0)
            store_dfs.append(bse500_df)
        else:
            print("dataset is empty")
        
        complete_bse500_data = pd.concat(store_dfs, ignore_index=True)
        complete_bse500_data['datetime'] = pd.to_datetime(complete_bse500_data['datetime'])
        complete_bse500_data['datetime'] = complete_bse500_data['datetime'].dt.date
        complete_bse500_data = complete_bse500_data.drop_duplicates().sort_values(by="datetime").reset_index()
        return complete_bse500_data

def read_image_as_base64(file_path):
    with open(file_path, 'rb') as image_file:
        return base64.b64encode(image_file.read()).decode()
    
def calculate_years(start_date: str, end_date: str):
    start_date = datetime.strptime(start_date, "%Y-%m-%d")
    end_date = datetime.strptime(end_date, "%Y-%m-%d")
    years = end_date.year - start_date.year
    months_diff = end_date.month - start_date.month
    days_diff = end_date.day - start_date.day
    
    if months_diff < 0 or (months_diff == 0 and days_diff < 0):
        years -= 1
        months_diff += 12
    fraction_of_year = months_diff / 12 + days_diff / 365.25
    return years + fraction_of_year

def get_bse500_twrr_cagr(acc_start_date: str, snapshot_date: str, bse500_df: pd.DataFrame):
    acc_start_date = pd.to_datetime(acc_start_date)
    snapshot_date = pd.to_datetime(snapshot_date)
    bse500_df['datetime'] = pd.to_datetime(bse500_df['datetime'])
    bse500_df = bse500_df[
        (bse500_df['datetime'] >= acc_start_date) &
        (bse500_df['datetime'] <= snapshot_date)
    ]
    bse500_df['year'] = bse500_df['datetime'].dt.year
    bse500_df['month'] = bse500_df['datetime'].dt.month
    bse500_df['day'] = bse500_df['datetime'].dt.day

    march_values = []
    for year, year_group in bse500_df.groupby('year'):
        for month, month_group in year_group.groupby('month'):
            for day, day_group in month_group.groupby('day'):
                if month == 4 and day == 1:
                    march_values.append(month_group.iloc[-1])

    march_df = pd.DataFrame(march_values)

    first_row = bse500_df.iloc[[0]]
    last_row = bse500_df.iloc[[-1]]

    bse500_df = pd.concat([first_row, last_row, march_df], ignore_index=False)
    bse500_df = bse500_df[['datetime', 'close']]
    bse500_df = bse500_df.sort_values(by='datetime').reset_index(drop=True)

    bse500_df['start_date'] = bse500_df['datetime']
    bse500_df['end_date'] = bse500_df['datetime'].shift(-1).fillna(0)
    bse500_df['start_value'] = bse500_df['close']
    bse500_df['end_value'] = bse500_df['close'].shift(-1).fillna(0)
    bse500_df = bse500_df.drop(columns=['datetime', 'close'])
    bse500_df = bse500_df[bse500_df['end_value'] != 0]
    bse500_df['Returns'] = (bse500_df['end_value'] / bse500_df['start_value']) - 1
    bse500_df['Returns+1'] = bse500_df['Returns'] + 1

    absolute_twrr = ((bse500_df['end_value'].iloc[-1] / bse500_df['start_value'].iloc[0]) - 1) * 100
    current_year_twrr = bse500_df['Returns'].iloc[-1] * 100
    
    cagrs = list(bse500_df['Returns+1'])
    product = reduce(lambda x, y: x * y, cagrs)
    absolute_cagr = 0
    years_value = calculate_years(start_date=str(acc_start_date.date()), end_date=str(snapshot_date.date()))
    if years_value > 1:
        absolute_cagr = ((product ** (1/years_value)) - 1) * 100
    return round(current_year_twrr, 2), round(absolute_twrr, 2), round(absolute_cagr, 2)

def get_portfolio_summary(inception_date: str, total_holdings: float, invested_amt: float) -> pd.DataFrame:
    pnl = int(total_holdings - invested_amt)
    summary_data = {
        "PORTFOLIO SUMMARY": [f"SINCE - {inception_date}", "NET CAPITAL INVESTED", "TOTAL GAIN/LOSS", "PORTFOLIO VALUE"],
        "": [ "Amount(INR)", f"₹{round(invested_amt, 2)}", f"₹{pnl}", f"₹{round(total_holdings, 2)}"]
    }
    portfolio_summary = pd.DataFrame(summary_data)
    portfolio_summary.set_index("PORTFOLIO SUMMARY", inplace=True)
    return portfolio_summary

def get_returns_table(
        plus91_current_year_twrr: float = 0,
        plus91_absolute_twrr: float = 0,
        plus91_absolute_cagr: float = 0,
        bse500_current_year_twrr: float = 0,
        bse500_absolute_twrr: float = 0,
        bse500_absolute_cagr: float = 0
    ) -> pd.DataFrame:
    returns_data = {
    'RETURNS': ['PLUS91', 'BSE500'],
    'FY 24-25': [f'{round(plus91_current_year_twrr, 1)}%', f'{round(bse500_current_year_twrr, 1)}%'],
    'ABSOLUTE RETURNS': [f'{round(plus91_absolute_twrr, 1)}%', f'{round(bse500_absolute_twrr, 1)}%'],
    'SINCE INCEPTION CAGR': [f'{round(plus91_absolute_cagr, 1)}%', f'{round(bse500_absolute_cagr, 1)}%']
    }
    returns_df = pd.DataFrame(returns_data)
    returns_df.replace('0%', 'NA', inplace=True)
    return returns_df

def get_portfolio_report(
        actual_portfolio: pd.DataFrame, 
        cash_value: float, 
        total_holdings: float
    ) -> pd.DataFrame:
    actual_portfolio = actual_portfolio.rename(columns={
        'trading_symbol': 'SECURITY',
        'quantity': 'QUANTITY',
        'market_value': 'MARKET VALUE',
    })
    actual_portfolio['MARKET VALUE'] = actual_portfolio['MARKET VALUE'].astype(float)
    actual_portfolio['QUANTITY'] = actual_portfolio['QUANTITY'].astype(float)
    actual_portfolio = actual_portfolio.sort_values(by='MARKET VALUE', ascending=False)
    actual_portfolio['ASSETS'] = (actual_portfolio['MARKET VALUE'] / total_holdings) * 100
    actual_portfolio['ASSETS'] = actual_portfolio['ASSETS'].round(1)
    actual_portfolio['ASSETS'] = actual_portfolio['ASSETS'].astype(str) + '%'
    actual_portfolio['SECURITY'] = actual_portfolio['SECURITY'].str.replace(r' (EQ|MF)$', '', regex=True)
    actual_portfolio = actual_portfolio.sort_values(by='MARKET VALUE', ascending=False)

    print(cash_value, total_holdings)
    merge_data = {
        'SECURITY': ['TOTAL HOLDINGS VALUE', 'CASH', 'TOTAL PORTFOLIO VALUE'],
        'QUANTITY': [0, 0, 0],
        'MARKET VALUE': [
            actual_portfolio['MARKET VALUE'].sum(), 
            cash_value, 
            total_holdings
        ],
        'ASSETS': [
            '0%', 
            f'{int(((cash_value / total_holdings) * 100))}%', 
            '0%'
        ] 
    }
    merged_df = pd.DataFrame(merge_data)
    actual_portfolio = pd.concat([actual_portfolio, merged_df], ignore_index=True)
    actual_portfolio[['MARKET VALUE']] = actual_portfolio[['MARKET VALUE']].astype(int)
    actual_portfolio.replace(0, '', inplace=True)
    return actual_portfolio

def generate_report(
        portfolio_report: pd.DataFrame,
        portfolio_summary: pd.DataFrame,
        returns_df: pd.DataFrame, 
        logo_path: str,
        down_design_path: str,
        account_name: str,
        broker_code: str,
        acc_start_date: str
    ) -> bytes:
    page_width = 18
    base_page_height = 20
    page_dpi = 50
    page_margin = 0

    num_table_rows = 28
    table_row_height = 20
    table_height_in_inches = 11.2
    total_page_height = 31.2

    downDesign_base64 = read_image_as_base64(down_design_path)
    logo_base64 = read_image_as_base64(logo_path)
    account_name = account_name.strip()
    broker_code = broker_code.strip()

    # CREATE SUBPLOT
    fig = make_subplots(
        rows=2, cols=1,
        row_heights=[0.5, 0.6],
        vertical_spacing=0.2,
        specs=[[{"type": "xy"}], [{"type": "domain"}]]
    )

    fig.update_layout(
        width=page_width * page_dpi,
        height=total_page_height * page_dpi,
        margin=dict(l=page_margin, r=page_margin, t=page_margin, b=page_margin),
        paper_bgcolor='white',
        plot_bgcolor='white',
        xaxis=dict(visible=False),
        yaxis=dict(visible=False)
    )

    # Add down design
    fig.add_layout_image(
        dict(
            source=f"data:image/png;base64,{downDesign_base64}",
            xref='paper', yref='paper',
            x=1, y=0,
            sizex=0.25, sizey=0.25,
            xanchor='right', yanchor='bottom',
            layer='below'
        )
    )

    # ADD HORIZONTAL BLACK TOP LINE
    fig.add_shape(
        type='line',
        x0=0.015, y0=0.937,
        x1=0.985, y1=0.937,
        xref='paper', yref='paper',
        line=dict(color='black', width=3)
    )

    # ADD VERTICAL RED TOP LINE
    fig.add_shape(
        type='line',
        x0=0.65, y0=0.993,
        x1=0.65, y1=0.95,
        xref='paper', yref='paper',
        line=dict(color='red', width=3)
    )

    # ADD PLUS91 LOGO
    fig.add_layout_image(
        dict(
            source=f"data:image/png;base64,{logo_base64}",
            xref='paper', yref='paper',
            x=0.63, y=0.952,
            sizex=0.065, sizey=0.065,
            xanchor='right', yanchor='bottom',
            layer='below'
        )
    )

    # ADD RED BOX
    fig.add_shape(
        type='rect',
        x0=0.015, y0=0.87,
        x1=0.49, y1=0.925,
        xref='paper', yref='paper',
        line=dict(color='black', width=2),
        fillcolor='rgba(255,0,0,0.5)',
        opacity=1
    )

    # ADD TWRR TABLE
    #############################################################################################################
    twrr_values = [returns_df[col].tolist() for col in returns_df.columns]

    fig.add_trace(
        go.Table(
            columnwidth=[0.25, 0.25, 0.25, 0.25],
            header=dict(
                values=["<b>RETURNS</b>", "<b>FY 24-25 %</b>", "<b>ABSOLUTE RETURNS %</b>", "<b>SINCE INCEPTION CAGR %</b>"],
                fill_color='rgba(211, 211, 211, 0.2)',
                align='center',
                line_color='black',
                height=25,
                font=dict(size=11, color='black', family='Arial')
            ),
            cells=dict(
                values=twrr_values,
                fill_color='rgba(0,0,0,0)',
                align=['center', 'center', 'center', 'center'],
                line_color='black',
                height=25,
                font=dict(
                    size=11,
                    color='black',
                    family='Arial'
                )
            ),
            domain=dict(x=[0.02, 0.49], y=[0.1, 0.716])  # Adjust domain to position the table correctly
        )
    )



    # ADD PORTFOLIO SUMMARY
    #############################################################################################################
    portfolio_summary_values = [portfolio_summary.index.tolist(), portfolio_summary.iloc[:, 0].tolist()]

    fig.add_trace(
        go.Table(
            columnwidth=[0.7, 0.3],
            header=dict(
                values=["<b>PORTFOLIO SUMMARY", ""],
                fill_color='rgba(211, 211, 211, 0.2)',
                align='left',
                line_color='black',
                height=25,
                font=dict(size=11, color='black', family='Arial')
            ),
            cells=dict(
                values=portfolio_summary_values,
                fill_color='rgba(0,0,0,0)',
                align=['left', 'right'],
                line_color='black',
                height=25,
                font=dict(
                    size=11,
                    color='black',
                    family='Arial'
                )
            ),
            domain=dict(x=[0.02, 0.49], y=[0.25, 0.815])
        )
    )


    # ADD PORTFOLIO TABLE
    #############################################################################################################

    light_grey = 'rgba(211, 211, 211, 0.35)'

    values = [portfolio_report[col].tolist() for col in portfolio_report.columns]
    values[0][-1] = f"<b>{values[0][-1]}</b>"
    values[2][-1] = f"<b>{values[2][-1]}</b>"

    fig.add_trace(
        go.Table(
            columnwidth=[0.4, 0.2, 0.27, 0.2],
            header=dict(
                values=["<b>SECURITY</b>", "<b>QUANTITY</b>", "<b>MARKET VALUE</b>", "<b>ASSETS</b>"],
                fill_color='rgba(211, 211, 211, 0.2)',
                align='center',
                line_color='black',
                height=25,
                font=dict(size=11, color='black', family='Arial')
            ),
            cells=dict(
                values=[portfolio_report['SECURITY'], portfolio_report['QUANTITY'], portfolio_report['MARKET VALUE'], portfolio_report['ASSETS'].round(1)],
                fill_color='rgba(0,0,0,0)',
                align=['left', 'right', 'right', 'right'],
                line_color='black',
                height=20,
                font=dict(
                    size=11,
                    color='black',
                    family='Arial',
                    )
            ),
            domain=dict(x=[0.51, 0.985], y=[0, min(1.1, 0.925)])
        )
    )

    # ADD HEADING RECTANGLE
    fig.add_shape(
        type="rect",
        x0=0.015, y0=0.99, x1=0.5, y1=0.95,
        line=dict(color="black", width=3),
        fillcolor="rgba(0, 0, 0, 0)",
        layer='below'
    )

    # ADD PLUS91 ASSET MANAGEMENT HEADING TEXT
    fig.add_annotation(
        x=0.02, y=0.99,
        text=f'PLUS91 ASSET MANAGEMENT',
        showarrow=False,
        font=dict(family='Arial', size=25, color='black'),
        yanchor='top',
        xanchor='left',
        bgcolor=None,
        opacity=1
    )

    # ADD AS OF: HEADING
    today_date = date.today().strftime('%d-%m-%Y')
    fig.add_annotation(
        x=0.02, y=0.968,
        text=f'AS OF: {today_date}',
        showarrow=False,
        font=dict(family='Arial', size=19, color='black'),
        yanchor='top',
        xanchor='left',
        bgcolor=None,
        opacity=1
    )

    # ADD EXPERTISE YOU NEED
    fig.add_annotation(
        x=0.7, y=0.985,
        text=f'EXPERTISE YOU NEED',
        showarrow=False,
        font=dict(family='Arial', size=15, color='black'),
        yanchor='top',
        xanchor='left',
        bgcolor=None,
        opacity=1
    )

    # ADD SERVICE YOU DESERVE
    fig.add_annotation(
        x=0.68, y=0.972,
        text=f'SERVICE YOU DESERVE',
        showarrow=False,
        font=dict(family='Arial', size=18, color='black'),
        yanchor='top',
        xanchor='left',
        bgcolor=None,
        opacity=1
    )

    # ADD REDBOX DETAILS
    # Add Strategy Name
    fig.add_annotation(
        x=0.02, y=0.922,
        text=f'STRATEGY: PLUS91 CUSTOMIZED PORTFOLIO',
        showarrow=False,
        font=dict(family='Arial', size=14, color='black'),
        yanchor='top',
        xanchor='left',
        bgcolor=None,
        opacity=1
    )

    client_name = f"{account_name.upper()} {broker_code}"
    # Add Account
    fig.add_annotation(
        x=0.02, y=0.906,
        text=f'ACCOUNT: {client_name}',
        showarrow=False,
        font=dict(family='Arial', size=14, color='black'),
        yanchor='top',
        xanchor='left',
        bgcolor=None,
        opacity=1
    )

    # Add Inception Date
    fig.add_annotation(
        x=0.02, y=0.889,
        text=f'INCEPTION DATE: {acc_start_date}',
        showarrow=False,
        font=dict(family='Arial', size=14, color='black'),
        yanchor='top',
        xanchor='left',
        bgcolor=None,
        opacity=1
    )

    # ADD INVESTMENT OBJECTIVE TITLE:
    fig.add_annotation(
        x=0.02, y=0.86,
        text=f"<span style='text-decoration:underline;'>INVESTMENT OBJECTIVE</span>:",
        showarrow=False,
        font=dict(family='Arial', size=14, color='black'),
        yanchor='top',
        xanchor='left',
        bgcolor=None,
        opacity=1
    )

    # ADD INVESTMENT OBJECTIVE CONTENT:
    fig.add_annotation(
        x=0.02, y=0.845,
        text=f"To achieve greater than average market return with customized<br>baskets based on the client's risk profile and objectives.",
        showarrow=False,
        align='left',
        font=dict(family='Arial', size=12, color='black'),
        yanchor='top',
        xanchor='left',
        bgcolor=None,
        opacity=1
    )

    # ADD RETURNS STATEMENT:
    fig.add_annotation(
        x=0.02, y=0.635,
        text=f"Portfolio returns are after advisory fees and other expenses.",
        showarrow=False,
        align='left',
        font=dict(family='Arial', size=12, color='black'),
        yanchor='top',
        xanchor='left',
        bgcolor=None,
        opacity=1
    )


    # ADD MANAGER DETAILS:
    fig.add_annotation(
        x=0.015, y=0.02,
        text=f"plus91.co    parth@plus91.co     +91-9930795667",
        showarrow=False,
        align='left',
        font=dict(family='Arial', size=18, color='black'),
        yanchor='top',
        xanchor='left',
        bgcolor=None,
        opacity=1
    )
    
    fig.update_xaxes(visible=False)
    fig.update_yaxes(visible=False)

    pdf_bytes = fig.to_image(format="pdf", engine="kaleido", scale=3)
    return pdf_bytes


================================================================================
# File: app/scripts/report_generation/data_feeder.py
================================================================================

import asyncio
from sqlalchemy.orm import aliased
from sqlalchemy import select, func, literal, union_all

from app.models.accounts.single_account import SingleAccount
from app.models.accounts.joint_account import JointAccount
from app.models.clients.client_details import Client
from app.models.clients.distributor_details import Distributor
from app.models.clients.broker_details import Broker
from app.models.accounts.account_cashflow_details import AccountCashflow
from app.models.accounts.account_time_periods import AccountTimePeriods
from app.models.accounts.account_actual_portfolio import AccountActualPortfolio
from app.models.accounts.account_ideal_portfolio import AccountIdealPortfolio
from app.models.accounts.account_performance import AccountPerformance
from app.models.accounts.joint_account_mapping import JointAccountMapping
from app.models.portfolio.portfolio_template_details import PortfolioTemplate
from app.models.portfolio.pf_bracket_basket_allocation import PfBracketBasketAllocation
from app.models.portfolio.portfolio_basket_mapping import PortfolioBasketMapping
from app.models.portfolio.bracket_details import Bracket
from app.models.portfolio.basket_details import Basket
from app.models.portfolio.basket_stock_mapping import BasketStockMapping
from app.database import AsyncSessionLocal

sa = aliased(SingleAccount, name='single_account_1')
ja = aliased(JointAccount, name='joint_account_1')
cd = aliased(Client, name='client_details_1')
ap = aliased(AccountActualPortfolio, name='account_actual_portfolio_1')
perf = aliased(AccountPerformance, name='account_performance_1')
jam = aliased(JointAccountMapping, name='joint_account_mapping_1')

latest_ap_single = select(
    AccountActualPortfolio.owner_id,
    func.max(AccountActualPortfolio.snapshot_date).label('latest_date')
).where(AccountActualPortfolio.owner_type == 'single'
).group_by(AccountActualPortfolio.owner_id
).subquery('latest_ap_single')

latest_ap_joint = select(
    AccountActualPortfolio.owner_id,
    func.max(AccountActualPortfolio.snapshot_date).label('latest_date')
).where(AccountActualPortfolio.owner_type == 'joint'
).group_by(AccountActualPortfolio.owner_id
).subquery('latest_ap_joint')

single_query = select(
    literal('single').label('account_type'),
    sa.single_account_id.label('account_id'),
    sa.account_name,
    cd.acc_start_date,
    sa.cash_value,
    sa.invested_amt,
    sa.pf_value,
    sa.total_holdings,
    ap.trading_symbol,
    ap.quantity,
    ap.market_value,
    ap.snapshot_date,
    perf.total_twrr,
    perf.current_yr_twrr,
    perf.cagr,
    cd.broker_code.label('broker_codes')
).join(
    cd, sa.single_account_id == cd.account_id
).join(
    latest_ap_single, sa.single_account_id == latest_ap_single.c.owner_id
).outerjoin(
    perf, (sa.single_account_id == perf.owner_id) & (perf.owner_type == 'single')
).outerjoin(
    ap, (sa.single_account_id == ap.owner_id) &
        (ap.owner_type == 'single') &
        (ap.snapshot_date == latest_ap_single.c.latest_date)
)

# Joint account query with explicit joins
joint_query = select(
    literal('joint').label('account_type'),
    ja.joint_account_id.label('account_id'),
    ja.joint_account_name.label('account_name'),
    func.min(cd.acc_start_date).label('acc_start_date'),
    ja.cash_value,
    ja.invested_amt,
    ja.pf_value,
    ja.total_holdings,
    ap.trading_symbol,
    ap.quantity,
    ap.market_value,
    ap.snapshot_date,
    perf.total_twrr,
    perf.current_yr_twrr,
    perf.cagr,
    func.string_agg(cd.broker_code, ', ').label('broker_codes')
).join(
    jam, ja.joint_account_id == jam.joint_account_id
).join(
    sa, jam.account_id == sa.single_account_id
).join(
    cd, sa.single_account_id == cd.account_id
).join(
    latest_ap_joint, ja.joint_account_id == latest_ap_joint.c.owner_id
).outerjoin(
    perf, (ja.joint_account_id == perf.owner_id) & (perf.owner_type == 'joint')
).outerjoin(
    ap, (ja.joint_account_id == ap.owner_id) &
        (ap.owner_type == 'joint') &
        (ap.snapshot_date == latest_ap_joint.c.latest_date)
).group_by(
    ja.joint_account_id,
    ja.joint_account_name,
    ja.cash_value,
    ja.invested_amt,
    ja.pf_value,
    ja.total_holdings,
    ap.trading_symbol,
    ap.quantity,
    ap.market_value,
    ap.snapshot_date,
    perf.total_twrr,
    perf.current_yr_twrr,
    perf.cagr
)

combined_query = union_all(single_query, joint_query)

async def report_datafeeder():
    async with AsyncSessionLocal() as session:
        result = await session.execute(combined_query)
        rows = result.mappings().all()
        report_data = [dict(row) for row in rows]
        return report_data



================================================================================
# File: app/scripts/data_fetchers/broker_data.py
================================================================================

import requests
from typing import List, Dict

class BrokerData:
    """Class containing static methods to interact with the Upstox API."""
    
    BASE_URL = "http://13.127.138.190:8000/api/v1/upstox/"
    
    @staticmethod
    def get_master_data() -> Dict:
        """
        Fetches the master data from the Upstox API.
        
        Returns:
            The JSON response from the API as a Python dictionary.
        
        Raises:
            Exception: If the API call fails, with the status code and error message.
        """
        url = f"{BrokerData.BASE_URL}master-data"
        response = requests.get(url)
        if response.status_code != 200:
            raise Exception(f"Failed to get master data: {response.status_code} - {response.text}")
        return response.json()
    
    @staticmethod
    def get_ltp_quote(instruments: List[Dict[str, str]]) -> Dict:
        """
        Fetches the Last Traded Price (LTP) quote for the given instruments.
        
        Args:
            instruments: A list of dictionaries, each with 'exchange_token' and 'exchange' keys.
        
        Returns:
            The JSON response from the API as a Python dictionary.
        
        Raises:
            Exception: If the API call fails, with the status code and error message.
        """
        url = f"{BrokerData.BASE_URL}ltp-quote"
        headers = {"Content-Type": "application/json"}
        response = requests.post(url, json=instruments, headers=headers)
        if response.status_code != 200:
            raise Exception(f"Failed to get LTP quote: {response.status_code} - {response.text}")
        return response.json()
    
    @staticmethod
    def historical_data(instrument: Dict) -> Dict:
        """
        Fetches the Last Traded Price (LTP) quote for the given instruments.
        
        Args:
            instruments: A list of dictionaries, each with 'exchange_token' and 'exchange' keys.
        
        Returns:
            The JSON response from the API as a Python dictionary.
        
        Raises:
            Exception: If the API call fails, with the status code and error message.
        """
        url = f"{BrokerData.BASE_URL}historical-data"
        headers = {"Content-Type": "application/json"}
        response = requests.post(url, json=instrument, headers=headers)
        if response.status_code != 200:
            raise Exception(f"Failed to get historical data: {response.status_code} - {response.text}")
        return response.json()
    


================================================================================
# File: app/scripts/data_fetchers/data_transformer.py
================================================================================

import time
import boto3
from io import BytesIO
import pandas as pd
from datetime import datetime, timedelta
from app.scripts.data_fetchers.broker_data import BrokerData
from app.scripts.data_fetchers.portfolio_data import KeynoteApi, ZerodhaDataFetcher
from typing import Dict, Optional, Tuple

broker_data = BrokerData()
master_data = broker_data.get_master_data()
upstox_master_df = pd.DataFrame(master_data["data"]) if master_data and "data" in master_data else pd.DataFrame()


class KeynoteDataTransformer:
    def __init__(self):
        """Initialize with KeynoteApi instance."""
        self.keynote_portfolio = KeynoteApi()

    async def transform_ledger_to_cashflow(
            self,
            broker_code: str,
            from_date: str,
            to_date: str
    ) -> Optional[Dict]:
        """Transform ledger data into cashflow format."""
        ledger_data = await self.keynote_portfolio.fetch_ledger(
            ucc=broker_code,
            from_date=from_date,
            to_date=to_date
        )
        if not ledger_data:
            print(f"Ledger data for {broker_code}, from: {from_date}, to: {to_date} was not found.")
            return None

        ledger_df = pd.DataFrame(ledger_data)
        if ledger_df.empty:
            print(f"Ledger data for {broker_code}, from: {from_date}, to: {to_date} is empty.")
            return None

        ledger_df_slice = ledger_df[ledger_df["type"].isin(["C"])]
        ledger_df_open_bal = ledger_df[ledger_df["type"].isin(["A"])]

        if ledger_df.empty:
            print(f"No cash transactions found in ledger data for {broker_code}.")
            return None

        try:
            cashflow_df = pd.DataFrame()
            cashflow_df["event_date"] = pd.to_datetime(ledger_df_slice["vrdt"]).dt.date
            cashflow_df["cashflow"] = ledger_df_slice["amtcr"].where(
                ledger_df_slice["amtcr"] > 0, -ledger_df_slice["amtdr"]
            ).astype(float)
            cashflow_df["tag"] = ""

            ledger_df['vrdt'] = pd.to_datetime(ledger_df['vrdt'])
            latest_row = ledger_df.sort_values('vrdt').iloc[-1]
            latest_balance = float(latest_row['runbal'])
            cashflow_df = cashflow_df.reset_index(drop=True)
            return cashflow_df.to_dict()
        except Exception as e:
            print(f"Error transforming ledger to cashflow for {broker_code}: {e}")
            return None

    async def transform_holdings_to_actual_portfolio(
            self,
            broker_code: str,
            for_date: str
        ) -> Optional[Dict]:
        """
        Transform holdings data into actual portfolio format.
        First attempts to fetch data from the Keynote API, and if that fails,
        falls back to fetching the latest data from S3 for the same month and year.

        Args:
            broker_code (str): The UCC (e.g., 'MK100').
            for_date (str): The date in 'YYYY-MM-DD' format (e.g., '2023-07-12').

        Returns:
            Optional[Dict]: A dictionary with aggregated holdings data or None if no data is found.
        """
        holdings_data = await self.keynote_portfolio.fetch_holding(
            ucc=broker_code,
            for_date=for_date
        )
        if not holdings_data:
            print(f"Holdings for {broker_code} at date: {for_date} could not be found.")
        else:
            holdings_df = pd.DataFrame(holdings_data)
            if not holdings_df.empty and not for_date.startswith("2024-03"):
                try:
                    holdings_df["quantity"] = (holdings_df["holding"] / holdings_df["closerate"]).round(2)
                    holdings_df = holdings_df[["isincd", "quantity", "holding"]]
                    holdings_df = holdings_df.rename(columns={
                        "isincd": "isin",
                        "holding": "market_value"
                    })
                    slice_master_df = upstox_master_df[upstox_master_df["segment"].isin(["NSE_EQ", "BSE_EQ"])]
                    holdings_df = holdings_df.merge(slice_master_df[["isin", "trading_symbol"]], on="isin", how="left")
                    holdings_df["trading_symbol"] = holdings_df["trading_symbol"].fillna(holdings_df["isin"]).drop_duplicates()
                    holdings_df = holdings_df.groupby("trading_symbol")[["quantity", "market_value"]].sum().reset_index()
                    
                    # upstox_master_df_slice = upstox_master_df[
                    #     (upstox_master_df["exchange"].isin(["NSE", "BSE"])) &
                    #     (upstox_master_df["instrument_type"] == "EQ")
                    # ][["trading_symbol", "exchange", "exchange_token", "instrument_type"]]
                    
                    # upstox_master_df_slice_sorted = upstox_master_df_slice.sort_values(
                    #     by=["trading_symbol", "exchange"],
                    #     ascending=[True, True]
                    # )
                    # upstox_master_df_slice = upstox_master_df_slice_sorted.drop_duplicates(
                    #     subset="trading_symbol", keep="first"
                    # )
                    # mapping_data = upstox_master_df_slice[
                    #     upstox_master_df_slice["trading_symbol"].isin(list(holdings_df['trading_symbol']))
                    # ].reset_index()

                    # to_date = pd.to_datetime(for_date)
                    # from_date = to_date - timedelta(days=10)
                    # for index, data in mapping_data.iterrows():
                    #     try:
                    #         print(data)
                    #         hist_data = broker_data.historical_data(
                    #             instrument={
                    #                 "exchange": data['exchange'],
                    #                 "exchange_token": data['exchange_token'],
                    #                 "instrument_type": data['instrument_type'],
                    #                 "interval": "day",
                    #                 "from_date": str(from_date.date()),
                    #                 "to_date": str(to_date.date())
                    #             }
                    #         )
                    #         hist_df = pd.DataFrame(hist_data['data'])
                    #         closest_row = hist_df.iloc[(hist_df['datetime'] - pd.to_datetime(for_date)).abs().argsort()[0]]
                    #         print(closest_row)
                            
                    #         time.sleep(1)

                    #     except Exception as e:
                    #         print(f"Historical Data not found for {data}")

                    print(f"Data fetched from API for {broker_code} on {for_date}")
                    return holdings_df.to_dict()
                except Exception as e:
                    print(f"Error transforming API holdings for {broker_code}: {e}")

        print(f"No data from API or transformation failed for {broker_code} on {for_date}. Falling back to S3...")

        bucket_name = 'plus91backoffice'
        prefix = f"PLUS91_PMS/ledgers_and_holdings/keynote/single_accounts/{broker_code}/holdings/"

        for_date_dt = datetime.strptime(for_date, "%Y-%m-%d")
        year = for_date_dt.year
        month = for_date_dt.month
        month_prefix = f"{year}-{month:02d}-"

        s3 = boto3.client('s3')
        response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)
        if 'Contents' not in response:
            print(f"No files found in S3 at {prefix}")
            return None

        files = [
            obj['Key'] for obj in response['Contents']
            if obj['Key'].endswith('.xlsx') and month_prefix in obj['Key']
        ]

        if not files:
            print(f"No files found for {year}-{month:02d} in S3 for {broker_code}")
            return None

        latest_file = max(files, key=lambda x: x.split('/')[-1].replace('.xlsx', ''))
        latest_date = latest_file.split('/')[-1].replace('.xlsx', '')
        print(f"Found latest file: {latest_file} for {broker_code}")

        obj = s3.get_object(Bucket=bucket_name, Key=latest_file)
        file_content = obj['Body'].read()
        holdings_df = pd.read_excel(BytesIO(file_content))
        holdings_df = holdings_df.groupby("trading_symbol")[["quantity", "market_value"]].sum().reset_index()
        print(f"Processed S3 data from {latest_date} for {broker_code}")
        return holdings_df.to_dict()


class ZerodhaDataTransformer:
    def __init__(self):
        """Initialize with ZerodhaDataFetcher instance."""
        self.zerodha_portfolio = ZerodhaDataFetcher()

    async def transform_ledger_to_cashflow(
            self,
            broker_code: str
        ) -> Optional[Dict]:
        """Transform Zerodha ledger data into cashflow format."""
        ledger_data = self.zerodha_portfolio.get_ledger(
            broker_code=broker_code
        )
        if not ledger_data:
            print(f"Ledger data for {broker_code} could not be fetched.")
            return None

        ledger_df = pd.DataFrame(ledger_data)
        if ledger_df.empty:
            print(f"Ledger data for {broker_code} is empty.")
            return None

        ledger_df_slice = ledger_df[ledger_df['Voucher Type'].isin(['Bank Receipts', 'Bank Payments'])]
        if ledger_df.empty:
            print(f"No cash transactions found in ledger data for {broker_code}.")
            return None

        try:
            inflow_df = ledger_df_slice[["Posting Date", "Credit"]].rename(columns={"Posting Date": "event_date", "Credit": "cashflow"})
            inflow_df = inflow_df[inflow_df['cashflow'] != 0]

            outflow_df = ledger_df_slice[["Posting Date", "Debit"]].rename(columns={"Posting Date": "event_date", "Debit": "cashflow"})
            outflow_df['cashflow'] = -outflow_df['cashflow']
            outflow_df = outflow_df[outflow_df['cashflow'] != 0]

            cashflow_df = pd.concat([inflow_df, outflow_df], ignore_index=True)
            cashflow_df["event_date"] = pd.to_datetime(cashflow_df["event_date"]).dt.date
            cashflow_df["cashflow"] = cashflow_df["cashflow"].astype(float)
            cashflow_df["tag"] = ""

            ledger_df['Posting Date'] = pd.to_datetime(ledger_df['Posting Date'])
            latest_row = ledger_df.sort_values('Posting Date').iloc[-1]
            latest_balance = float(latest_row['Net Balance'])
            cashflow_df = cashflow_df.reset_index(drop=True)
            return cashflow_df.to_dict()
        except Exception as e:
            print(f"Error transforming Zerodha ledger to cashflow for {broker_code}: {e}")
            return None

    async def transform_holdings_to_actual_portfolio(
            self,
            broker_code: str,
            month: int,
            year: int
    ) -> Optional[Dict]:
        """Transform Zerodha holdings data into actual portfolio format."""
        holdings_data = self.zerodha_portfolio.get_holdings(
            broker_code=broker_code,
            month=month,
            year=year
        )
        if not holdings_data:
            print(f"Holdings for {broker_code} in {year}-{month:02d} could not be found.")
            return None

        holdings_df = pd.DataFrame(holdings_data)
        if holdings_df.empty:
            print(f"Holdings data for {broker_code} in {year}-{month:02d} is empty.")
            return None

        holdings_df = holdings_df[holdings_df["Unrealized P&L"] != 0]
        if holdings_df.empty:
            print(f"No holdings with unrealized P&L for {broker_code} in {year}-{month:02d}.")
            return None

        try:
            holdings_df["quantity"] = (
                    holdings_df['Quantity Available'] + holdings_df['Quantity Pledged (Margin)']
            )
            holdings_df["market_value"] = holdings_df['quantity'] * holdings_df['Previous Closing']
            holdings_df = holdings_df[["Symbol", "quantity", "market_value"]].rename(columns={"Symbol": "trading_symbol"})
            holdings_df = holdings_df.drop_duplicates().dropna(subset=["trading_symbol"])
            holdings_df["trading_symbol"] = holdings_df["trading_symbol"].str.split("-").str[0]
            holdings_df = holdings_df.groupby("trading_symbol")[["quantity", "market_value"]].sum().reset_index()
            return holdings_df.to_dict()
        except Exception as e:
            print(f"Error transforming Zerodha holdings to portfolio for {broker_code}: {e}")
            return None

================================================================================
# File: app/scripts/data_fetchers/__init__.py
================================================================================



================================================================================
# File: app/scripts/data_fetchers/portfolio_data.py
================================================================================

import os
import io
import boto3
import httpx
import asyncio
import datetime
import pandas as pd
from datetime import datetime
from dotenv import load_dotenv
from app.scripts.data_fetchers.broker_data import BrokerData
from app.logger import logger
from typing import Dict


load_dotenv()
pd.set_option('future.no_silent_downcasting', True)

class ApiError(Exception):
    """Custom exception for API errors."""
    pass

class KeynoteApi:
    def __init__(self):
        """Initialize the API client with base URL, headers, and API key."""
        self.BASE_URL = "https://backoffice.wizzer.in/shrdbms/dotnet/api/stansoft/"
        self.HEADERS = {"Content-Type": "application/json"}
        self.API_KEY = os.getenv("SHAREPRO_WIZZER_API_KEY")

    async def fetch_holding(self, for_date: str, ucc: str) -> Dict:
        """
        Fetch holding data for a specific date and UCC.
        Returns a pandas DataFrame if successful, None otherwise.
        """
        url = self.BASE_URL + "GetDpHoldingData"
        payload = {
            "key": self.API_KEY,
            "ucc": ucc,
            "segments": "NSDL,CDSL,NFO,NSE,BSE",
            "date": datetime.strptime(for_date, "%Y-%m-%d").strftime("%d-%m-%Y")
        }
        try:
            data = await self._fetch_api_data(url, payload)
            return data.get("curdata", {})
        except ApiError as e:
            logger.error(f"Error fetching holding data: {e}")
            return None

    async def fetch_ledger(self, from_date: str, to_date: str, ucc: str) -> Dict:
        """
        Fetch ledger data between two dates for a UCC.
        Returns a pandas DataFrame if successful, None otherwise.
        """
        params = self._generate_ledger_params(from_date, to_date, ucc)
        all_transactions = []
        for p in params:
            url = self.BASE_URL + "ClientLedgerData"
            payload = {
                "key": self.API_KEY,
                "datefrom": p["datefrom"],
                "dateto": p["dateto"],
                "segments": "NSE,BSE,NFO",
                "ucc": p["ucc"],
                "accyear": p["accyear"]
            }
            try:
                data = await self._fetch_api_data(url, payload)
                if "curdata" in data:
                    all_transactions.extend(data["curdata"])
            except ApiError as e:
                logger.info(f"Error fetching ledger data for {p['datefrom']} to {p['dateto']}: {e}")
                continue
        if not all_transactions:
            return None
        
        all_transactions_df = pd.DataFrame(all_transactions)
        all_transactions_df["vrdt"] = pd.to_datetime(all_transactions_df["vrdt"]).dt.date
        return all_transactions_df.to_dict()
    
    def _generate_ledger_params(self, from_date: str, to_date: str, ucc: str):
        """
        Generate parameters for ledger API calls, splitting by financial years.
        Returns a list of parameter dictionaries.
        """
        from_date_dt = datetime.strptime(from_date, "%Y-%m-%d")
        to_date_dt = datetime.strptime(to_date, "%Y-%m-%d")
        params = []
        
        first_period_end = datetime(from_date_dt.year, 3, 31)
        if from_date_dt > first_period_end:
            first_period_end = datetime(from_date_dt.year + 1, 3, 31)
        if first_period_end > to_date_dt:
            first_period_end = to_date_dt
        
        params.append({
            "datefrom": from_date_dt.strftime("%d-%m-%Y"),
            "dateto": first_period_end.strftime("%d-%m-%Y"),
            "accyear": f"{from_date_dt.year % 100}{(from_date_dt.year + 1) % 100}",
            "ucc": ucc
        })
        
        while first_period_end < to_date_dt:
            next_period_start = datetime(first_period_end.year, 4, 1)
            next_period_end = datetime(first_period_end.year + 1, 3, 31)
            if next_period_end > to_date_dt:
                next_period_end = to_date_dt
            params.append({
                "datefrom": next_period_start.strftime("%d-%m-%Y"),
                "dateto": next_period_end.strftime("%d-%m-%Y"),
                "accyear": f"{next_period_start.year % 100}{(next_period_start.year + 1) % 100}",
                "ucc": ucc
            })
            first_period_end = next_period_end
        
        return params

    async def _fetch_api_data(self, url: str, payload: dict):
        """
        Helper function to fetch API data with retries and error handling.
        Returns a dictionary if successful, raises ApiError otherwise.
        """
        tries = 3
        for attempt in range(tries):
            try:
                async with httpx.AsyncClient(timeout=30) as client:
                    response = await client.post(url, headers=self.HEADERS, json=payload)
                    
                    if response.status_code != 200:
                        raise ApiError(f"Server error—status code {response.status_code}")
                    
                    content_type = response.headers.get('Content-Type', '').lower()
                    if 'application/json' not in content_type:
                        raise ApiError(f"Expected JSON, got {content_type or 'no content type'}")
                    
                    try:
                        data = response.json()
                    except ValueError:
                        raise ApiError("Response is not JSON")
                    
                    if not isinstance(data, dict):
                        raise ApiError(f"Expected dict, got {type(data).__name__}")
                    
                    return data
            
            except httpx.ReadTimeout:
                logger.info(f"Timeout on attempt {attempt + 1}/{tries}")
                if attempt < tries - 1:
                    await asyncio.sleep(5)
                else:
                    raise ApiError("Max retries reached, giving up")
            except httpx.HTTPStatusError as e:
                raise ApiError(f"HTTP error: {e.response.status_code} - {e.response.text}")
            except Exception as e:
                raise ApiError(f"Unexpected error: {str(e)}")

class ZerodhaDataFetcher:
    def __init__(self, bucket_name="plus91backoffice", 
                 base_prefix="PLUS91_PMS/ledgers_and_holdings/zerodha/single_accounts/"):
        """Set up the S3 connection with your bucket and base prefix."""
        self.s3 = boto3.client('s3')
        self.bucket_name = bucket_name
        self.base_prefix = base_prefix.rstrip('/') + '/'

    def _get_s3_file_bytes(self, key):
        """Fetch an XLSX file from S3 and return its raw bytes."""
        full_key = self.base_prefix + key
        try:
            obj = self.s3.get_object(Bucket=self.bucket_name, Key=full_key)
            return obj['Body'].read()
        except Exception as e:
            logger.warning(f"Couldn’t grab {full_key}: {e}")
            return None

    def get_ledger(
            self, 
            broker_code: str, 
            as_dataframe=True
            ) -> Dict:
        """
        Fetch the ledger XLSX file for a broker_code from S3.
        Returns raw bytes if as_dataframe=False, or a pandas DataFrame if True.
        """
        key = f"{broker_code}/ledger/ledger-{broker_code}.xlsx"
        file_bytes = self._get_s3_file_bytes(key)
        if file_bytes is None:
            return None
        
        if not as_dataframe:
            return file_bytes
        
        df = pd.read_excel(io.BytesIO(file_bytes), header=None)
        new_column_names = {
                0: 'Index',
                1: 'Particulars',
                2: 'Posting Date',
                3: 'Cost Center',
                4: 'Voucher Type',
                5: 'Debit',
                6: 'Credit',
                7: 'Net Balance'
            }
        df = df.rename(columns=new_column_names)
        df = df.fillna('blank')
        df = df[df["Net Balance"] != "blank"].reset_index(drop=True).drop(columns=["Index"], index=0).reset_index(drop=True)
        df["Posting Date"] = pd.to_datetime(df["Posting Date"].replace("blank", pd.NaT), errors="coerce")
        df["Credit"] = pd.to_numeric(df["Credit"].replace("blank", 0), errors='coerce')
        df["Debit"] = pd.to_numeric(df["Debit"].replace("blank", 0), errors='coerce')
        df["Net Balance"] = pd.to_numeric(df["Net Balance"].replace("blank", 0), errors="coerce")
        return df.to_dict()

    def get_holdings(
            self,
            year: int,
            month: int,
            broker_code: str,
            as_dataframe=True
            ) -> Dict:
        """
        Fetch the latest holdings XLSX file for a given month and year for a broker_code.
        Returns raw bytes if as_dataframe=False, or a list of pandas DataFrames if True.
        """
        target_year_month = f"{year}-{month:02d}"
        prefix = f"{broker_code}/holdings/"
        
        response = self.s3.list_objects_v2(Bucket=self.bucket_name, 
                                          Prefix=self.base_prefix + prefix)
        if 'Contents' not in response:
            logger.warning(f"No holdings files found for {broker_code} under {self.base_prefix + prefix}!")
            return None

        holdings_files = []
        for obj in response['Contents']:
            try:
                file_date_str = obj['Key'].split('/')[-1].replace('.xlsx', '')
                file_date = datetime.strptime(file_date_str, "%Y-%m-%d")
                if file_date.strftime("%Y-%m") == target_year_month:
                    holdings_files.append((obj['Key'].replace(self.base_prefix, ''), file_date))
            except ValueError:
                logger.info(f"Skipping odd file name: {obj['Key']}")

        if not holdings_files:
            logger.warning(f"No holdings files for {target_year_month} under {broker_code}!")
            return None

        latest_file = max(holdings_files, key=lambda x: x[1])
        file_bytes = self._get_s3_file_bytes(latest_file[0])
        if file_bytes is None:
            return None
        
        if not as_dataframe:
            return file_bytes

        df = pd.read_excel(io.BytesIO(file_bytes), header=None)
        new_column_names = {
            1: 'Symbol',
            2: 'ISIN',
            3: 'Sector',
            4: 'Quantity Available',
            5: 'Quantity Discrepant',
            6: 'Quantity Long Term',
            7: 'Quantity Pledged (Margin)',
            8: 'Quantity Pledged (Loan)',
            9: 'Average Price',
            10: 'Previous Closing',
            11: 'Unrealized P&L',
            12: 'Unrealized P&L Pct'
        }
        df = df.rename(columns=new_column_names).drop(columns=0).fillna("blank")
        df = df[df["Unrealized P&L Pct"] != "blank"].reset_index(drop=True).drop(index=0).reset_index(drop=True)
        return df.to_dict()



if __name__ == "__main__":
    fetcher = ZerodhaDataFetcher()


    keynote = KeynoteApi()


    holdingsexample = asyncio.run(keynote.fetch_holding(for_date="2023-01-31", ucc="SG102"))
    df = pd.DataFrame(holdingsexample)
    print(df)

    # ledgerexample = asyncio.run(keynote.fetch_ledger(from_date="2022-11-30", to_date="2025-03-19", ucc="SG102"))
    # ledger_df = pd.DataFrame(ledgerexample)
    # ledger_df.to_csv("LEDGER_SG102.csv")


================================================================================
# File: app/scripts/db_processors/helper_functions.py
================================================================================

import logging
from sqlalchemy import select
from datetime import datetime, timedelta, date
from dateutil.relativedelta import relativedelta
from sqlalchemy.ext.asyncio import AsyncSession
from app.models.accounts.account_actual_portfolio import AccountActualPortfolio
from typing import List

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def _generate_historical_month_ends(acc_start_date: date, today: date) -> List[date]:
    """Generate a list of month-end dates from account start date to today."""
    try:
        if isinstance(acc_start_date, str):
            acc_start_date = datetime.strptime(acc_start_date, "%Y-%m-%d").date()
        elif not isinstance(acc_start_date, date):
            raise ValueError(f"acc_start_date must be a datetime.date or string, got {type(acc_start_date)}")

        if isinstance(today, str):
            today = datetime.strptime(today, "%Y-%m-%d").date()
        elif not isinstance(today, date):
            raise ValueError(f"today must be a datetime.date or string, got {type(today)}")

        historical_dates = []
        current_date = acc_start_date.replace(day=1) + relativedelta(months=1) - timedelta(days=1)
        last_month_end = today.replace(day=1) - timedelta(days=1)
        
        while current_date <= last_month_end:
            historical_dates.append(current_date)
            current_date = (current_date.replace(day=1) + relativedelta(months=2)) - timedelta(days=1)
        return historical_dates
    except ValueError as e:
        logger.error(f"Invalid date format or value in generating historical month ends: {e}")
        return []
    except Exception as e:
        logger.error(f"Error generating historical month ends: {e}")
        return []

async def _get_existing_snapshot_dates(db: AsyncSession, account_id: str) -> list:
    """Fetch existing portfolio snapshot dates for an account."""
    try:
        result = await db.execute(
            select(AccountActualPortfolio.snapshot_date)
            .where(AccountActualPortfolio.owner_id == account_id)
            .where(AccountActualPortfolio.owner_type == 'single')
            .distinct()
        )
        existing_dates = [row[0] for row in result.all()]
        return existing_dates
    except Exception as e:
        logger.error(f"Error fetching existing snapshot dates for account {account_id}: {e}")
        return []


================================================================================
# File: app/scripts/db_processors/cashflow_progression_processor.py
================================================================================

import sys
import logging
import calendar
import pandas as pd
from functools import reduce
from datetime import datetime, timedelta, date
from sqlalchemy import select, func, delete
from sqlalchemy.ext.asyncio import AsyncSession
from app.services.accounts.joint_account_service import JointAccountService
from app.models.accounts.account_cashflow_details import AccountCashflow
from app.models.accounts.account_actual_portfolio import AccountActualPortfolio
from app.models.accounts.account_time_periods import AccountTimePeriods
from app.models.accounts.account_cashflow_progression import AccountCashflowProgression
from app.scripts.db_processors.cashflow_processor import CashflowProcessor
from app.scripts.db_processors.helper_functions import (
    _generate_historical_month_ends, _get_existing_snapshot_dates
)
from typing import List, Dict, Tuple

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class CashflowProgressionProcessor:
    def __init__(self, db: AsyncSession, cashflow_processor: CashflowProcessor):
        self.db = db
        self.cashflow_processor = cashflow_processor

    async def get_month_end_portfolio_df(self, account: dict) -> pd.DataFrame:
        """Generate a DataFrame with month-end portfolio values (portfolio + cash balance) for an account."""
        account_id = account['account_id']
        account_type = account['account_type']
        today = datetime.now().date()

        if account_type == 'single':
            acc_start_date = account['acc_start_date']
            month_ends = _generate_historical_month_ends(acc_start_date, today)
            if not month_ends:
                logger.warning(f"No month-end dates for account {account_id}")
                return pd.DataFrame(columns=['event_date', 'portfolio'])

            portfolio_values, month_ends = await self.get_portfolio_values(account_id, 'single')
            cash_balances = await self.cashflow_processor.get_month_end_cash_balances(account, month_ends)

            data = []
            for month_end in month_ends:
                portfolio_value = portfolio_values.get(month_end, 0)
                cash_balance = cash_balances.get(month_end, 0) if cash_balances else 0
                total_portfolio = portfolio_value + cash_balance
                data.append({'event_date': month_end, 'portfolio': total_portfolio})
            return pd.DataFrame(data)

        elif account_type == 'joint':
            single_accounts = await JointAccountService.get_linked_single_accounts(self.db, account_id)
            if not single_accounts:
                logger.warning(f"No linked single accounts for joint account {account_id}")
                return pd.DataFrame(columns=['event_date', 'portfolio'])

            earliest_start_date = min(acc['acc_start_date'] for acc in single_accounts)
            earliest_start_date
            month_ends = _generate_historical_month_ends(earliest_start_date, today)
            if not month_ends:
                logger.warning(f"No month-end dates for joint account {account_id}")
                return pd.DataFrame(columns=['event_date', 'portfolio'])

            data = []
            for month_end in month_ends:
                total_portfolio = 0
                for single_acc in single_accounts:
                    single_id = single_acc['account_id']
                    portfolio_values, temp_month_end = await self.get_portfolio_values(single_id, 'single', [month_end])
                    cash_balances = await self.cashflow_processor.get_month_end_cash_balances(single_acc, temp_month_end)
                    portfolio_value = portfolio_values.get(month_end, 0)
                    cash_balance = cash_balances.get(month_end, 0) if cash_balances else 0
                    total_portfolio += portfolio_value + cash_balance
                data.append({'event_date': month_end, 'portfolio': total_portfolio})
            return pd.DataFrame(data)

        else:
            raise ValueError(f"Invalid account_type: {account_type}")

    async def get_portfolio_values(self, owner_id: str, owner_type: str, month_ends: list = None) -> Tuple[Dict[date, float], List[date]]:
        """
        Fetch portfolio market values for specified month-end dates or all dates if not provided.

        Args:
            owner_id (str): The ID of the portfolio owner.
            owner_type (str): The type of the portfolio owner.
            month_ends (list, optional): A list of specific dates to filter the results. If None, all dates are included.

        Returns:
            Tuple[Dict[date, float], List[date]]: A tuple containing:
                - A dictionary with snapshot dates as keys and total market values as values.
                - A list of all snapshot dates in ascending order.
        """
        query = (
            select(
                AccountActualPortfolio.snapshot_date,
                func.sum(AccountActualPortfolio.market_value).label("total_value")
            )
            .where(
                AccountActualPortfolio.owner_id == owner_id,
                AccountActualPortfolio.owner_type == owner_type
            )
        )
        if month_ends is not None:
            query = query.where(AccountActualPortfolio.snapshot_date.in_(month_ends))
        query = query.group_by(AccountActualPortfolio.snapshot_date).order_by(AccountActualPortfolio.snapshot_date)
        result = await self.db.execute(query)
        rows = result.all()
        date_value_dict = {row.snapshot_date: row.total_value for row in rows}
        date_list = [row.snapshot_date for row in rows]
        return date_value_dict, date_list

    async def get_progression_df(self, owner_id: str, owner_type: str) -> pd.DataFrame:
        """Fetch cashflow data for an account from the database."""
        try:
            query = (
                select(AccountCashflow.event_date, AccountCashflow.cashflow)
                .where(AccountCashflow.owner_id == owner_id)
                .where(AccountCashflow.owner_type == owner_type)
            )
            result = await self.db.execute(query)
            cashflows = result.all()
            df = pd.DataFrame(
                [(row.event_date, row.cashflow) for row in cashflows],
                columns=['event_date', 'cashflow']
            )
            if df.empty:
                logger.warning(f"No cashflow data found for {owner_type} account {owner_id}")
            return df
        except Exception as e:
            logger.error(f"Error fetching cashflow data for {owner_type} account {owner_id}: {e}")
            return pd.DataFrame(columns=['event_date', 'cashflow'])

    async def get_cashflow_progression_df(self, account: dict) -> pd.DataFrame:
        """Generate a DataFrame with all event dates, cashflows, and portfolio values for an account."""
        account_id = account['account_id']
        account_type = account['account_type']

        if account_type == 'single':
            acc_start_date = pd.to_datetime(account['acc_start_date'])
            acc_start_date_monthend = acc_start_date + pd.offsets.MonthEnd(0)
            fiscal_start_date = acc_start_date.replace(day=1, month=4)

            progression_df = await self.get_progression_df(account_id, 'single')
            if progression_df.empty:
                logger.warning(f"No cashflow data for single account {account_id}")
            
            portfolio_df = await self.get_month_end_portfolio_df(account)

            if portfolio_df.empty:
                logger.warning(f"No portfolio data for single account {account_id}")
            
            if progression_df.empty and portfolio_df.empty:
                return pd.DataFrame(columns=['event_date', 'cashflow', 'portfolio'])
            
            all_dates = pd.concat(
                [progression_df['event_date'], portfolio_df['event_date']]
            ).drop_duplicates().sort_values().reset_index(drop=True)
            
            combined_df = pd.DataFrame({'event_date': all_dates})
        
            combined_df = combined_df.merge(progression_df, on='event_date', how='left')
            combined_df['cashflow'] = combined_df['cashflow'].fillna(0)
            combined_df = combined_df.merge(portfolio_df, on='event_date', how='left').fillna(0)
            
            if account['broker_name'] == 'zerodha':
                combined_df = combined_df[combined_df['event_date'] >= acc_start_date_monthend.date()]

            combined_df = self.get_main_cashflow_progression_df(combined_df)
            return combined_df[['event_date', 'cashflow', 'portfolio', 'portfolio_plus_cash']]

        elif account_type == 'joint':
            single_accounts = await JointAccountService.get_linked_single_accounts(self.db, account_id)
            if not single_accounts:
                logger.warning(f"No linked single accounts for joint account {account_id}")
                return pd.DataFrame(columns=['event_date', 'cashflow', 'portfolio'])
            
            progression_dfs = []
            for single_acc in single_accounts:
                single_acc['account_type'] = 'single'
                df = await self.get_cashflow_progression_df(single_acc)
                if not df.empty:
                    progression_dfs.append(df)
            
            if not progression_dfs:
                logger.warning(f"No progression data for joint account {account_id}")
                return pd.DataFrame(columns=['event_date', 'cashflow', 'portfolio'])
            
            combined_df = pd.concat(progression_dfs, ignore_index=True)
            aggregated_df = combined_df.groupby('event_date').agg({
                'cashflow': 'sum',
                'portfolio': lambda x: x.sum(min_count=1)
            }).reset_index().fillna(0)

            progression_df = self.get_main_cashflow_progression_df(aggregated_df)
            return progression_df[['event_date', 'cashflow', 'portfolio', 'portfolio_plus_cash']]
        
        else:
            raise ValueError(f"Invalid account_type: {account_type}")

    async def update_cashflow_progression_table(
            self, account: dict, 
            cashflow_progression_df: pd.DataFrame,
            ):
        """Update the account_cashflow_progression table for a specific account by overwriting existing data."""
        account_id = account['account_id']
        account_type = account['account_type']

        if cashflow_progression_df['cashflow'].sum() == 0 and cashflow_progression_df['portfolio'].sum() == 0:
                print(f"Skipping cashflow progression update for {account_type} account {account_id}: no meaningful data")
                return

        await self.db.execute(
            delete(AccountCashflowProgression).where(
                (AccountCashflowProgression.owner_id == account_id) &
                (AccountCashflowProgression.owner_type == account_type)
            )
        )

        new_records = []
        for _, row in cashflow_progression_df.iterrows():
            new_record = AccountCashflowProgression(
                owner_id=account_id,
                owner_type=account_type,
                event_date=row['event_date'],
                cashflow=row['cashflow'],
                portfolio_value=row['portfolio'],
                portfolio_plus_cash=row.get('portfolio_plus_cash', 0.0)
            )
            new_records.append(new_record)

        self.db.add_all(new_records)
        await self.db.commit()

        logger.info(f"Updated cashflow progression for {account_type} account {account_id} with {len(new_records)} records")

    async def update_time_periods_table(self, account: dict, time_periods_df: pd.DataFrame):
        """Update the AccountTimePeriods table with calculated time periods."""
        account_id = account['account_id']
        account_type = account['account_type']
        try:
            await self.db.execute(
                delete(AccountTimePeriods).where(
                    AccountTimePeriods.owner_id == account_id,
                    AccountTimePeriods.owner_type == account_type
                )
            )
            new_records = [
                AccountTimePeriods(
                    owner_id=account_id,
                    owner_type=account_type,
                    start_date=row['start_date'],
                    end_date=row['end_date'],
                    start_value=row['start_value'],
                    end_value=row['end_value'],
                    returns=row['returns'],
                    returns_1=row['returns_1']
                ) for _, row in time_periods_df.iterrows()
            ]
            self.db.add_all(new_records)
            await self.db.commit()
            logger.info(f"Updated {len(new_records)} time periods for {account_type} account {account_id}")
        except Exception as e:
            logger.error(f"Error updating time periods for {account_type} account {account_id}: {e}")
            await self.db.rollback()

    def get_time_periods_df(self, cashflow_progression_df: pd.DataFrame) -> pd.DataFrame:
        cashflow_progression_df_v1 = cashflow_progression_df
        abs_twrr_df, absolute_twrr = self._get_twrr(cashflow_progression_df)
        if not absolute_twrr:
            absolute_twrr = 0.0

        financial_year_dfs = self._create_financial_year_dataframes(df=cashflow_progression_df_v1)

        yearly_twrrs = []
        for i, cashflow_progression_df in enumerate(financial_year_dfs):
            cashflow_progression_df = cashflow_progression_df.reset_index(drop=True)
            twrr_df, twrr = self._get_twrrs_for_cagr(cashflow_progression_df)
            print("TWRR: ", twrr)
            twrr_df.to_excel(f"MK100_time_period_{i}.xlsx")
            if not twrr:
                twrr = 0.0
            yearly_twrrs.append(twrr)

        cagrs = [cagr + 1 for cagr in yearly_twrrs]
        product = reduce(lambda x, y: x * y, cagrs)

        cagr_value = 0
        years_value = self._calculate_years(
            start_date=str(cashflow_progression_df_v1['event_date'].iloc[0].date()),
            end_date=str(cashflow_progression_df_v1['event_date'].iloc[-1].date())
            )
        if years_value > 1:
            cagr_value = ((product ** (1 / years_value)) - 1) * 100
        return abs_twrr_df, round(absolute_twrr * 100, 2), round(twrr * 100, 2), round(cagr_value, 2)

    def get_main_cashflow_progression_df(self, cashflow_progression_df: pd.DataFrame) -> pd.DataFrame:
        """Calculate portfolio_plus_cash for the progression DataFrame."""
        cashflow_progression_df['event_date'] = pd.to_datetime(cashflow_progression_df['event_date'])
        cashflow_progression_df = cashflow_progression_df.sort_values('event_date').reset_index(drop=True)
        cashflow_progression_df['month'] = cashflow_progression_df['event_date'].dt.month
        cashflow_progression_df['year'] = cashflow_progression_df['event_date'].dt.year
        cashflow_progression_df['portfolio_plus_cash'] = 0

        grouped_df = cashflow_progression_df.groupby(['year', 'month'])

        monthly_cashflow_list = []
        index_list = []
        for (year, month), group in grouped_df:
            monthly_cashflow = group['cashflow'].sum()
            last_day_of_current_month = calendar.monthrange(year, month)[1]
            last_date_of_current_month = f'{year}-{month:02d}-{last_day_of_current_month:02d}'
            last_date_of_current_month_date = pd.to_datetime(last_date_of_current_month)
            first_date_of_current_month = last_date_of_current_month_date.replace(day=1)
            last_date_of_previous_month = first_date_of_current_month - timedelta(days=1)
            
            index_of_previous_month = 0
            if last_date_of_previous_month in cashflow_progression_df['event_date'].values:
                indices_of_previous_month = cashflow_progression_df.index[
                    cashflow_progression_df['event_date'] == last_date_of_previous_month
                ].tolist()
                index_of_previous_month = indices_of_previous_month[0]

            monthly_cashflow_list.append(monthly_cashflow)
            if index_of_previous_month == 0:
                index_list.append(index_of_previous_month)
            else:
                index_list.append(index_of_previous_month)

        cashflow_progression_df['portfolio_plus_cash'] = cashflow_progression_df['portfolio_plus_cash'].astype(float)
      
        for i in range(1, len(index_list)):
            cashflow_progression_df.loc[index_list[i], 'portfolio_plus_cash'] = (
                monthly_cashflow_list[i] + cashflow_progression_df.loc[index_list[i], 'portfolio']
            )
        cashflow_progression_df = cashflow_progression_df.drop(columns=['year', 'month'])
        cashflow_progression_df = cashflow_progression_df[
            (cashflow_progression_df['cashflow'] != 0) |
            (cashflow_progression_df['portfolio'] != 0) |
            (cashflow_progression_df['portfolio_plus_cash'] != 0)
        ].sort_values('event_date').reset_index(drop=True)

        first_portfolio_date = cashflow_progression_df[
            cashflow_progression_df['portfolio'] > 0
        ]['event_date'].min()
 
        pre_invest_cashflows = cashflow_progression_df[
            (cashflow_progression_df['event_date'] < first_portfolio_date) & 
            (cashflow_progression_df['cashflow'] != 0)
        ]
        starting_cash = pre_invest_cashflows['cashflow'].sum()
        starting_date = pre_invest_cashflows['event_date'].max()

        starting_row = pd.DataFrame([{
            'event_date': starting_date,
            'cashflow': starting_cash,
            'portfolio': 0.0,
            'portfolio_plus_cash': starting_cash
        }])
        rest_cashflow_progression_df = cashflow_progression_df[cashflow_progression_df['event_date'] >= first_portfolio_date].copy()
        final_cashflow_progression_df = pd.concat([starting_row, rest_cashflow_progression_df], ignore_index=True)
        final_cashflow_progression_df.sort_values('event_date', inplace=True)
        return cashflow_progression_df

    def _get_twrr(self, cashflow_progression_df: pd.DataFrame) -> tuple[pd.DataFrame, float]:
        if not cashflow_progression_df.empty and cashflow_progression_df['portfolio'].sum() != 0:
            month_end_mask = cashflow_progression_df['portfolio'] > 0
            month_ends = cashflow_progression_df[month_end_mask].copy()

            for i in range(len(month_ends) - 1):
                current_date = month_ends.iloc[i]['event_date']
                next_date = month_ends.iloc[i + 1]['event_date']

                next_month_start = current_date + pd.offsets.MonthBegin(1)
                next_month_end = next_date

                next_month_cashflow = cashflow_progression_df[
                    (cashflow_progression_df['event_date'] > current_date) &
                    (cashflow_progression_df['event_date'] <= next_date)
                ]['cashflow']

                next_month_cashflow_sum = next_month_cashflow.sum()
                # has_no_cashflows = (next_month_cashflow == 0).all()

                if next_month_cashflow_sum == 0:
                    cashflow_progression_df.loc[
                        cashflow_progression_df['event_date'] == current_date,
                        'portfolio_plus_cash'
                    ] = 0
                else:
                    cashflow_progression_df.loc[
                        cashflow_progression_df['event_date'] == current_date,
                        'portfolio_plus_cash'
                    ] = month_ends.iloc[i]['portfolio'] + next_month_cashflow_sum

                last_date = month_ends.iloc[-1]['event_date']
                cashflow_progression_df.loc[
                    cashflow_progression_df['event_date'] == last_date,
                    'portfolio_plus_cash'
                ] = 0
            last_row = cashflow_progression_df.tail(1).copy()
            oldest_date = cashflow_progression_df['event_date'].min()
            oldest_year = oldest_date.year
            oldest_month = oldest_date.month

            first_month_mask = (cashflow_progression_df['event_date'].dt.year == oldest_year) & \
                (cashflow_progression_df['event_date'].dt.month == oldest_month)
            
            first_month = cashflow_progression_df[first_month_mask]
            first_month_cashflow_sum = first_month['cashflow'].sum()

            cashflow_progression_df['portfolio_plus_cash'].iloc[0] = first_month_cashflow_sum
            cashflow_progression_df['portfolio_plus_cash'].iloc[0] += cashflow_progression_df['portfolio'].iloc[0]
            cashflow_progression_df = cashflow_progression_df[cashflow_progression_df['portfolio_plus_cash'] != 0]
            
            cashflow_progression_df = pd.concat([cashflow_progression_df, last_row], ignore_index=True)
            if cashflow_progression_df.empty:
                return pd.DataFrame(columns=["start_value", "start_date", "end_value", "end_date", "returns", "returns_1"]), 1
            start_value_list = list(cashflow_progression_df['portfolio_plus_cash'])
            end_value_list = list(cashflow_progression_df['portfolio'])
            start_date_list = list(cashflow_progression_df['event_date'])
            end_date_list = list(cashflow_progression_df['event_date'])

            start_value_list.pop(-1)
            end_value_list.pop(0)
            start_date_list.pop(-1)
            end_date_list.pop(0)

            twrr_data = {
                'start_date' : start_date_list,
                'start_value' : start_value_list,
                'end_date': end_date_list,
                'end_value' : end_value_list,
            }

            abs_twrr_df = pd.DataFrame(twrr_data)
            abs_twrr_df['returns'] = (abs_twrr_df['end_value'] / abs_twrr_df['start_value']) - 1
            abs_twrr_df['returns_1'] = abs_twrr_df['returns'] + 1
            absolute_twrr = ((abs_twrr_df['returns_1'].prod()) - 1)
            return abs_twrr_df, absolute_twrr

        else:
            return pd.DataFrame(columns=[['start_date', 'start_value', 'end_date', 'end_value', 'returns', 'returns_1']]), None

    def _get_twrrs_for_cagr(self, cashflow_progression_df: pd.DataFrame) -> tuple[pd.DataFrame, float]:
        if not cashflow_progression_df.empty and cashflow_progression_df['portfolio'].sum() != 0:
            month_end_mask = cashflow_progression_df['portfolio'] > 0
            month_ends = cashflow_progression_df[month_end_mask].copy()

            for i in range(len(month_ends) - 1):
                current_date = month_ends.iloc[i]['event_date']
                next_date = month_ends.iloc[i + 1]['event_date']

                next_month_start = current_date + pd.offsets.MonthBegin(1)
                next_month_end = next_date

                next_month_cashflow = cashflow_progression_df[
                    (cashflow_progression_df['event_date'] > current_date) &
                    (cashflow_progression_df['event_date'] <= next_date)
                ]['cashflow']

                next_month_cashflow_sum = next_month_cashflow.sum()
                has_no_cashflows = (next_month_cashflow == 0).all()

                if has_no_cashflows:
                    cashflow_progression_df.loc[
                        cashflow_progression_df['event_date'] == current_date,
                        'portfolio_plus_cash'
                    ] = 0
                else:
                    cashflow_progression_df.loc[
                        cashflow_progression_df['event_date'] == current_date,
                        'portfolio_plus_cash'
                    ] = month_ends.iloc[i]['portfolio'] + next_month_cashflow_sum

                last_date = month_ends.iloc[-1]['event_date']
                cashflow_progression_df.loc[
                    cashflow_progression_df['event_date'] == last_date,
                    'portfolio_plus_cash'
                ] = 0  

            last_row = cashflow_progression_df.tail(1).copy()

            oldest_date = cashflow_progression_df['event_date'].min()
            oldest_year = oldest_date.year
            oldest_month = oldest_date.month

            first_month_mask = (cashflow_progression_df['event_date'].dt.year == oldest_year) & \
                (cashflow_progression_df['event_date'].dt.month == oldest_month)
            
            first_month = cashflow_progression_df[first_month_mask]
            first_month_cashflow_sum = first_month['cashflow'].sum()

            cashflow_progression_df['portfolio_plus_cash'].iloc[0] = first_month_cashflow_sum + cashflow_progression_df['portfolio'].iloc[0]
            cashflow_progression_df['portfolio_plus_cash'].iloc[-1] = cashflow_progression_df['portfolio'].iloc[-1]
            cashflow_progression_df = cashflow_progression_df[cashflow_progression_df['portfolio_plus_cash'] != 0]
            
            if cashflow_progression_df.empty:
                return pd.DataFrame(columns=["start_value", "start_date", "end_value", "end_date", "returns", "returns_1"]), 1

            start_value_list = list(cashflow_progression_df['portfolio_plus_cash'])
            end_value_list = list(cashflow_progression_df['portfolio'])
            start_date_list = list(cashflow_progression_df['event_date'])
            end_date_list = list(cashflow_progression_df['event_date'])
            
            start_value_list.pop(-1)
            end_value_list.pop(0)
            start_date_list.pop(-1)
            end_date_list.pop(0)

            twrr_data = {
                'start_date' : start_date_list,
                'start_value' : start_value_list,
                'end_date': end_date_list,
                'end_value' : end_value_list,
            }

            abs_twrr_df = pd.DataFrame(twrr_data)
            abs_twrr_df['returns'] = (abs_twrr_df['end_value'] / abs_twrr_df['start_value']) - 1
            abs_twrr_df['returns_1'] = abs_twrr_df['returns'] + 1
            absolute_twrr = ((abs_twrr_df['returns_1'].prod()) - 1)  
            return abs_twrr_df, absolute_twrr

        else:
            return pd.DataFrame(columns=[['start_date', 'start_value', 'end_date', 'end_value', 'returns', 'returns_1']]), None

    def _create_financial_year_dataframes(self, df: pd.DataFrame) -> List[pd.DataFrame]:
        df['event_date'] = pd.to_datetime(df['event_date'])
        df = df.sort_values(by='event_date')
        df['year'] = df['event_date'].dt.year
        df['month'] = df['event_date'].dt.month
        df['FinancialYear'] = df.apply(lambda x: x['year'] if x['month'] > 3 else x['year'] - 1, axis=1)
        unique_financial_years = df['FinancialYear'].unique()

        financial_year_dataframes = []
        for year in unique_financial_years:
            start_date_prev = pd.Timestamp(year, 3, 31)
            end_date = pd.Timestamp(year+1, 3, 31)
            if not df[df['event_date'] == start_date_prev].empty:
                start_date = start_date_prev
            else:
                start_date = df[(df['FinancialYear'] == year)]['event_date'].min()
            fy_df = df[(df['event_date'] >= start_date) & (df['event_date'] <= end_date)]
            financial_year_dataframes.append(fy_df)
        return financial_year_dataframes

    def _calculate_years(self, start_date: str, end_date: str):
        start_date = datetime.strptime(start_date, "%Y-%m-%d")
        end_date = datetime.strptime(end_date, "%Y-%m-%d")
        years = end_date.year - start_date.year
        months_diff = end_date.month - start_date.month
        days_diff = end_date.day - start_date.day
        
        if months_diff < 0 or (months_diff == 0 and days_diff < 0):
            years -= 1
            months_diff += 12
        fraction_of_year = months_diff / 12 + days_diff / 365.25
        return years + fraction_of_year
        


================================================================================
# File: app/scripts/db_processors/cashflow_processor.py
================================================================================

import sys
import logging
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from sqlalchemy import select, func
from sqlalchemy.ext.asyncio import AsyncSession
from app.models.accounts.account_cashflow_details import AccountCashflow
from app.scripts.data_fetchers.data_transformer import KeynoteDataTransformer, ZerodhaDataTransformer
from app.scripts.data_fetchers.portfolio_data import ZerodhaDataFetcher, KeynoteApi
from app.scripts.db_processors.helper_functions import _generate_historical_month_ends
from typing import List, Dict

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

keynote_portfolio = KeynoteApi()
zerodha_portfolio = ZerodhaDataFetcher()

class CashflowProcessor:
    def __init__(
            self, 
            db: AsyncSession, 
            keynote_transformer: KeynoteDataTransformer, 
            zerodha_transformer: ZerodhaDataTransformer
        ):
        self.db = db
        self.keynote_transformer = keynote_transformer
        self.zerodha_transformer = zerodha_transformer

    async def process_single_account_ledger(self, account: dict):
        """Process ledger data for a single account based on broker type."""
        try:
            broker_name = account.get('broker_name')
            account_id = account.get('account_id', 'unknown')
            if not broker_name:
                logger.warning(f"Broker name missing for account {account_id}")
                return

            if broker_name == "keynote":
                await self._process_keynote_ledger(account)
            elif broker_name == "zerodha":
                await self._process_zerodha_ledger(account)
            else:
                logger.warning(f"Unknown broker {broker_name} for account {account_id}. Skipping.")
        except Exception as e:
            logger.error(f"Error in process_single_account_ledger for account {account_id}: {e}")

    async def _process_keynote_ledger(self, account: dict):
        """Process ledger data for a single Keynote account."""
        try:
            account_id = account.get('account_id')
            acc_start_date = account.get('acc_start_date')
            fiscal_acc_start_date = self.get_fiscal_start(acc_start_date)
            broker_code = account.get('broker_code')
            if not all([account_id, acc_start_date, fiscal_acc_start_date, broker_code]):
                logger.warning(f"Missing required account data for Keynote account: {account}")
                return

            today = datetime.now().date()
            latest_event = await self.db.execute(
                select(func.max(AccountCashflow.event_date))
                .where(AccountCashflow.owner_id == account_id)
                .where(AccountCashflow.owner_type == 'single')
            )
            latest_event_date = latest_event.scalar()

            cashflow_dict = await self.keynote_transformer.transform_ledger_to_cashflow(
                broker_code=broker_code,
                from_date=fiscal_acc_start_date,
                to_date=today.strftime("%Y-%m-%d")
            )
            if not cashflow_dict or not cashflow_dict.get("event_date"):
                logger.warning(f"No cashflow data for Keynote account {account_id}. Skipping.")
                return

            cashflow_records = [
                {
                    "event_date": cashflow_dict["event_date"][i],
                    "cashflow": cashflow_dict["cashflow"][i],
                    "tag": cashflow_dict["tag"][i],
                    "owner_id": account_id,
                    "owner_type": "single"
                }
                for i in range(len(cashflow_dict["event_date"]))
                if latest_event_date is None or cashflow_dict["event_date"][i] > latest_event_date
            ]

            for record in cashflow_records:
                self.db.add(AccountCashflow(**record))
            await self.db.commit()
            logger.info(f"Inserted {len(cashflow_records)} cashflow records for account {account_id}")
        except Exception as e:
            logger.error(f"Error processing ledger for Keynote account {account_id}: {e}", exc_info=True)
            await self.db.rollback()

    async def _process_zerodha_ledger(self, account: dict):
        """Process ledger data for a single Zerodha account."""
        try:
            account_id = account.get('account_id')
            acc_start_date = account.get('acc_start_date')
            broker_code = account.get('broker_code')
            if not all([account_id, acc_start_date, broker_code]):
                logger.warning(f"Missing required account data for Zerodha account: {account}")
                return

            today = datetime.now().date()
            latest_event = await self.db.execute(
                select(func.max(AccountCashflow.event_date))
                .where(AccountCashflow.owner_id == account_id)
                .where(AccountCashflow.owner_type == 'single')
            )
            latest_event_date = latest_event.scalar()
            
            cashflow_dict = await self.zerodha_transformer.transform_ledger_to_cashflow(broker_code=broker_code)
            if not cashflow_dict or not cashflow_dict.get("event_date"):
                logger.warning(f"No cashflow data for Zerodha account {account_id}. Skipping.")
                return

            cashflow_records = [
                {
                    "event_date": cashflow_dict["event_date"][i],
                    "cashflow": cashflow_dict["cashflow"][i],
                    "tag": cashflow_dict["tag"][i],
                    "owner_id": account_id,
                    "owner_type": "single"
                }
                for i in range(len(cashflow_dict["event_date"]))
                if latest_event_date is None or cashflow_dict["event_date"][i] > latest_event_date
            ]

            for record in cashflow_records:
                self.db.add(AccountCashflow(**record))
            await self.db.commit()
            logger.info(f"Inserted {len(cashflow_records)} cashflow records for account {account_id}")
        except Exception as e:
            logger.error(f"Error processing ledger for Zerodha account {account_id}: {e}", exc_info=True)
            await self.db.rollback()

    async def process_joint_accounts_ledger(self, joint_accounts: List[Dict]):
        """Process and update cashflows for joint accounts by aggregating from single accounts."""
        for joint_account in joint_accounts:
            joint_id = joint_account["joint_account_id"]
            single_accounts = joint_account["single_accounts"]
            single_ids = [acc["account_id"] for acc in single_accounts]

            if not single_ids:
                logger.warning(f"No single accounts for joint account {joint_id}")
                continue

            try:
                cashflow_query = (
                    select(AccountCashflow)
                    .where(AccountCashflow.owner_id.in_(single_ids))
                    .where(AccountCashflow.owner_type == "single")
                )
                result = await self.db.execute(cashflow_query)
                cashflows = result.scalars().all()

                if not cashflows:
                    logger.info(f"No cashflows found for single accounts of joint account {joint_id}")
                    continue

                aggregated_cashflows = {}
                for cf in cashflows:
                    date = cf.event_date
                    if date in aggregated_cashflows:
                        aggregated_cashflows[date]["cashflow"] += cf.cashflow                            
                    else:
                        aggregated_cashflows[date] = {
                            "event_date": date,
                            "cashflow": cf.cashflow,
                            "tag": ""
                        }
                    
                aggregated_list = list(aggregated_cashflows.values())
                aggregated_list.sort(key=lambda x: x["event_date"])
                latest_event_query = (
                    select(func.max(AccountCashflow.event_date))
                    .where(AccountCashflow.owner_id == joint_id)
                    .where(AccountCashflow.owner_type == "joint")
                )
                result = await self.db.execute(latest_event_query)
                latest_event_date = result.scalar()

                new_cashflows = (
                    [cf for cf in aggregated_list if cf["event_date"] > latest_event_date]
                    if latest_event_date
                    else aggregated_list
                )

                if not new_cashflows:
                    logger.info(f"No new cashflows to insert for joint account {joint_id}")
                    continue

                for cf in new_cashflows:
                    new_record = AccountCashflow(
                        owner_id=joint_id,
                        owner_type="joint",
                        event_date=cf["event_date"],
                        cashflow=cf["cashflow"],
                        tag=cf["tag"]
                    )
                    self.db.add(new_record)

                await self.db.commit()
                logger.info(f"Inserted {len(new_cashflows)} cashflow records for joint account {joint_id}")
            except Exception as e:
                logger.error(f"Error processing cashflows for joint account {joint_id}: {e}")
                await self.db.rollback()

    async def get_month_end_cash_balances(self, account: dict, month_ends: list):
        """Retrieve month-end cash balances for a given account from acc_start_date to today."""
        broker_name = account.get('broker_name')
        account_id = account.get('account_id')
        acc_start_date = account.get('acc_start_date')
        broker_code = account.get('broker_code')
        
        if not all([broker_name, account_id, acc_start_date, broker_code]):
            logger.warning(f"Missing required account data for {broker_name} account: {account}")
            return None
        
        today = datetime.now().date()
        # month_ends = _generate_historical_month_ends(acc_start_date, today)
        
        month_ends_shifted = []
        for month_end in month_ends:
            new_month_end = month_end - timedelta(days=1)
            month_ends_shifted.append(new_month_end)

        if not month_ends:
            logger.warning(f"No month-end dates generated for account {account_id}")
            return None
        
        try:
            balances = []
            if broker_name == "zerodha":
                ledger_data = zerodha_portfolio.get_ledger(broker_code=broker_code)
                if not ledger_data:
                    logger.warning(f"No ledger data for Zerodha account {account_id}")
                    return {}
                ledger_df = pd.DataFrame(ledger_data)
                if ledger_df.empty:
                    logger.warning(f"Ledger data for Zerodha account {account_id} is empty")
                    return {}
                date_col = "Posting Date"
                balance_col = "Net Balance"
                balances = self.get_month_end_balances(ledger_df, date_col, balance_col, month_ends, broker_name)
            
            elif broker_name == "keynote":
                from_date = acc_start_date
                to_date = today.strftime("%Y-%m-%d")
                ledger_data = await keynote_portfolio.fetch_ledger(
                    from_date=from_date,
                    to_date=to_date,
                    ucc=broker_code
                )
                if not ledger_data:
                    logger.warning(f"No ledger data for Keynote account {account_id}")
                    return None
                ledger_df = pd.DataFrame(ledger_data)
                if ledger_df.empty:
                    logger.warning(f"Ledger data for Keynote account {account_id} is empty")
                    return None
                date_col = "vrdt"
                balance_col = "runbal"
                balances = self.get_month_end_balances(ledger_df, date_col, balance_col, month_ends_shifted, broker_name)

            else:
                logger.warning(f"Unknown broker {broker_name} for account {account_id}")
                return None
            
            logger.info(f"Calculated {len(balances)} month-end cash balances for account {account_id}")
            return balances
        
        except Exception as e:
            logger.error(f"Error processing cash balances for account {account_id}: {e}")
            return None

    def get_month_end_balances(self, ledger_df: pd.DataFrame, date_col, balance_col, month_ends, broker_name):
        ledger_df[date_col] = pd.to_datetime(ledger_df[date_col])
        ledger_df = ledger_df.sort_values(by=date_col)
        month_ends = [pd.to_datetime(me) for me in month_ends]

        dates = ledger_df[date_col].values
        dates = [pd.Timestamp(date) for date in dates]
        balances = ledger_df[balance_col].values

        result = {}
        for monthend_date in month_ends:
            idx = np.searchsorted(dates, monthend_date, side='right')
            if idx > 0:
                balance = balances[idx - 1]
            else:
                balance = 0.0
            result[monthend_date.date()] = balance

        shifted_result = {key + timedelta(days=1): value for key, value in result.items()}
        
        if broker_name.lower() == 'keynote':
            return shifted_result
        else:
            return result

    async def calculate_invested_amt(self, account_id: str, account_type: str) -> float:
        """Calculate the total invested amount as the sum of all cashflows."""
        try:
            sum_query = select(func.sum(AccountCashflow.cashflow)).where(
                AccountCashflow.owner_id == account_id,
                AccountCashflow.owner_type == account_type
            )
            result = await self.db.execute(sum_query)
            invested_amt = result.scalar() or 0.0
            return invested_amt
        except Exception as e:
            logger.error(f"Error calculating invested_amt for {account_type} account {account_id}: {e}")
            return 0.0

    async def calculate_cash_value(self, account: dict, month_ends: list) -> float:
        """Calculate the latest cash balance for the account."""
        try:
            balances = await self.get_month_end_cash_balances(account, month_ends)
            print("BALANCES: ", balances)
            if balances:
                latest_date = max(balances.keys())
                return balances[latest_date]
            return 0.0
        except Exception as e:
            logger.error(f"Error calculating cash_value for account {account['account_id']}: {e}")
            return 0.0

    def get_fiscal_start(self, date_str: str) -> str:
        date = datetime.strptime(date_str, "%Y-%m-%d")
        fiscal_year_start = datetime(date.year, 4, 1)
        if date < fiscal_year_start:
            fiscal_year_start = datetime(date.year - 1, 4, 1)
        
        return fiscal_year_start.strftime("%Y-%m-%d")

    async def initialize(self, accounts: List[Dict], joint_accounts: List[Dict]):
        """Orchestrate the processing of cashflows for single and joint accounts."""
        for account in accounts:
            await self.process_single_account_ledger(account)
        await self.process_joint_accounts_ledger(joint_accounts)


================================================================================
# File: app/scripts/db_processors/db_runner.py
================================================================================

import sys
import asyncio
import logging
from sqlalchemy.orm import selectinload
from app.database import AsyncSessionLocal
from app.services.accounts.account_service import AccountService
from app.services.accounts.joint_account_service import JointAccountService
from app.models.accounts.single_account import SingleAccount
from app.models.accounts.joint_account import JointAccount
from app.models.accounts.account_performance import AccountPerformance
from app.scripts.data_fetchers.data_transformer import KeynoteDataTransformer, ZerodhaDataTransformer
from app.scripts.data_fetchers.portfolio_data import KeynoteApi, ZerodhaDataFetcher
from app.scripts.db_processors.cashflow_processor import CashflowProcessor
from app.scripts.db_processors.actual_portfolio_processor import ActualPortfolioProcessor
from app.scripts.db_processors.cashflow_progression_processor import CashflowProgressionProcessor
from app.scripts.db_processors.ltp_processor import LtpProcessor


logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

keynote_portfolio = KeynoteApi()
zerodha_portfolio = ZerodhaDataFetcher()

async def runner():
    """Main function to process accounts and update all required fields."""
    try:
        async with AsyncSessionLocal() as db:
            accounts_data = await AccountService.get_single_accounts_with_broker_info(db)

            accounts_data = [
                {
                    "account_id": "ACC_000303",
                    "broker_code": "MK100",
                    "broker_name": "keynote",
                    "acc_start_date": "2022-04-01"
                },
                # {
                #     "account_id": "ACC_000313",
                #     "broker_code": "MM5525",
                #     "broker_name": "zerodha",
                #     "acc_start_date": "2022-05-01"        
                # },
                # {
                #     "account_id": "ACC_000312",
                #     "broker_code": "MDK705",
                #     "broker_name": "zerodha",
                #     "acc_start_date": "2022-05-01"     
                # }
            # # # #     # {
            # # # #     #     "account_id": "ACC_000325",
            # # # #     #     "broker_code": "RXU639",
            # # # #     #     "broker_name": "zerodha",
            # # # #     #     "acc_start_date": "2022-11-01" 
            # # # #     # },
            # # # #     # {
            # # # #     #     "account_id": "ACC_000326",
            # # # #     #     "broker_code": "GB2876",
            # # # #     #     "broker_name": "zerodha",
            # # # #     #     "acc_start_date": "2024-10-06" 
            # # # #     # },
            # # # #     # {
            # # # #     #     "account_id": "ACC_000505",
            # # # #     #     "broker_code": "FS7741",
            # # # #     #     "broker_name": "zerodha",
            # # # #     #     "acc_start_date": "2022-11-01" 
            # # # #     # }
            ]

            # if not accounts_data:
            #     logger.warning("No single accounts found.")
            #     return
            
            joint_accounts = await JointAccountService.get_joint_accounts_with_single_accounts(db)

            # joint_accounts = [{
            #     'joint_account_id': 'JACC_000012',
            #     'single_accounts': [{
            #         'account_id': 'ACC_000312',
            #         'acc_start_date': '2022-05-01',
            #         'broker_code': 'MDK705',
            #         'broker_name': 'zerodha'
            #     }, {
            #         'account_id': 'ACC_000313',
            #         'acc_start_date': '2022-05-01',
            #         'broker_code': 'MM5525',
            #         'broker_name': 'zerodha'
            #     }]
            # }]

            joint_accounts = []

            if not joint_accounts:
                logger.warning("No joint accounts found.")

            cashflow_processor = CashflowProcessor(db, KeynoteDataTransformer(), ZerodhaDataTransformer())
            portfolio_processor = ActualPortfolioProcessor(db, KeynoteDataTransformer(), ZerodhaDataTransformer())
            progression_processor = CashflowProgressionProcessor(db, cashflow_processor)

            await cashflow_processor.initialize(accounts_data, joint_accounts)
            await portfolio_processor.initialize(accounts_data, joint_accounts)

            for acc in accounts_data:
                acc['account_type'] = 'single'
                df_single = await progression_processor.get_cashflow_progression_df(acc)
                if not df_single.empty:
                    await progression_processor.update_cashflow_progression_table(acc, df_single)
                    logger.info(f"Updated cashflow progression for single account {acc['account_id']}")

                    invested_amt = await cashflow_processor.calculate_invested_amt(acc['account_id'], 'single')
                    pf_value = await portfolio_processor.calculate_pf_value(acc['account_id'], 'single')
                    portfolio_values, month_ends = await progression_processor.get_portfolio_values(acc['account_id'], 'single')
                    cash_value = await cashflow_processor.calculate_cash_value(acc, month_ends)
                    total_holdings = pf_value + cash_value
                    time_periods_df, total_twrr, current_yr_twrr, cagr = progression_processor.get_time_periods_df(df_single)
                    await progression_processor.update_time_periods_table(acc, time_periods_df)

                    account_model = await db.get(
                        SingleAccount,
                        acc['account_id'],
                        options=[selectinload(SingleAccount.performance)]
                        )
                    if account_model:
                        account_model.invested_amt = invested_amt
                        account_model.pf_value = pf_value
                        account_model.cash_value = cash_value
                        account_model.total_holdings = total_holdings

                        if account_model.performance:
                            account_model.performance.total_twrr = total_twrr
                            account_model.performance.current_yr_twrr = current_yr_twrr
                            account_model.performance.cagr = cagr
                        else:
                            new_perf = AccountPerformance(
                                performance_id=f"PERF_{acc['account_id']}",
                                owner_id=acc['account_id'],
                                owner_type='single',
                                total_twrr=total_twrr,
                                current_yr_twrr=current_yr_twrr,
                                cagr=cagr
                            )
                            db.add(new_perf)
                            account_model.performance = new_perf
                        
                        await db.commit()
                        logger.info(f"Updated single account {acc['account_id']}: "
                                    f"invested_amt={invested_amt}, pf_value={pf_value}, "
                                    f"cash_value={cash_value}, total_holdings={total_holdings}, "
                                    f"total_twrr={total_twrr}")

            for joint_acc in joint_accounts:
                joint_acc_dict = {
                    'account_id': joint_acc['joint_account_id'],
                    'account_type': 'joint'
                }
                df_joint = await progression_processor.get_cashflow_progression_df(joint_acc_dict)

                if not df_joint.empty:
                    await progression_processor.update_cashflow_progression_table(joint_acc_dict, df_joint)
                    logger.info(f"Updated cashflow progression for joint account {joint_acc['joint_account_id']}")

                    invested_amt = await cashflow_processor.calculate_invested_amt(joint_acc['joint_account_id'], 'joint')
                    pf_value = await portfolio_processor.calculate_pf_value(joint_acc['joint_account_id'], 'joint')
                    cash_value = 0.0
                    for single_acc in joint_acc['single_accounts']:
                        portfolio_values, month_ends = await progression_processor.get_portfolio_values(single_acc['account_id'], 'single')
                        cash_value += await cashflow_processor.calculate_cash_value(single_acc, month_ends)
                    total_holdings = pf_value + cash_value
                    time_periods_df, total_twrr, current_yr_twrr, cagr = progression_processor.get_time_periods_df(df_joint)
                    await progression_processor.update_time_periods_table(joint_acc_dict, time_periods_df)

                    try:
                        account_model = await db.get(
                            JointAccount, 
                            joint_acc['joint_account_id'],
                            options=[selectinload(JointAccount.performance)])
                        if account_model:
                            account_model.invested_amt = round(invested_amt, 2)
                            account_model.pf_value = round(pf_value, 2)
                            account_model.cash_value = round(cash_value, 2)
                            account_model.total_holdings = round(total_holdings, 2)

                            if account_model.performance:
                                account_model.performance.total_twrr = round(total_twrr, 2)
                                account_model.performance.current_yr_twrr = round(current_yr_twrr, 2)
                                account_model.performance.cagr = round(cagr, 2)
                            else:
                                new_perf = AccountPerformance(
                                    performance_id=f"PERF_{joint_acc['joint_account_id']}",
                                    owner_id=joint_acc['joint_account_id'],
                                    owner_type='joint',
                                    total_twrr=total_twrr,
                                    current_yr_twrr=current_yr_twrr,
                                    cagr=cagr
                                )
                                db.add(new_perf)
                                account_model.performance = new_perf
                            
                            logger.info(f"Attempting to commit updates for joint account {joint_acc['joint_account_id']}")
                            await db.commit()
                            logger.info(f"Updated joint account {joint_acc['joint_account_id']}: "
                                        f"invested_amt={invested_amt}, pf_value={pf_value}, "
                                        f"cash_value={cash_value}, total_holdings={total_holdings}, "
                                        f"total_twrr={total_twrr}")
                    except Exception as e:
                        logger.error(f"Error updating joint account {joint_acc['joint_account_id']}: {e}", exc_info=True)
                        await db.rollback()
    except Exception as e:
        logger.error(f"Error in run function: {e}")

if __name__ == "__main__":
    asyncio.run(runner())

================================================================================
# File: app/scripts/db_processors/actual_portfolio_processor.py
================================================================================

import sys
import asyncio
import logging
from datetime import datetime
from sqlalchemy import select, func, delete
from sqlalchemy.ext.asyncio import AsyncSession
from app.models.accounts.account_actual_portfolio import AccountActualPortfolio
from app.scripts.data_fetchers.data_transformer import (
    KeynoteDataTransformer, ZerodhaDataTransformer
)
from app.scripts.db_processors.helper_functions import (
    _generate_historical_month_ends, _get_existing_snapshot_dates
)
from typing import List, Dict
from app.models.accounts.account_actual_portfolio_exceptions import AccountActualPortfolioException

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ActualPortfolioProcessor:
    def __init__(
            self,
            db: AsyncSession,
            keynote_transformer: KeynoteDataTransformer,
            zerodha_transformer: ZerodhaDataTransformer
    ):
        self.db = db
        self.keynote_transformer = keynote_transformer
        self.zerodha_transformer = zerodha_transformer

    async def process_single_account_holdings(self, account: dict):
        """Process holdings data for a single account based on broker type."""
        try:
            broker_name = account.get('broker_name')
            account_id = account.get('account_id', 'unknown')
            if not broker_name:
                logger.warning(f"Broker name missing for account {account_id}. Skipping.")
                return

            if broker_name == "keynote":
                await self._process_keynote_holdings(account)
            elif broker_name == "zerodha":
                await self._process_zerodha_holdings(account)
            else:
                logger.warning(f"Unknown broker {broker_name} for account {account_id}. Skipping.")
        except Exception as e:
            logger.error(f"Error in process_single_account_holdings for account {account_id}: {e}")

    async def _fetch_exceptions(self, owner_id: str, owner_type: str) -> List[Dict]:
        """Fetch exceptions for a given owner_id and owner_type."""
        query = select(AccountActualPortfolioException).where(
            AccountActualPortfolioException.owner_id == owner_id,
            AccountActualPortfolioException.owner_type == owner_type
        )
        result = await self.db.execute(query)
        exceptions = result.scalars().all()
        return [{'trading_symbol': e.trading_symbol, 'quantity': e.quantity} for e in exceptions]

    def _adjust_portfolio(self, portfolio_dict: Dict, exceptions: List[Dict]) -> List[Dict]:
        """Adjust the portfolio data based on exceptions."""
        adjusted_portfolio = []
        exceptions_dict = {e['trading_symbol']: e['quantity'] for e in exceptions}

        for i in range(len(portfolio_dict['trading_symbol'])):
            symbol = portfolio_dict['trading_symbol'][i]
            quantity = portfolio_dict['quantity'][i]
            market_value = portfolio_dict['market_value'][i]

            if symbol in exceptions_dict:
                exclude_qty = exceptions_dict[symbol]
                if quantity <= exclude_qty:
                    continue
                else:
                    quantity -= exclude_qty
                    
                    market_value = (market_value / portfolio_dict['quantity'][i]) * quantity

            record = {
                "owner_id": portfolio_dict['owner_id'],
                "owner_type": portfolio_dict['owner_type'],
                "snapshot_date": portfolio_dict['snapshot_date'],
                "trading_symbol": symbol,
                "quantity": quantity,
                "market_value": market_value
            }
            adjusted_portfolio.append(record)

        return adjusted_portfolio

    async def _process_keynote_holdings(self, account: dict):
        """Process holdings data for a single Keynote account."""
        try:
            account_id = account.get('account_id')
            acc_start_date = account.get('acc_start_date')
            fiscal_acc_start_date = self.get_fiscal_start(acc_start_date)
            broker_code = account.get('broker_code')
            if not all([account_id, acc_start_date, fiscal_acc_start_date, broker_code]):
                logger.warning(f"Missing required account data for Keynote account: {account}. Skipping.")
                return

            today = datetime.now().date()
            historical_dates = _generate_historical_month_ends(fiscal_acc_start_date, today)
            if not historical_dates:
                logger.warning(f"No historical dates generated for account {account_id}. Skipping.")
                return

            existing_dates = await _get_existing_snapshot_dates(self.db, account_id)
            missing_historical_dates = [date for date in historical_dates if date not in existing_dates]
            current_month_start = today.replace(day=1)
            await self.db.execute(
                delete(AccountActualPortfolio)
                .where(AccountActualPortfolio.owner_id == account_id)
                .where(AccountActualPortfolio.owner_type == 'single')
                .where(AccountActualPortfolio.snapshot_date >= current_month_start)
            )
            await self.db.commit()

            dates_to_fetch = missing_historical_dates + [today]
            for date in dates_to_fetch:
                portfolio_dict = await asyncio.wait_for(
                    self.keynote_transformer.transform_holdings_to_actual_portfolio(
                        broker_code=broker_code,
                        for_date=date.strftime("%Y-%m-%d")
                    ),
                    timeout=30
                )
                if not portfolio_dict or not portfolio_dict.get("trading_symbol"):
                    logger.warning(f"No portfolio data for account {account_id} on {date}. Adding a null entry.")
                    null_portfolio = {
                        'owner_id': account_id,
                        'owner_type': 'single',
                        'snapshot_date': date,
                        'trading_symbol': "place holder",
                        'quantity': 0,
                        'market_value': 0
                    }
                    null_portfolio
                    self.db.add(AccountActualPortfolio(**null_portfolio))
                    await self.db.commit()
                    continue

                exceptions = await self._fetch_exceptions(account_id, 'single')

                adjusted_portfolio = self._adjust_portfolio({
                    'owner_id': account_id,
                    'owner_type': 'single',
                    'snapshot_date': date,
                    'trading_symbol': portfolio_dict['trading_symbol'],
                    'quantity': portfolio_dict['quantity'],
                    'market_value': portfolio_dict['market_value']
                }, exceptions)

                for record in adjusted_portfolio:
                    self.db.add(AccountActualPortfolio(**record))
                await self.db.commit()
                logger.info(f"Inserted adjusted portfolio records for {account_id} on {date}")
        except asyncio.TimeoutError:
            logger.warning(f"Timeout processing holdings for Keynote account {account_id}. Skipping.")
            await self.db.rollback()
        except Exception as e:
            logger.error(f"Error processing holdings for Keynote account {account_id}: {e}")
            await self.db.rollback()

    async def _process_zerodha_holdings(self, account: dict):
        """Process holdings data for a single Zerodha account."""
        try:
            account_id = account.get('account_id')
            acc_start_date = account.get('acc_start_date')
            broker_code = account.get('broker_code')
            if not all([account_id, acc_start_date, broker_code]):
                logger.warning(f"Missing required account data for Zerodha account: {account}. Skipping.")
                return

            today = datetime.now().date()
            historical_dates = _generate_historical_month_ends(acc_start_date, today)
            if not historical_dates:
                logger.warning(f"No historical dates generated for account {account_id}. Skipping.")
                return

            existing_dates = await _get_existing_snapshot_dates(self.db, account_id)
            missing_historical_dates = [date for date in historical_dates if date not in existing_dates]
            current_month_start = today.replace(day=1)
            await self.db.execute(
                delete(AccountActualPortfolio)
                .where(AccountActualPortfolio.owner_id == account_id)
                .where(AccountActualPortfolio.owner_type == 'single')
                .where(AccountActualPortfolio.snapshot_date >= current_month_start)
            )
            await self.db.commit()

            dates_to_fetch = missing_historical_dates + [today]
            for date in dates_to_fetch:
                portfolio_dict = await asyncio.wait_for(
                    self.zerodha_transformer.transform_holdings_to_actual_portfolio(
                        broker_code=broker_code,
                        year=date.year,
                        month=date.month
                    ),
                    timeout=30
                )
                if not portfolio_dict or not portfolio_dict.get("trading_symbol"):
                    logger.warning(f"No portfolio data for account {account_id} on {date}. Skipping date.")
                    null_portfolio = {
                        'owner_id': account_id,
                        'owner_type': 'single',
                        'snapshot_date': date,
                        'trading_symbol': "place holder",
                        'quantity': 0,
                        'market_value': 0
                    }
                    null_portfolio
                    self.db.add(AccountActualPortfolio(**null_portfolio))
                    await self.db.commit()
                    continue

                exceptions = await self._fetch_exceptions(account_id, 'single')

                adjusted_portfolio = self._adjust_portfolio({
                    'owner_id': account_id,
                    'owner_type': 'single',
                    'snapshot_date': date,
                    'trading_symbol': portfolio_dict['trading_symbol'],
                    'quantity': portfolio_dict['quantity'],
                    'market_value': portfolio_dict['market_value']
                }, exceptions)

                for record in adjusted_portfolio:
                    self.db.add(AccountActualPortfolio(**record))
                await self.db.commit()
                logger.info(f"Inserted adjusted portfolio records for {account_id} on {date}")
        except asyncio.TimeoutError:
            logger.warning(f"Timeout processing holdings for Zerodha account {account_id}. Skipping.")
            await self.db.rollback()
        except Exception as e:
            logger.error(f"Error processing holdings for Zerodha account {account_id}: {e}")
            await self.db.rollback()

    async def process_joint_accounts_holdings(self, joint_accounts: List[Dict]):
        """Process and update actual portfolios for joint accounts by aggregating from single accounts."""
        for joint_account in joint_accounts:
            joint_id = joint_account["joint_account_id"]
            single_accounts = joint_account["single_accounts"]

            if not single_accounts:
                logger.warning(f"No single accounts for joint account {joint_id}")
                continue

            try:
                acc_start_dates = [acc["acc_start_date"] for acc in single_accounts if acc["acc_start_date"]]
                if not acc_start_dates:
                    logger.warning(f"No valid acc_start_date for joint account {joint_id}")
                    continue
                earliest_start_date = min(acc_start_dates)

                today = datetime.now().date()
                historical_dates = _generate_historical_month_ends(earliest_start_date, today)
                if not historical_dates:
                    logger.warning(f"No historical dates generated for joint account {joint_id}")
                    continue

                existing_dates_query = (
                    select(AccountActualPortfolio.snapshot_date)
                    .where(AccountActualPortfolio.owner_id == joint_id)
                    .where(AccountActualPortfolio.owner_type == "joint")
                    .distinct()
                )
                result = await self.db.execute(existing_dates_query)
                existing_dates = set(row[0] for row in result.all())

                missing_historical_dates = [d for d in historical_dates if d not in existing_dates]
                dates_to_process = missing_historical_dates + [today]

                single_ids = [acc["account_id"] for acc in single_accounts]

                if today in dates_to_process:
                    current_month_start = today.replace(day=1)
                    delete_query = (
                        delete(AccountActualPortfolio)
                        .where(AccountActualPortfolio.owner_id == joint_id)
                        .where(AccountActualPortfolio.owner_type == "joint")
                        .where(AccountActualPortfolio.snapshot_date >= current_month_start)
                    )
                    await self.db.execute(delete_query)
                    await self.db.commit()

                for snapshot_date in dates_to_process:
                    portfolio_query = (
                        select(AccountActualPortfolio)
                        .where(AccountActualPortfolio.owner_id.in_(single_ids))
                        .where(AccountActualPortfolio.owner_type == "single")
                        .where(AccountActualPortfolio.snapshot_date == snapshot_date)
                    )
                    result = await self.db.execute(portfolio_query)
                    portfolios = result.scalars().all()

                    if not portfolios:
                        logger.warning(f"No portfolio data for joint account {joint_id} on {snapshot_date}")
                        continue

                    aggregated_portfolio = {}
                    for p in portfolios:
                        symbol = p.trading_symbol
                        if symbol in aggregated_portfolio:
                            aggregated_portfolio[symbol]["quantity"] += p.quantity
                            aggregated_portfolio[symbol]["market_value"] += p.market_value
                        else:
                            aggregated_portfolio[symbol] = {
                                "trading_symbol": symbol,
                                "quantity": p.quantity,
                                "market_value": p.market_value
                            }
                    exceptions = await self._fetch_exceptions(joint_id, 'joint')

                    adjusted_portfolio = self._adjust_portfolio({
                        'owner_id': joint_id,
                        'owner_type': 'joint',
                        'snapshot_date': snapshot_date,
                        'trading_symbol': list(aggregated_portfolio.keys()),
                        'quantity': [data['quantity'] for data in aggregated_portfolio.values()],
                        'market_value': [data['market_value'] for data in aggregated_portfolio.values()]
                    }, exceptions)

                    for record in adjusted_portfolio:
                        self.db.add(AccountActualPortfolio(**record))

                    await self.db.commit()
                    logger.info(f"Inserted adjusted portfolio for joint account {joint_id} on {snapshot_date}")
            except Exception as e:
                logger.error(f"Error processing portfolios for joint account {joint_id}: {e}")
                await self.db.rollback()

    async def calculate_pf_value(self, account_id: str, account_type: str) -> float:
        """Calculate the current portfolio value from the latest snapshot."""
        try:
            async with self.db as session:
                latest_date_query = select(func.max(AccountActualPortfolio.snapshot_date)).where(
                    AccountActualPortfolio.owner_id == account_id,
                    AccountActualPortfolio.owner_type == account_type
                )
                latest_date = (await session.execute(latest_date_query)).scalar()
                if latest_date:
                    sum_query = select(func.sum(AccountActualPortfolio.market_value)).where(
                        AccountActualPortfolio.owner_id == account_id,
                        AccountActualPortfolio.owner_type == account_type,
                        AccountActualPortfolio.snapshot_date == latest_date
                    )
                    result = await session.execute(sum_query)
                    return result.scalar()
                return 0.0
        except Exception as e:
            logger.error(f"Error calculating pf_value for {account_type} account {account_id}: {e}")
            return 0.0

    def get_fiscal_start(self, date_str: str) -> str:
        date = datetime.strptime(date_str, "%Y-%m-%d")
        fiscal_year_start = datetime(date.year, 4, 1)
        if date < fiscal_year_start:
            fiscal_year_start = datetime(date.year - 1, 4, 1)
        
        return fiscal_year_start.strftime("%Y-%m-%d")

    async def initialize(self, accounts: List[Dict], joint_accounts: List[Dict]):
        """Orchestrate the processing of actual portfolios for single and joint accounts."""
        for account in accounts:
            await self.process_single_account_holdings(account)
        await self.process_joint_accounts_holdings(joint_accounts)

================================================================================
# File: app/scripts/db_processors/__init__.py
================================================================================



================================================================================
# File: app/scripts/db_processors/ltp_processor.py
================================================================================

import asyncio
import pandas as pd
from sqlalchemy import select, union, delete, insert
from sqlalchemy.ext.asyncio import AsyncSession
from app.models.stock_ltps import StockLTP
from app.models.accounts.account_actual_portfolio import AccountActualPortfolio
from app.models.accounts.account_ideal_portfolio import AccountIdealPortfolio
from app.scripts.data_fetchers.broker_data import BrokerData
from typing import List, Dict, Any
from app.database import AsyncSessionLocal
from datetime import datetime


class LtpProcessor:
    def __init__(self):
        """Initialize the LtpProcessor."""
        pass

    async def get_trading_symbols(self, db: AsyncSession) -> List[str]:
        """
        Retrieve all unique trading symbols from account_actual_portfolio and account_ideal_portfolio tables.

        Args:
            db (AsyncSession): The asynchronous database session to execute the query.

        Returns:
            List[str]: A list of unique trading symbols.
        """
        actual_symbols = select(AccountActualPortfolio.trading_symbol)
        ideal_symbols = select(AccountIdealPortfolio.trading_symbol)
        unique_symbols_query = union(actual_symbols, ideal_symbols)
        result = await db.execute(unique_symbols_query)
        unique_symbols = result.scalars().all()
        return unique_symbols

    async def get_ltps(self, trading_symbols: List[str]) -> List[Dict[str, Any]]:
        """
        Fetch Last Traded Prices (LTPs) for the given trading symbols from the broker.

        Args:
            trading_symbols (List[str]): List of trading symbols to fetch LTPs for.

        Returns:
            List[Dict[str, Any]]: List of dictionaries containing 'trading_symbol' and 'ltp'.
        
        Raises:
            ValueError: If fetching master data or LTP quotes from the broker fails.
        """
        upstox_master_data = BrokerData.get_master_data()
        if upstox_master_data.get("status") != "success":
            raise ValueError("Failed to fetch master data from broker")

        upstox_master_df = pd.DataFrame(upstox_master_data["data"])

        upstox_master_df_slice = upstox_master_df[
            (upstox_master_df["exchange"].isin(["NSE", "BSE"])) &
            (upstox_master_df["instrument_type"] == "EQ")
        ][["trading_symbol", "exchange", "exchange_token", "instrument_type"]]
        
        upstox_master_df_slice_sorted = upstox_master_df_slice.sort_values(
            by=["trading_symbol", "exchange"],
            ascending=[True, True]
        )
        upstox_master_df_slice = upstox_master_df_slice_sorted.drop_duplicates(
            subset="trading_symbol", keep="first"
        )
        mapping_data = upstox_master_df_slice[
            upstox_master_df_slice["trading_symbol"].isin(trading_symbols)
        ]
        mapping_data.reset_index(drop=True, inplace=True)
        ltp_request_data = mapping_data[["exchange_token", "exchange", "instrument_type"]].to_dict(orient="records")
        ltp_response_data = BrokerData.get_ltp_quote(ltp_request_data)
        if ltp_response_data.get("status") != "success":
            raise ValueError("Failed to fetch LTP quotes from broker")
        ltp_response_data = ltp_response_data["data"]
        
        trading_symbols_list = []
        ltps_list = []
        for key in ltp_response_data:
            trading_symbols_list.append(ltp_response_data[key]["trading_symbol"])
            ltps_list.append(ltp_response_data[key]["last_price"])

        trading_symbol_ltp = pd.DataFrame({
            "trading_symbol": trading_symbols_list,
            "ltp": ltps_list
        })
        ltp_data = trading_symbol_ltp.to_dict(orient="records")
        return ltp_data

    async def delete_existing_ltps(self, db: AsyncSession) -> None:
        """
        Delete all existing rows from the stock_ltps table.

        Args:
            db (AsyncSession): The asynchronous database session.
        """
        await db.execute(delete(StockLTP))
        await db.commit()

    async def insert_ltps(self, db: AsyncSession, ltp_data: List[Dict[str, Any]]) -> None:
        """
        Insert new LTP data into the stock_ltps table.

        Args:
            db (AsyncSession): The asynchronous database session.
            ltp_data (List[Dict[str, Any]]): List of dictionaries with 'trading_symbol' and 'ltp'.
        """
        if ltp_data:
            async with db.begin():
                await db.execute(
                    insert(StockLTP),
                    [
                        {
                            "trading_symbol": item["trading_symbol"],
                            "ltp": item["ltp"],
                        }
                        for item in ltp_data
                    ]
                )

    async def process_ltps(self, db: AsyncSession) -> None:
        """
        Process the entire LTP update: fetch symbols, get LTPs, delete existing data, and insert new data.

        Args:
            db (AsyncSession): The asynchronous database session.
        """
        symbols = await self.get_trading_symbols(db)
        ltp_data = await self.get_ltps(symbols)
        await self.delete_existing_ltps(db)
        await self.insert_ltps(db, ltp_data)


async def main() -> None:
    """Main function to run the LTP processing."""
    async with AsyncSessionLocal() as db:
        processor = LtpProcessor()
        await processor.process_ltps(db)


if __name__ == "__main__":
    asyncio.run(main())

================================================================================
# File: app/schemas/non_tradable_logs.py
================================================================================

from pydantic import BaseModel, Field
from typing import Optional
from datetime import date, datetime

class NonTradableLogBase(BaseModel):
    account_id: str = Field(..., description="Account ID")
    trading_symbol: str = Field(..., description="Stock trading symbol")
    reason: Optional[str] = Field(None, description="Reason why the stock is non-tradable")
    event_date: date = Field(..., description="Date of the log entry")

class NonTradableLogCreate(NonTradableLogBase):
    pass

class NonTradableLogUpdate(BaseModel):
    reason: Optional[str] = Field(None, description="Updated reason")

class NonTradableLogResponse(NonTradableLogBase):
    id: int = Field(..., description="Unique identifier for the log entry")
    created_at: datetime = Field(..., description="Record creation timestamp")

    class Config:
        from_attributes = True

================================================================================
# File: app/schemas/__init__.py
================================================================================




================================================================================
# File: app/schemas/stock_ltps.py
================================================================================

from pydantic import BaseModel, Field
from typing import Optional
from datetime import datetime

class StockLTPBase(BaseModel):
    trading_symbol: str = Field(..., description="Stock trading symbol")
    ltp: float = Field(..., description="Last traded price")

class StockLTPCreate(StockLTPBase):
    pass

class StockLTPUpdate(BaseModel):
    ltp: Optional[float] = Field(None, description="Updated last traded price")

class StockLTPResponse(StockLTPBase):
    id: int = Field(..., description="Unique identifier for the stock LTP record")
    created_at: datetime = Field(..., description="Record creation timestamp")

    class Config:
        from_attributes = True

================================================================================
# File: app/schemas/stock_exceptions.py
================================================================================

# app/schemas/stock_schemas.py
from pydantic import BaseModel, Field
from typing import Optional
from datetime import datetime

class StockExceptionBase(BaseModel):
    account_id: str = Field(..., description="Account ID (single or joint)")
    trading_symbol: str = Field(..., description="Stock trading symbol")

class StockExceptionCreate(StockExceptionBase):
    pass

class StockExceptionUpdate(BaseModel):
    pass  

class StockExceptionResponse(StockExceptionBase):
    id: int = Field(..., description="Unique identifier for the stock exception")
    created_at: datetime = Field(..., description="Record creation timestamp")

    class Config:
        from_attributes = True

================================================================================
# File: app/schemas/portfolio/basket_stock_mapping.py
================================================================================

from pydantic import BaseModel, Field
from typing import Optional


class BasketStockMappingBase(BaseModel):
    basket_id: int = Field(..., description="Basket ID to which the stock belongs")
    trading_symbol: str = Field(..., description="Stock trading symbol")
    multiplier: float = Field(..., description="Multiplier or weight for this stock")

class BasketStockMappingCreate(BasketStockMappingBase):
    pass

class BasketStockMappingUpdate(BaseModel):
    trading_symbol: Optional[str] = None
    multiplier: Optional[float] = None

class BasketStockMappingResponse(BaseModel):
    basket_stock_mapping_id: int = Field(..., description="Primary key of basket-stock mapping")
    basket_name: str = Field(..., description="Name of the basket")
    basket_id: int = Field(..., description="Basket ID to which the stock belongs")
    trading_symbol: str = Field(..., description="Stock trading symbol")
    multiplier: float = Field(..., description="Multiplier or weight for this stock")

    class Config:
        from_attributes = True


================================================================================
# File: app/schemas/portfolio/portfolio_template_details.py
================================================================================

from pydantic import BaseModel, Field
from typing import Optional
from datetime import datetime


class PortfolioTemplateBase(BaseModel):
    portfolio_name: str = Field(..., description="Name of the portfolio")
    description: Optional[str] = Field(None, description="Description of the portfolio")

class PortfolioTemplateCreate(PortfolioTemplateBase):
    pass

class PortfolioTemplateUpdate(BaseModel):
    portfolio_name: Optional[str] = None
    description: Optional[str] = None

class PortfolioTemplateResponse(PortfolioTemplateBase):
    portfolio_id: int = Field(..., description="Portfolio template ID")
    created_at: datetime = Field(..., description="Record creation time")

    class Config:
        from_attributes = True


================================================================================
# File: app/schemas/portfolio/portfolio_basket_mapping.py
================================================================================

from pydantic import BaseModel, Field
from typing import Optional
from datetime import datetime


class PortfolioBasketMappingBase(BaseModel):
    portfolio_id: int = Field(..., description="Portfolio template ID")
    basket_id: int = Field(..., description="Basket ID")
    allocation_pct: Optional[float] = Field(None, description="Allocation percentage for this basket in the portfolio")

class PortfolioBasketMappingCreate(PortfolioBasketMappingBase):
    pass

class PortfolioBasketMappingUpdate(BaseModel):
    allocation_pct: Optional[float] = None

class PortfolioBasketMappingResponse(PortfolioBasketMappingBase):
    portfolio_basket_mapping_id: int = Field(..., description="Primary key for this mapping")

    class Config:
        from_attributes = True


================================================================================
# File: app/schemas/portfolio/__init__.py
================================================================================

from .pf_bracket_basket_allocation import (
    PfBracketBasketAllocationBase,
    PfBracketBasketAllocationCreate,
    PfBracketBasketAllocationUpdate,
    PfBracketBasketAllocationResponse
)

# Portfolio / Basket / Bracket
from .portfolio_template_details import (
    PortfolioTemplateBase,
    PortfolioTemplateCreate,
    PortfolioTemplateUpdate,
    PortfolioTemplateResponse
)
from .bracket_details import (
    BracketBase,
    BracketCreate,
    BracketUpdate,
    BracketResponse
)
from .basket_details import (
    BasketBase,
    BasketCreate,
    BasketUpdate,
    BasketResponse
)
from .basket_stock_mapping import (
    BasketStockMappingBase,
    BasketStockMappingCreate,
    BasketStockMappingUpdate,
    BasketStockMappingResponse
)
from .portfolio_basket_mapping import (
    PortfolioBasketMappingBase,
    PortfolioBasketMappingCreate,
    PortfolioBasketMappingUpdate,
    PortfolioBasketMappingResponse
)


__all__ = [
    # Pf bracket-basket allocation
    "PfBracketBasketAllocationBase",
    "PfBracketBasketAllocationCreate",
    "PfBracketBasketAllocationUpdate",
    "PfBracketBasketAllocationResponse",

    # Portfolio template
    "PortfolioTemplateBase",
    "PortfolioTemplateCreate",
    "PortfolioTemplateUpdate",
    "PortfolioTemplateResponse",

    # Bracket
    "BracketBase",
    "BracketCreate",
    "BracketUpdate",
    "BracketResponse",

    # Basket
    "BasketBase",
    "BasketCreate",
    "BasketUpdate",
    "BasketResponse",

    # Basket-stock mapping
    "BasketStockMappingBase",
    "BasketStockMappingCreate",
    "BasketStockMappingUpdate",
    "BasketStockMappingResponse",

    # Portfolio-basket mapping
    "PortfolioBasketMappingBase",
    "PortfolioBasketMappingCreate",
    "PortfolioBasketMappingUpdate",
    "PortfolioBasketMappingResponse",
]

================================================================================
# File: app/schemas/portfolio/bracket_details.py
================================================================================

from pydantic import BaseModel, Field
from typing import Optional


class BracketBase(BaseModel):
    bracket_min: float = Field(..., description="Minimum investment for bracket")
    bracket_max: float = Field(..., description="Maximum investment for bracket")
    bracket_name: str = Field(..., description="Bracket name or label")

class BracketCreate(BracketBase):
    pass

class BracketUpdate(BaseModel):
    bracket_min: Optional[float] = None
    bracket_max: Optional[float] = None
    bracket_name: Optional[str] = None

class BracketResponse(BracketBase):
    bracket_id: int = Field(..., description="Bracket ID")

    class Config:
        from_attributes = True



================================================================================
# File: app/schemas/portfolio/basket_details.py
================================================================================

from pydantic import BaseModel, Field, field_validator
from typing import Literal, Optional
from datetime import datetime


class BasketBase(BaseModel):
    basket_name: str = Field(..., description="Name of the basket")
    allocation_method: Literal["equal","manual"] = Field(..., description="Method of allocation")

class BasketCreate(BasketBase):
    pass

class BasketUpdate(BaseModel):
    basket_name: Optional[str] = None
    allocation_method: Optional[Literal["equal","manual"]] = None

class BasketResponse(BasketBase):
    basket_id: int = Field(..., description="Basket ID")
    created_at: datetime = Field(..., description="Record creation time")

    class Config:
        from_attributes = True


================================================================================
# File: app/schemas/portfolio/pf_bracket_basket_allocation.py
================================================================================

from pydantic import BaseModel, Field
from typing import Literal, Optional
from datetime import datetime


class PfBracketBasketAllocationBase(BaseModel):
    owner_id: str = Field(..., description="Owner ID (single or joint)")
    owner_type: Literal["single","joint"] = Field(..., description="Owner type indicator")
    bracket_id: Optional[int] = Field(None, description="Associated bracket ID")
    basket_id: Optional[int] = Field(None, description="Associated basket ID")
    portfolio_id: Optional[int] = Field(None, description="Associated portfolio ID")
    allocation_pct: float = Field(..., description="Allocation percentage")

class PfBracketBasketAllocationCreate(PfBracketBasketAllocationBase):
    pass

class PfBracketBasketAllocationUpdate(BaseModel):
    bracket_id: Optional[int] = None
    basket_id: Optional[int] = None
    portfolio_id: Optional[int] = None
    allocation_pct: Optional[float] = None

class PfBracketBasketAllocationResponse(PfBracketBasketAllocationBase):
    allocation_id: int = Field(..., description="Primary key for this allocation record")
    created_at: datetime = Field(..., description="Record creation time")

    class Config:
        from_attributes = True


================================================================================
# File: app/schemas/clients/broker_details.py
================================================================================

from pydantic import BaseModel, Field
from typing import Optional
from datetime import datetime


class BrokerBase(BaseModel):
    broker_name: str = Field(..., description="Name of the broker")

class BrokerCreate(BrokerBase):
    pass

class BrokerUpdate(BaseModel):
    broker_name: Optional[str] = None

class BrokerResponse(BrokerBase):
    broker_id: str = Field(..., description="Unique ID for the broker")
    created_at: datetime = Field(..., description="Timestamp of record creation")

    class Config:
        from_attributes = True



================================================================================
# File: app/schemas/clients/client_details.py
================================================================================

import re
from datetime import datetime
from pydantic import BaseModel, Field
from typing import Optional, List


class ClientListResponse(BaseModel):
    client_id: str = Field(..., description="Unique client identifier")
    account_id: Optional[str] = Field(..., description="Unique Account identifier")
    client_name: str = Field(..., description="Legal name of client")
    broker_name: str = Field(..., description="Associated broker name")
    broker_code: Optional[str] = Field(..., description="Associated broker code")
    broker_passwd: Optional[str] = Field(..., description="Password of the broker account")
    distributor_name: Optional[str] = Field(None, description="Distributor reference")
    pan_no: str = Field(..., description="PAN card number")
    country_code: Optional[str] = Field(None, description="Dialing code prefix")
    phone_no: Optional[str] = Field(None, description="Primary contact number")
    email_id: Optional[str] = Field(None, description="Registered email")
    addr: Optional[str] = Field(None, description="Current address")
    acc_start_date: Optional[str] = Field(None, description="Account activation date")
    type: Optional[str] = Field(None, description="Client classification")
    onboard_status: Optional[str] = Field(..., description="Active | Pending | Suspended")
    created_at: datetime = Field(..., description="Record creation timestamp")

    class Config:
        from_attributes = True
        json_encoders = {
            datetime: lambda v: v.isoformat()
        }
        json_schema_extra = {
            "example": {
                "client_id": "CLIENT_0001",
                "account_id": "ACC_00001",
                "client_name": "Acme Corp",
                "broker_name": "TradeMaster",
                "broker_code": "XXXXXX",
                "broker_passwd": "somepassword?",
                "distributor_name": "FinDistro",
                "pan_no": "ABCDE1234F",
                "country_code": "91",
                "phone_no": "9876543210",
                "email_id": "contact@acme.com",
                "addr": "Mumbai, India",
                "acc_start_date": "2024-01-01",
                "type": "Institutional",
                "onboard_status": "active",
                "created_at": "2024-03-15T12:34:56"
            }
        }

class ClientCreateRequest(BaseModel):
    """
    Request schema for creating or updating a client in bulk.
    Also used for partial updates if 'client_id' is present.
    """
    client_id: Optional[str] = None  # only used for update
    client_name: Optional[str] = Field(None, description="Name of the client (required for create)")
    broker_name: Optional[str] = Field(None, description="Name of the existing broker (required for create)")
    pan_no: Optional[str] = Field(None, description="PAN number (required for create)")
    broker_code: Optional[str] = None
    broker_passwd: Optional[str] = None
    email_id: Optional[str] = None
    country_code: Optional[str] = None
    phone_no: Optional[str] = None
    addr: Optional[str] = None
    acc_start_date: Optional[str] = None
    distributor_name: Optional[str] = None  # optional
    alias_name: Optional[str] = None
    alias_phone_no: Optional[str] = None
    alias_addr: Optional[str] = None
    type: Optional[str] = None

class BulkClientResult(BaseModel):
    """
    Indicates success/failure for a single row in a bulk operation.
    """
    row_index: int
    status: str  # "success" or "failed"
    detail: str
    client_id: Optional[str] = None

class BulkClientResponse(BaseModel):
    """
    Summarizes the entire bulk operation (create/update/delete).
    """
    total_rows: int
    processed_rows: int
    results: List[BulkClientResult]




================================================================================
# File: app/schemas/clients/__init__.py
================================================================================

from .client_details import (
    ClientCreateRequest,
    ClientListResponse,
    BulkClientResponse,
    BulkClientResult
    
)
from .broker_details import (
    BrokerBase,
    BrokerCreate,
    BrokerUpdate,
    BrokerResponse
)
from .distributor_details import (
    DistributorBase,
    DistributorCreate,
    DistributorUpdate,
    DistributorResponse
)

__all__ = [
    # Client
    "ClientCreateRequest",
    "ClientListResponse",
    "BulkClientResponse",
    "BulkClientResult"

    # Broker
    "BrokerBase",
    "BrokerCreate",
    "BrokerUpdate",
    "BrokerResponse",

    # Distributor
    "DistributorBase",
    "DistributorCreate",
    "DistributorUpdate",
    "DistributorResponse"
]

================================================================================
# File: app/schemas/clients/distributor_details.py
================================================================================

from pydantic import BaseModel, Field, field_validator
from typing import Optional
from datetime import datetime


class DistributorBase(BaseModel):
    client_id: Optional[str] = Field(None, description="If a client is also a distributor")
    is_internal: bool = Field(False, description="Is distributor internal or external")
    name: str = Field(..., description="Name of the distributor")
    email_id: Optional[str] = Field(None, description="Email ID")
    country_code: Optional[int] = Field(None, description="Country code")
    phone_no: Optional[int] = Field(None, description="Phone number")
    commission_rate: float = Field(50.0, description="Commission rate in percent")

    @field_validator("phone_no")
    def validate_phone_no(cls, v):
        if v and len(str(v)) > 15:
            raise ValueError("Phone number must be <= 15 digits")
        return v
    
    @field_validator("email_id")
    def validate_email(cls, v):
        if v is None:
            return None
        if isinstance(v, str) and "@" in v:
            return v.lower()
        raise ValueError("Invalid email address")

class DistributorCreate(DistributorBase):
    pass

class DistributorUpdate(BaseModel):
    name: Optional[str] = None
    email_id: Optional[str] = None
    country_code: Optional[int] = None
    phone_no: Optional[int] = None
    is_internal: Optional[bool] = None
    commission_rate: Optional[float] = None

class DistributorResponse(DistributorBase):
    distributor_id: str = Field(..., description="Unique ID for the distributor")
    created_at: datetime = Field(..., description="Timestamp of record creation")

    class Config:
        from_attributes = True



================================================================================
# File: app/schemas/accounts/joint_account.py
================================================================================

from pydantic import BaseModel, Field
from typing import Optional, List
from datetime import datetime


class JointAccountCreateRequest(BaseModel):
    joint_account_name: str = Field(..., description="Name of the joint account.")
    single_account_ids: List[str] = Field(
        default_factory=list,
        description="List of SingleAccount IDs to link to this joint account."
    )

class JointAccountUpdateRequest(BaseModel):
    joint_account_name: Optional[str] = Field(
        None,
        description="Updated name of the joint account (if changing)."
    )
    single_account_ids: Optional[List[str]] = Field(
        None,
        description="New or updated list of SingleAccount IDs for this joint account."
    )


class JointAccountDeleteRequest(BaseModel):
    joint_account_id: str = Field(..., description="ID of the joint account to delete.")

class JointAccountResponse(BaseModel):
    status: str = Field(..., description="Operation status: 'success' or 'failed'.")
    joint_account_id: str = Field(..., description="Unique ID of the joint account.")
    joint_account_name: Optional[str] = Field(None, description="Name of the joint account.")
    linked_single_accounts: List[str] = Field(
        default_factory=list,
        description="List of single account IDs linked to this joint account."
    )

================================================================================
# File: app/schemas/accounts/account_bracket_basket_allocation.py
================================================================================

from pydantic import BaseModel, Field
from typing import Literal, Optional
from datetime import datetime

class AccountBracketBasketAllocationBase(BaseModel):
    account_id: str = Field(..., description="Account ID (single or joint)")
    account_type: Literal["single", "joint"] = Field(..., description="Type of account")
    bracket_id: int = Field(..., description="Bracket ID")
    basket_id: int = Field(..., description="Basket ID")
    allocation_pct: float = Field(..., description="Custom allocation percentage")
    is_custom: bool = Field(..., description="Whether this is a custom allocation")

class AccountBracketBasketAllocationCreate(AccountBracketBasketAllocationBase):
    pass

class AccountBracketBasketAllocationUpdate(BaseModel):
    allocation_pct: Optional[float] = Field(None, description="Updated allocation percentage")
    is_custom: Optional[bool] = Field(None, description="Updated custom flag")

class AccountBracketBasketAllocationResponse(AccountBracketBasketAllocationBase):
    id: int = Field(..., description="Unique identifier for the allocation record")
    created_at: datetime = Field(..., description="Record creation timestamp")

    class Config:
        from_attributes = True

================================================================================
# File: app/schemas/accounts/account_performance.py
================================================================================

from pydantic import BaseModel, Field
from typing import Literal, Optional
from datetime import datetime


class AccountPerformanceBase(BaseModel):
    owner_id: str = Field(..., description="Single or joint account ID")
    owner_type: Literal["single","joint"] = Field(..., description="Owner type")
    total_twrr: Optional[float] = Field(None, description="Total time-weighted return")
    current_yr_twrr: Optional[float] = Field(None, description="Current year's TWRR")
    cagr: Optional[float] = Field(None, description="Compound annual growth rate")

class AccountPerformanceCreate(AccountPerformanceBase):
    performance_id: str = Field(..., description="Unique performance ID if not system-generated")

class AccountPerformanceUpdate(BaseModel):
    total_twrr: Optional[float] = None
    current_yr_twrr: Optional[float] = None
    cagr: Optional[float] = None

class AccountPerformanceResponse(AccountPerformanceBase):
    performance_id: str = Field(..., description="Unique performance ID")
    created_at: datetime = Field(..., description="Record creation time")

    class Config:
        from_attributes = True


================================================================================
# File: app/schemas/accounts/account_cashflow_progression.py
================================================================================

from pydantic import BaseModel, Field
from typing import Literal, Optional
from datetime import date, datetime

class AccountCashflowProgressionBase(BaseModel):
    owner_id: str = Field(..., description="Owner ID (single or joint account)")
    owner_type: Literal["single", "joint"] = Field(..., description="Type of account owner")
    event_date: date = Field(..., description="Date of the cashflow event")
    cashflow: float = Field(..., description="Cashflow amount")
    portfolio_value: float = Field(..., description="Portfolio value at the event date")
    portfolio_plus_cash: float = Field(..., description="Portfolio value plus cash at the event date")

class AccountCashflowProgressionCreate(AccountCashflowProgressionBase):
    pass

class AccountCashflowProgressionUpdate(BaseModel):
    cashflow: Optional[float] = Field(None, description="Updated cashflow amount")
    portfolio_value: Optional[float] = Field(None, description="Updated portfolio value")
    portfolio_plus_cash: Optional[float] = Field(None, description="Updated portfolio plus cash value")

class AccountCashflowProgressionResponse(AccountCashflowProgressionBase):
    id: int = Field(..., description="Unique identifier for the progression record")
    created_at: datetime = Field(..., description="Record creation timestamp")

    class Config:
        from_attributes = True

================================================================================
# File: app/schemas/accounts/account_ideal_portfolio.py
================================================================================

from pydantic import BaseModel, Field
from typing import Literal, Optional
from datetime import datetime


class AccountIdealPortfolioBase(BaseModel):
    owner_id: str = Field(..., description="Owner ID (single account or joint account)")
    owner_type: Literal["single", "joint"] = Field(..., description="Indicates single or joint")
    basket: str = Field(..., description="Name of the basket or strategy")
    trading_symbol: str = Field(..., description="Stock trading symbol")
    allocation_pct: float = Field(..., description="Allocation percentage")
    investment_amount: float = Field(..., description="Investment amount in currency")

class AccountIdealPortfolioCreate(AccountIdealPortfolioBase):
    pass

class AccountIdealPortfolioUpdate(BaseModel):
    basket: Optional[str] = None
    trading_symbol: Optional[str] = None
    allocation_pct: Optional[float] = None
    investment_amount: Optional[float] = None

class AccountIdealPortfolioResponse(AccountIdealPortfolioBase):
    ideal_portfolio_id: int = Field(..., description="Unique ID for the ideal portfolio record")
    created_at: datetime = Field(..., description="Creation timestamp")

    class Config:
        from_attributes = True


================================================================================
# File: app/schemas/accounts/account_actual_portfolio.py
================================================================================

from pydantic import BaseModel, Field
from typing import Literal, Optional
from datetime import datetime


class AccountActualPortfolioBase(BaseModel):
    owner_id: str = Field(..., description="ID of the associated single or joint account")
    owner_type: Literal["single", "joint"] = Field(..., description="Account type indicator")
    trading_symbol: str = Field(..., description="Trading symbol for this holding")
    quantity: float = Field(..., description="Number of units held")
    market_value: float = Field(..., description="Current market value")

class AccountActualPortfolioCreate(AccountActualPortfolioBase):
    pass

class AccountActualPortfolioUpdate(BaseModel):
    quantity: Optional[float] = None
    market_value: Optional[float] = None

class AccountActualPortfolioResponse(AccountActualPortfolioBase):
    created_at: datetime = Field(..., description="Record creation time")

    class Config:
        from_attributes = True


================================================================================
# File: app/schemas/accounts/account_cashflow_details.py
================================================================================

from pydantic import BaseModel, Field
from typing import Literal, Optional
from datetime import date, datetime


class AccountCashflowBase(BaseModel):
    owner_id: str = Field(..., description="Single or joint account ID")
    owner_type: Literal["single","joint"] = Field(..., description="Owner type")
    event_date: date = Field(..., description="Date of the cashflow")
    cashflow: float = Field(..., description="Cashflow amount")
    tag: Optional[str] = Field(None, description="Additional tag or note")

class AccountCashflowCreate(AccountCashflowBase):
    pass

class AccountCashflowUpdate(BaseModel):
    event_date: Optional[date] = None
    cashflow: Optional[float] = None
    tag: Optional[str] = None

class AccountCashflowResponse(AccountCashflowBase):
    cashflow_id: int = Field(..., description="Primary key for this cashflow record")
    created_at: datetime = Field(..., description="Record creation time")

    class Config:
        from_attributes = True


================================================================================
# File: app/schemas/accounts/account.py
================================================================================

import math
from typing import Optional, List, Literal
from pydantic import BaseModel, Field
from datetime import datetime


def sanitize_float(value):
    """
    Sanitize a float value by replacing NaN or Infinity with None.
    
    Args:
        value: The value to sanitize (could be None, float, or other types).
    
    Returns:
        None if the value is NaN or Infinity, otherwise the original value.
    """
    if value is None:
        return None
    if isinstance(value, float) and (math.isnan(value) or math.isinf(value)):
        return None
    return value

class ViewAccount(BaseModel):
    account_type: str = Field(..., description="Type of account: 'single' or 'joint'")
    account_id: str = Field(..., description="Unique account ID (single_account_id or joint_account_id)")
    account_name: str = Field(..., description="Name of the single or joint account")

    bracket_name: Optional[str] = Field(None, description="Friendly name of the associated bracket")
    portfolio_name: Optional[str] = Field(None, description="Friendly name of the associated portfolio template")

    pf_value: Optional[float] = Field(None, description="Portfolio value")
    cash_value: Optional[float] = Field(None, description="Current cash balance in the account")
    total_holdings: Optional[float] = Field(None, description="Total worth of all holdings in the account")
    invested_amt: Optional[float] = Field(None, description="Total invested amount so far")

    total_twrr: Optional[float] = Field(None, description="Time-weighted rate of return (overall)")
    current_yr_twrr: Optional[float] = Field(None, description="Current year TWRR (time-weighted rate of return)")
    cagr: Optional[float] = Field(None, description="Compound annual growth rate")

    created_at: Optional[str] = Field(None, description="Timestamp of account creation, in ISO format")

    class Config:
        json_encoders = {
            float: sanitize_float
        }

class ViewAccountsResponse(BaseModel):
    status: str = Field("success", description="Response status, e.g., 'success' or 'error'")
    data: List[ViewAccount] = Field(..., description="List of accounts with bracket, portfolio, and TWRR details")


class AccountUpdateRequest(BaseModel):
    account_id: str = Field(..., description="ID of the account, e.g. ACC_000001 or JACC_000001")
    account_type: Literal["single", "joint"] = Field(..., description="Either 'single' or 'joint'")
    pf_value: Optional[float] = None
    cash_value: Optional[float] = None
    invested_amt: Optional[float] = None
    total_twrr: Optional[float] = None
    current_yr_twrr: Optional[float] = None
    cagr: Optional[float] = None

class BulkAccountResult(BaseModel):
    row_index: int
    status: str
    detail: str
    account_id: Optional[str] = None

class BulkAccountResponse(BaseModel):
    total_rows: int
    processed_rows: int
    results: list[BulkAccountResult]




================================================================================
# File: app/schemas/accounts/__init__.py
================================================================================

from .account import (
    ViewAccount,
    ViewAccountsResponse
)

from .joint_account import (
    JointAccountCreateRequest,
    JointAccountDeleteRequest,
    JointAccountResponse,
    JointAccountUpdateRequest
)

# Ideal Portfolio
from .account_ideal_portfolio import (
    AccountIdealPortfolioBase,
    AccountIdealPortfolioCreate,
    AccountIdealPortfolioUpdate,
    AccountIdealPortfolioResponse
)

# Polymorphic Tables
from .account_actual_portfolio import (
    AccountActualPortfolioBase,
    AccountActualPortfolioCreate,
    AccountActualPortfolioUpdate,
    AccountActualPortfolioResponse
)
from .account_cashflow_details import (
    AccountCashflowBase,
    AccountCashflowCreate,
    AccountCashflowUpdate,
    AccountCashflowResponse
)
from .account_performance import (
    AccountPerformanceBase,
    AccountPerformanceCreate,
    AccountPerformanceUpdate,
    AccountPerformanceResponse
)
from .account_time_periods import (
    AccountTimePeriodsBase,
    AccountTimePeriodsCreate,
    AccountTimePeriodsUpdate,
    AccountTimePeriodsResponse
)
from .account_cashflow_progression import (
    AccountCashflowProgressionBase,
    AccountCashflowProgressionCreate,
    AccountCashflowProgressionResponse,
    AccountCashflowProgressionUpdate
)

__all__ = [
    # Account
    "ViewAccount",
    "ViewAccountsResponse",

    # Joint Account
    "JointAccountCreateRequest",
    "JointAccountDeleteRequest",
    "JointAccountResponse",
    "JointAccountUpdateRequest"
    
    # Ideal portfolio
    "AccountIdealPortfolioBase",
    "AccountIdealPortfolioCreate",
    "AccountIdealPortfolioUpdate",
    "AccountIdealPortfolioResponse",

    # Polymorphic: account_actual_portfolio
    "AccountActualPortfolioBase",
    "AccountActualPortfolioCreate",
    "AccountActualPortfolioUpdate",
    "AccountActualPortfolioResponse",

    # Polymorphic: account_cashflow_details
    "AccountCashflowBase",
    "AccountCashflowCreate",
    "AccountCashflowUpdate",
    "AccountCashflowResponse",

    # Polymorphic: account_performance
    "AccountPerformanceBase",
    "AccountPerformanceCreate",
    "AccountPerformanceUpdate",
    "AccountPerformanceResponse",

    # Polymorphic: account_time_periods
    "AccountTimePeriodsBase",
    "AccountTimePeriodsCreate",
    "AccountTimePeriodsUpdate",
    "AccountTimePeriodsResponse",

    "AccountCashflowProgressionBase",
    "AccountCashflowProgressionCreate",
    "AccountCashflowProgressionResponse",
    "AccountCashflowProgressionUpdate"
]

================================================================================
# File: app/schemas/accounts/account_time_periods.py
================================================================================

from pydantic import BaseModel, Field
from typing import Literal, Optional
from datetime import date, datetime


class AccountTimePeriodsBase(BaseModel):
    owner_id: str = Field(..., description="Single or joint account ID")
    owner_type: Literal["single","joint"] = Field(..., description="Owner type")
    start_date: date = Field(..., description="Start date of the period")
    start_value: float = Field(..., description="Value at the start date")
    end_date: date = Field(..., description="End date of the period")
    end_value: float = Field(..., description="Value at the end date")
    returns: float = Field(..., description="Returns fraction or percent")
    returns_1: Optional[float] = Field(None, description="Optional additional returns metric")

class AccountTimePeriodsCreate(AccountTimePeriodsBase):
    pass

class AccountTimePeriodsUpdate(BaseModel):
    start_date: Optional[date] = None
    start_value: Optional[float] = None
    end_date: Optional[date] = None
    end_value: Optional[float] = None
    returns: Optional[float] = None
    returns_1: Optional[float] = None

class AccountTimePeriodsResponse(AccountTimePeriodsBase):
    time_period_id: int = Field(..., description="Primary key for the time period record")
    created_at: datetime = Field(..., description="Record creation time")

    class Config:
        from_attributes = True


================================================================================
# File: app/services/report_service.py
================================================================================

from datetime import datetime
from dateutil.relativedelta import relativedelta
import boto3
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select
from app.models.clients.client_details import Client
from app.models.accounts.single_account import SingleAccount
from app.models.accounts.joint_account import JointAccount
from app.models.accounts.joint_account_mapping import JointAccountMapping

import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from email.mime.application import MIMEApplication


class ReportService:
    def __init__(self, db: AsyncSession, s3_client):
        self.db = db
        self.s3_client = s3_client
        self.bucket_name = "plus91backoffice"

    async def verify_user(self, broker_code: str, pan_no: str) -> Client:
        """Verify the client based on broker_code and pan_no."""
        stmt = select(Client).where(
            Client.broker_code == broker_code,
            Client.pan_no == pan_no
        )
        result = await self.db.execute(stmt)
        client = result.scalars().first()
        if not client:
            raise ValueError("Client not found or PAN number does not match")
        return client

    async def get_accounts(self, client: Client) -> dict:
        """Retrieve single and joint accounts for the client."""
        stmt_single = select(SingleAccount).where(
            SingleAccount.single_account_id == client.account_id
        )
        result_single = await self.db.execute(stmt_single)
        single_account = result_single.scalars().first()

        stmt_mappings = select(JointAccountMapping).where(
            JointAccountMapping.account_id == single_account.single_account_id
        )
        result_mappings = await self.db.execute(stmt_mappings)
        mappings = result_mappings.scalars().all()
        joint_account_ids = [m.joint_account_id for m in mappings]

        stmt_joint = select(JointAccount).where(
            JointAccount.joint_account_id.in_(joint_account_ids)
        )
        result_joint = await self.db.execute(stmt_joint)
        joint_accounts = result_joint.scalars().all()

        return {"single": single_account, "joint": joint_accounts}

    def get_latest_report(self, broker_codes: list, is_joint: bool) -> bytes:
        """Fetch the latest report from S3 based on broker codes."""
        account_id = self._get_account_identifier(broker_codes, is_joint)
        current_date = datetime.now()
        for _ in range(60):
            year = current_date.strftime("%Y")
            month = current_date.strftime("%b").upper()
            file_name = f"{account_id} {month} {year} Report.pdf"
            s3_key = f"PLUS91_PMS/reports/{year}/{month}/{file_name}"
            try:
                obj = self.s3_client.get_object(Bucket=self.bucket_name, Key=s3_key)
                return obj['Body'].read()
            except self.s3_client.exceptions.NoSuchKey:
                current_date = current_date - relativedelta(months=1)
        return None

    def _get_account_identifier(self, broker_codes: list, is_joint: bool) -> str:
        """Generate the account identifier for the file name."""
        if is_joint:
            sorted_codes = sorted(broker_codes)
            return f"[{' - '.join(sorted_codes)}]"
        else:
            return f"[{broker_codes[0]}]"

    def send_reports_email(self, email: str, reports: list):
        """Send an email with the reports as attachments."""
        msg = MIMEMultipart()
        msg['From'] = " pratham@plus91.co"
        msg['To'] = email
        msg['Subject'] = "Latest Reports"
        body = "This mail is to test the report mailing to the clients. Incorrect values " \
        "will be corrected in sometime."
        msg.attach(MIMEText(body, 'plain'))

        for report_data, filename in reports:
            attachment = MIMEApplication(report_data, _subtype="pdf")
            attachment.add_header('Content-Disposition', 'attachment', filename=filename)
            msg.attach(attachment)

        with smtplib.SMTP("smtp.gmail.com", 587) as server:
            server.starttls()
            server.login(msg['From'], "feok yolq wqgs ieoo")
            server.send_message(msg)
        
    

================================================================================
# File: app/services/__init__.py
================================================================================



================================================================================
# File: app/services/email_service.py
================================================================================

import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from email.mime.application import MIMEApplication
from dotenv import load_dotenv


def send_email_with_attachment(
        from_email: str,
        to_email: str,
        subject: str,
        message: str,
        pdf_path: str
        ):
    msg = MIMEMultipart()
    msg['From'] = from_email
    msg['To'] = to_email
    msg['Subject'] = subject

    msg.attach(MIMEText(message, 'plain'))

    with open(pdf_path, "rb") as pdf_file:
        pdf_attachment = MIMEApplication(pdf_file.read(), _subtype="pdf")
        pdf_attachment.add_header('Content-Disposition', 'attachment', filename="attachment.pdf")
        msg.attach(pdf_attachment)

    smtp_server = "smtp.gmail.com"
    smtp_port = 587
    password = "iile fnhj pyct dgcf"

    try:
        with smtplib.SMTP(smtp_server, smtp_port) as server:
            server.starttls()
            server.login(from_email, password)
            server.send_message(msg)
        print(f"Email with attachment sent successfully to {to_email}!")
    except Exception as e:
        print(f"Failed to send email: {e}")



================================================================================
# File: app/services/portfolio/portfolio_service.py
================================================================================

from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select
from app.models.portfolio.portfolio_template_details import PortfolioTemplate
from app.models.portfolio.bracket_details import Bracket
from app.models.portfolio.basket_details import Basket
from app.models.portfolio.pf_bracket_basket_allocation import PfBracketBasketAllocation
from app.models.portfolio.basket_stock_mapping import BasketStockMapping
from app.logger import logger, log_function_call
from typing import Dict, Any, List

class PortfolioService:
    @staticmethod
    @log_function_call
    async def get_portfolios(db: AsyncSession) -> List[Dict[str, Any]]:
        """Fetch all portfolio templates from the database, ordered by portfolio_id."""
        logger.info("Fetching all portfolio templates")
        q = await db.execute(select(PortfolioTemplate).order_by(PortfolioTemplate.portfolio_id))
        rows = q.scalars().all()
        data = [
            {
                "portfolio_id": r.portfolio_id,
                "portfolio_name": r.portfolio_name,
                "description": r.description
            }
            for r in rows
        ]
        logger.debug(f"Retrieved {len(data)} portfolio templates")
        return data

    @staticmethod
    @log_function_call
    async def get_portfolio_structure(db: AsyncSession, portfolio_id: int) -> Dict[str, Any]:
        """Fetch the complete structure of a portfolio, including brackets, baskets, allocations, and basket stocks."""
        logger.info(f"Fetching portfolio structure for portfolio_id: {portfolio_id}")
        p = await db.get(PortfolioTemplate, portfolio_id)
        if not p:
            logger.warning(f"Portfolio with ID {portfolio_id} not found")
            return {}
        
        logger.debug("Fetching brackets")
        qb = await db.execute(select(Bracket).order_by(Bracket.bracket_id))
        brackets = qb.scalars().all()
        
        logger.debug("Fetching baskets")
        qb2 = await db.execute(select(Basket).order_by(Basket.basket_id))
        baskets = qb2.scalars().all()
        
        logger.debug(f"Fetching allocations for portfolio_id: {portfolio_id}")
        qa = await db.execute(select(PfBracketBasketAllocation).where(PfBracketBasketAllocation.portfolio_id == portfolio_id))
        allocations = qa.scalars().all()
        
        bracket_list = [
            {
                "bracket_id": b.bracket_id,
                "bracket_name": b.bracket_name,
                "min_amount": b.bracket_min,
                "max_amount": b.bracket_max
            }
            for b in brackets
        ]
        
        basket_list = [
            {
                "basket_id": b.basket_id,
                "basket_name": b.basket_name,
                "allocation_method": b.allocation_method
            }
            for b in baskets
        ]
        
        alloc_map = {(a.bracket_id, a.basket_id): a.allocation_pct for a in allocations}
        
        basket_stocks = {}
        for b in baskets:
            logger.debug(f"Fetching stocks for basket: {b.basket_name}")
            qbsm = await db.execute(select(BasketStockMapping).where(BasketStockMapping.basket_id == b.basket_id))
            stocks = qbsm.scalars().all()
            stock_list = [
                {
                    "basket_stock_mapping_id": s.basket_stock_mapping_id,
                    "stock": s.trading_symbol,
                    "multiplier": s.multiplier
                }
                for s in stocks
            ]
            basket_stocks[b.basket_name] = stock_list
        
        logger.info(f"Portfolio structure fetched successfully for portfolio_id: {portfolio_id}")
        return {
            "portfolio_id": p.portfolio_id,
            "portfolio_name": p.portfolio_name,
            "description": p.description,
            "brackets": bracket_list,
            "baskets": basket_list,
            "allocations": alloc_map,
            "basket_stocks": basket_stocks
        }

    @staticmethod
    @log_function_call
    async def save_portfolio_structure(db: AsyncSession, portfolio_id: int, payload: Dict[str, Any]) -> Dict[str, Any]:
        """Save or update the portfolio structure, including brackets, baskets, allocations, and basket stocks."""
        logger.info(f"Saving portfolio structure for portfolio_id: {portfolio_id}")
        brackets_data = payload.get("brackets", [])
        baskets_data = payload.get("baskets", [])
        alloc_map = payload.get("allocations", {})
        basket_stocks = payload.get("basket_stocks", {})

        # Fetch existing brackets and baskets
        logger.debug("Fetching existing brackets")
        qb = await db.execute(select(Bracket))
        existing_brackets = qb.scalars().all()
        bracket_by_id = {str(b.bracket_id): b for b in existing_brackets}

        logger.debug("Fetching existing baskets")
        qb2 = await db.execute(select(Basket))
        existing_baskets = qb2.scalars().all()
        basket_by_id = {str(b.basket_id): b for b in existing_baskets}

        # Fetch existing allocations for the portfolio
        logger.debug(f"Fetching existing allocations for portfolio_id: {portfolio_id}")
        qa = await db.execute(
            select(PfBracketBasketAllocation).where(PfBracketBasketAllocation.portfolio_id == portfolio_id)
        )
        existing_allocs = qa.scalars().all()
        alloc_index = {(str(a.bracket_id), str(a.basket_id)): a for a in existing_allocs}

        # Process brackets
        bracket_ids_seen = set()
        for br in brackets_data:
            bracket_id = (br.get("bracket_id") or "").strip()
            b_name = (br.get("bracket_name") or "").strip()
            min_amt = br.get("min_amount", 0)
            max_amt = br.get("max_amount", 0)

            if not b_name and min_amt == 0 and max_amt == 0 and bracket_id:
                continue
            if bracket_id:
                existing_br = bracket_by_id.get(bracket_id)
                if existing_br:
                    if not b_name:
                        b_name = f"BRACKET_{existing_br.bracket_id}"
                    if min_amt <= max_amt:
                        existing_br.bracket_name = b_name
                        existing_br.bracket_min = min_amt
                        existing_br.bracket_max = max_amt
                    bracket_ids_seen.add(bracket_id)
            else:
                if not b_name:
                    b_name = "BRACKET_NULL"
                new_br = Bracket(bracket_name=b_name, bracket_min=min_amt, bracket_max=max_amt)
                db.add(new_br)
                await db.flush()
                bracket_ids_seen.add(str(new_br.bracket_id))
                br["bracket_id"] = str(new_br.bracket_id)
        await db.commit()
        logger.info("Brackets saved/updated successfully")

        # Process baskets
        basket_ids_seen = set()
        for bs in baskets_data:
            basket_id = (bs.get("basket_id") or "").strip()
            b_name = (bs.get("basket_name") or "").strip()
            allocation_method = bs.get("allocation_method", "manual")
            if not b_name and basket_id:
                continue
            if basket_id:
                existing_ba = basket_by_id.get(basket_id)
                if existing_ba:
                    if not b_name:
                        b_name = f"BASKET_{existing_ba.basket_id}"
                    existing_ba.basket_name = b_name
                    existing_ba.allocation_method = allocation_method
                    basket_ids_seen.add(basket_id)
            else:
                if not b_name:
                    b_name = "BASKET_NULL"
                new_bs = Basket(basket_name=b_name, allocation_method=allocation_method)
                db.add(new_bs)
                await db.flush()
                basket_ids_seen.add(str(new_bs.basket_id))
                bs["basket_id"] = str(new_bs.basket_id)
        await db.commit()
        logger.info("Baskets saved/updated successfully")

        # Delete unused brackets and baskets
        qb3 = await db.execute(select(Bracket))
        all_brackets_after = qb3.scalars().all()
        qb4 = await db.execute(select(Basket))
        all_baskets_after = qb4.scalars().all()

        for oldb in all_brackets_after:
            str_id = str(oldb.bracket_id)
            if str_id not in bracket_ids_seen:
                await db.delete(oldb)
                logger.debug(f"Deleted unused bracket: {str_id}")

        for oldb in all_baskets_after:
            str_id = str(oldb.basket_id)
            if str_id not in basket_ids_seen:
                await db.delete(oldb)
                logger.debug(f"Deleted unused basket: {str_id}")

        await db.commit()

        # Update allocations
        qb5 = await db.execute(select(Bracket))
        final_brackets = qb5.scalars().all()
        bracket_by_id_final = {str(b.bracket_id): b for b in final_brackets}

        qb6 = await db.execute(select(Basket))
        final_baskets = qb6.scalars().all()
        basket_by_id_final = {str(b.basket_id): b for b in final_baskets}

        for k, v in alloc_map.items():
            parts = k.split("::", 1)
            if len(parts) != 2:
                continue
            br_id_str, ba_id_str = parts[0].strip(), parts[1].strip()
            if not br_id_str or not ba_id_str:
                continue

            br_obj = bracket_by_id_final.get(br_id_str)
            ba_obj = basket_by_id_final.get(ba_id_str)
            if not br_obj or not ba_obj:
                continue

            pair = (br_id_str, ba_id_str)
            ex_alloc = alloc_index.get(pair)
            if ex_alloc:
                ex_alloc.allocation_pct = v
            else:
                new_alloc = PfBracketBasketAllocation(
                    portfolio_id=portfolio_id,
                    bracket_id=br_obj.bracket_id,
                    basket_id=ba_obj.basket_id,
                    allocation_pct=v
                )
                db.add(new_alloc)
                logger.debug(f"Added new allocation for bracket {br_id_str} and basket {ba_id_str}")

        await db.commit()
        logger.info("Allocations saved/updated successfully")

        # Update basket stocks
        qb7 = await db.execute(select(Basket))
        updated_baskets = qb7.scalars().all()
        updated_baskets_by_id = {str(b.basket_id): b for b in updated_baskets}

        for bId, stlist in basket_stocks.items():
            b_obj = updated_baskets_by_id.get(bId.strip())
            if not b_obj:
                continue

            logger.debug(f"Updating stocks for basket: {b_obj.basket_name}")
            qbsm = await db.execute(select(BasketStockMapping).where(BasketStockMapping.basket_id == b_obj.basket_id))
            old_stocks = qbsm.scalars().all()
            old_map = {str(o.basket_stock_mapping_id): o for o in old_stocks}
            new_ids = set()

            for row in stlist:
                sy = (row.get("stock") or "").strip()
                mt = row.get("multiplier", 0)
                mapping_id = row.get("basket_stock_mapping_id")

                if not sy and mt == 0 and not mapping_id:
                    continue

                if mapping_id:
                    old_record = old_map.get(mapping_id)
                    if old_record:
                        old_record.trading_symbol = sy
                        old_record.multiplier = mt
                    new_ids.add(mapping_id)
                else:
                    nm = BasketStockMapping(
                        basket_id=b_obj.basket_id,
                        trading_symbol=sy if sy else "STOCK_NULL",
                        multiplier=mt
                    )
                    db.add(nm)
                    await db.flush()
                    new_ids.add(str(nm.basket_stock_mapping_id))
                    logger.debug(f"Added new stock mapping for basket {b_obj.basket_id}")

            for old_mid, old_ob in old_map.items():
                if old_mid not in new_ids:
                    await db.delete(old_ob)
                    logger.debug(f"Deleted unused stock mapping: {old_mid}")

        await db.commit()
        logger.info(f"Portfolio structure saved successfully for portfolio_id: {portfolio_id}")
        return {"status": "ok"}

================================================================================
# File: app/services/clients/clients_service.py
================================================================================

from typing import List, Optional
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select
from sqlalchemy.orm import selectinload
from app.models.clients.client_details import Client
from app.models.clients.broker_details import Broker
from app.models.clients.distributor_details import Distributor
from app.models.accounts.single_account import SingleAccount
from app.schemas.clients.client_details import (
    ClientCreateRequest,
    BulkClientResponse,
    BulkClientResult,
)
from app.logger import logger, log_function_call

class ClientService:
    @staticmethod
    @log_function_call
    async def get_all_clients(db: AsyncSession) -> List[Client]:
        """Fetch all client records with related broker and distributor data."""
        logger.info("Starting fetch of all client records")
        try:
            result = await db.execute(
                select(Client)
                .options(
                    selectinload(Client.broker),
                    selectinload(Client.distributor),
                    selectinload(Client.account)
                )
            )
            clients = result.scalars().all()
            logger.debug(f"Successfully retrieved {len(clients)} client records")
            return clients
        except Exception as e:
            logger.error(f"Database error fetching clients: {str(e)}", exc_info=True)
            raise ValueError("Failed to retrieve clients due to database error") from e

    @staticmethod
    @log_function_call
    async def bulk_create_clients(
        db: AsyncSession,
        data_list: List[ClientCreateRequest],
    ) -> BulkClientResponse:
        """Bulk create clients, each in its own transaction block for partial success."""
        logger.info(f"Starting bulk create for {len(data_list)} clients")
        total = len(data_list)
        results: List[BulkClientResult] = []
        processed_count = 0

        for idx, row in enumerate(data_list):
            row_index = idx + 1
            logger.debug(f"Processing row {row_index}")
            if not row.client_name or not row.broker_name or not row.pan_no:
                msg = "Missing mandatory field: client_name, broker_name, or pan_no."
                results.append(BulkClientResult(
                    row_index=row_index,
                    status="failed",
                    detail=msg
                ))
                logger.warning(f"Row {row_index} - {msg}")
                continue

            async with db.begin():
                try:
                    broker = await ClientService._get_broker_by_name(db, row.broker_name)
                    if not broker:
                        detail = f"Broker '{row.broker_name}' not found."
                        results.append(BulkClientResult(
                            row_index=row_index,
                            status="failed",
                            detail=detail
                        ))
                        logger.warning(f"Row {row_index} - {detail}")
                        continue

                    new_acc = SingleAccount(
                        account_name=row.client_name[:50].strip().title(),
                        account_type="single",
                        portfolio_id=1,
                        bracket_id=None,
                    )
                    db.add(new_acc)
                    await db.flush()
                    logger.debug(f"Created SingleAccount with ID: {new_acc.single_account_id}")

                    new_client = Client(
                        client_name=row.client_name.strip().title(),
                        broker_id=broker.broker_id,
                        broker_code=(row.broker_code or "").strip(),
                        broker_passwd=(row.broker_passwd or "").strip(),
                        email_id=(row.email_id or "").strip() if row.email_id else None,
                        pan_no=row.pan_no.strip(),
                        phone_no=row.phone_no,
                        country_code=row.country_code,
                        addr=(row.addr or "").strip(),
                        acc_start_date=row.acc_start_date,
                        distributor_id=None,
                        account_id=new_acc.single_account_id,
                        type=(row.type or "").strip(),
                        alias_name=(row.alias_name or "").strip(),
                        alias_phone_no=(row.alias_phone_no or "").strip(),
                        alias_addr=(row.alias_addr or "").strip(),
                        onboard_status="pending"
                    )
                    if row.distributor_name:
                        dist_id = await ClientService._get_distributor_id(db, row.distributor_name)
                        if dist_id:
                            new_client.distributor_id = dist_id
                            logger.debug(f"Assigned distributor ID: {dist_id}")
                        else:
                            logger.info(f"Distributor '{row.distributor_name}' not found for row {row_index}, ignoring.")

                    db.add(new_client)
                    await db.flush()
                    logger.info(f"Client created with ID: {new_client.client_id}")

                    results.append(BulkClientResult(
                        row_index=row_index,
                        status="success",
                        detail="Inserted successfully",
                        client_id=new_client.client_id
                    ))
                    processed_count += 1

                except Exception as exc:
                    logger.error(f"Row {row_index} create failed: {str(exc)}", exc_info=True)
                    results.append(BulkClientResult(
                        row_index=row_index,
                        status="failed",
                        detail=f"DB error: {str(exc)}"
                    ))

        logger.info(f"Bulk create completed: {processed_count}/{total} rows processed")
        return BulkClientResponse(
            total_rows=total,
            processed_rows=processed_count,
            results=results
        )

    @staticmethod
    @log_function_call
    async def bulk_update_clients(
        db: AsyncSession,
        data_list: List[ClientCreateRequest],
    ) -> BulkClientResponse:
        """Bulk update clients with partial success."""
        logger.info(f"Starting bulk update for {len(data_list)} clients")
        total = len(data_list)
        results: List[BulkClientResult] = []
        processed_count = 0

        for idx, row in enumerate(data_list):
            row_index = idx + 1
            logger.debug(f"Processing row {row_index}")
            if not row.client_id:
                detail = "No client_id provided for update."
                results.append(BulkClientResult(
                    row_index=row_index,
                    status="failed",
                    detail=detail
                ))
                logger.warning(f"Row {row_index} - {detail}")
                continue

            async with db.begin():
                try:
                    client_obj = await db.get(Client, row.client_id)
                    if not client_obj:
                        detail = f"Client '{row.client_id}' not found."
                        results.append(BulkClientResult(
                            row_index=row_index,
                            status="failed",
                            detail=detail
                        ))
                        logger.warning(f"Row {row_index} - {detail}")
                        continue

                    if row.broker_name is not None:
                        broker = await ClientService._get_broker_by_name(db, row.broker_name)
                        if not broker:
                            detail = f"Broker '{row.broker_name}' not found."
                            results.append(BulkClientResult(
                                row_index=row_index,
                                status="failed",
                                detail=detail
                            ))
                            logger.warning(f"Row {row_index} - {detail}")
                            continue
                        client_obj.broker_id = broker.broker_id
                        logger.debug(f"Updated broker_id to {broker.broker_id}")

                    if row.client_name is not None:
                        client_obj.client_name = row.client_name.strip()
                    if row.broker_code is not None:
                        client_obj.broker_code = row.broker_code.strip()
                    if row.broker_passwd is not None:
                        client_obj.broker_passwd = row.broker_passwd.strip()
                    if row.pan_no is not None:
                        client_obj.pan_no = row.pan_no.strip()
                    if row.email_id is not None:
                        client_obj.email_id = row.email_id.strip()
                    if row.country_code is not None:
                        client_obj.country_code = row.country_code
                    if row.phone_no is not None:
                        client_obj.phone_no = row.phone_no
                    if row.addr is not None:
                        client_obj.addr = row.addr.strip()
                    if row.acc_start_date is not None:
                        client_obj.acc_start_date = row.acc_start_date
                        print("ACC_START_DATE: ", row.acc_start_date)
                    if row.distributor_name is not None:
                        dist_id = await ClientService._get_distributor_id(db, row.distributor_name)
                        client_obj.distributor_id = dist_id
                        logger.debug(f"Updated distributor_id to {dist_id}")
                    if row.type is not None:
                        client_obj.type = row.type.strip()
                    if row.alias_name is not None:
                        client_obj.alias_name = row.alias_name.strip()
                    if row.alias_phone_no is not None:
                        client_obj.alias_phone_no = row.alias_phone_no.strip()
                    if row.alias_addr is not None:
                        client_obj.alias_addr = row.alias_addr.strip()

                    if row.client_name is not None and client_obj.account_id:
                        single_acc = await db.get(SingleAccount, client_obj.account_id)
                        if single_acc:
                            single_acc.account_name = row.client_name[:50]
                            logger.debug(f"Updated SingleAccount name to '{row.client_name[:50]}'")

                    await db.flush()
                    logger.info(f"Client {row.client_id} updated successfully")

                    results.append(BulkClientResult(
                        row_index=row_index,
                        status="success",
                        detail="Updated successfully",
                        client_id=client_obj.client_id
                    ))
                    processed_count += 1

                except Exception as exc:
                    logger.error(f"Row {row_index} update failed: {str(exc)}", exc_info=True)
                    results.append(BulkClientResult(
                        row_index=row_index,
                        status="failed",
                        detail=f"DB error: {str(exc)}"
                    ))

        logger.info(f"Bulk update completed: {processed_count}/{total} rows processed")
        return BulkClientResponse(
            total_rows=total,
            processed_rows=processed_count,
            results=results
        )

    @staticmethod
    @log_function_call
    async def bulk_delete_clients(
        db: AsyncSession,
        client_ids: List[str]
    ) -> BulkClientResponse:
        """Bulk delete clients with partial success, also deleting associated single accounts."""
        logger.info(f"Starting bulk delete for {len(client_ids)} clients")
        total = len(client_ids)
        results: List[BulkClientResult] = []
        processed_count = 0

        for idx, cid in enumerate(client_ids):
            row_index = idx + 1
            logger.debug(f"Processing row {row_index} for client_id: {cid}")
            async with db.begin():
                try:
                    client_obj = await db.get(Client, cid)
                    if not client_obj:
                        detail = f"Client '{cid}' not found."
                        results.append(BulkClientResult(
                            row_index=row_index,
                            status="failed",
                            detail=detail
                        ))
                        logger.warning(f"Row {row_index} - {detail}")
                        continue

                    if client_obj.account_id:
                        single_acc = await db.get(SingleAccount, client_obj.account_id)
                        if single_acc:
                            await db.delete(single_acc)
                            logger.debug(f"Deleted SingleAccount with ID: {client_obj.account_id}")

                    await db.delete(client_obj)
                    await db.flush()
                    logger.info(f"Client {cid} deleted successfully")

                    results.append(BulkClientResult(
                        row_index=row_index,
                        status="success",
                        detail="Deleted successfully",
                        client_id=cid
                    ))
                    processed_count += 1

                except Exception as exc:
                    logger.error(f"Row {row_index} delete failed: {str(exc)}", exc_info=True)
                    results.append(BulkClientResult(
                        row_index=row_index,
                        status="failed",
                        detail=f"DB error: {str(exc)}"
                    ))

        logger.info(f"Bulk delete completed: {processed_count}/{total} rows processed")
        return BulkClientResponse(
            total_rows=total,
            processed_rows=processed_count,
            results=results
        )

    @staticmethod
    async def _get_broker_by_name(db: AsyncSession, broker_name: str) -> Optional[Broker]:
        """Helper method to fetch a broker by name (case-insensitive)."""
        if not broker_name:
            logger.debug("No broker_name provided for lookup")
            return None
        stmt = select(Broker).where(Broker.broker_name.ilike(broker_name.strip()))
        result = await db.execute(stmt)
        broker = result.scalars().first()
        logger.debug(f"Broker lookup for '{broker_name}': {'found' if broker else 'not found'}")
        return broker

    @staticmethod
    async def _get_distributor_id(db: AsyncSession, dist_name: str) -> Optional[str]:
        """Helper method to fetch a distributor ID by name (case-insensitive)."""
        if not dist_name:
            logger.debug("No distributor_name provided for lookup")
            return None
        stmt = select(Distributor).where(Distributor.name.ilike(dist_name.strip()))
        result = await db.execute(stmt)
        dist_obj = result.scalars().first()
        logger.debug(f"Distributor lookup for '{dist_name}': {'found' if dist_obj else 'not found'}")
        return dist_obj.distributor_id if dist_obj else None

================================================================================
# File: app/services/clients/__init__.py
================================================================================



================================================================================
# File: app/services/clients/distributors_service.py
================================================================================

from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select
from typing import List, Dict, Any
from app.models.clients.distributor_details import Distributor
from app.logger import logger, log_function_call

class DistributorService:
    @staticmethod
    @log_function_call
    async def get_distributor_id(db: AsyncSession, distributor_name: str) -> str:
        """Retrieve distributor ID by name (exact match)."""
        logger.info(f"Looking up distributor ID for name: '{distributor_name}'")
        result = await db.execute(select(Distributor).where(Distributor.name == distributor_name))
        distributor = result.scalars().first()
        if not distributor:
            logger.error(f"Distributor '{distributor_name}' not found")
            raise Exception(f"Distributor '{distributor_name}' not found.")
        logger.debug(f"Found distributor ID: {distributor.distributor_id}")
        return distributor.distributor_id

    @staticmethod
    @log_function_call
    async def get_distributors(db: AsyncSession) -> List[Dict[str, Any]]:
        """Fetch all distributors from the database, ordered by distributor_id."""
        logger.info("Fetching all distributors")
        q = await db.execute(select(Distributor).order_by(Distributor.distributor_id))
        rows = q.scalars().all()
        data = [
            {
                "distributor_id": r.distributor_id,
                "name": r.name,
                "created_at": r.created_at
            }
            for r in rows
        ]
        logger.debug(f"Retrieved {len(data)} distributors")
        return data

    @staticmethod
    @log_function_call
    async def add_distributor(db: AsyncSession, name: str) -> Dict[str, Any]:
        """Add a new distributor if it doesn't already exist."""
        logger.info(f"Attempting to add distributor: '{name}'")
        result = await db.execute(select(Distributor).where(Distributor.name.ilike(name)))
        if result.scalars().first():
            logger.warning(f"Distributor '{name}' already exists")
            raise Exception("Distributor already exists")
        new_distributor = Distributor(name=name)
        db.add(new_distributor)
        await db.commit()
        await db.refresh(new_distributor)
        logger.info(f"Distributor added with ID: {new_distributor.distributor_id}")
        return {
            "distributor_id": new_distributor.distributor_id,
            "name": new_distributor.name,
            "created_at": new_distributor.created_at
        }

    @staticmethod
    @log_function_call
    async def update_distributor(db: AsyncSession, old_value: str, new_value: str) -> Dict[str, Any]:
        """Update an existing distributor's name."""
        logger.info(f"Updating distributor from '{old_value}' to '{new_value}'")
        result = await db.execute(select(Distributor).where(Distributor.name.ilike(old_value)))
        distributor = result.scalars().first()
        if not distributor:
            logger.error(f"Distributor '{old_value}' not found")
            raise Exception("Distributor not found")
        distributor.name = new_value
        await db.commit()
        await db.refresh(distributor)
        logger.info(f"Distributor updated to '{new_value}' with ID: {distributor.distributor_id}")
        return {
            "distributor_id": distributor.distributor_id,
            "name": distributor.name,
            "created_at": distributor.created_at
        }

    @staticmethod
    @log_function_call
    async def delete_distributor(db: AsyncSession, name: str) -> Dict[str, Any]:
        """Delete a distributor by name."""
        logger.info(f"Deleting distributor: '{name}'")
        result = await db.execute(select(Distributor).where(Distributor.name.ilike(name)))
        distributor = result.scalars().first()
        if not distributor:
            logger.error(f"Distributor '{name}' not found")
            raise Exception("Distributor not found")
        await db.delete(distributor)
        await db.commit()
        logger.info(f"Distributor '{name}' deleted successfully")
        return {"status": "ok"}

================================================================================
# File: app/services/clients/brokers_service.py
================================================================================

from typing import List, Dict, Any
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, update
from app.models.clients.broker_details import Broker
from app.models.clients.client_details import Client
from app.logger import logger, log_function_call

class BrokerService:
    @staticmethod
    @log_function_call
    async def get_broker_id(db: AsyncSession, broker_name: str) -> str:
        """Retrieve broker ID by name (case-insensitive)."""
        logger.info(f"Looking up broker ID for broker_name: '{broker_name}'")
        result = await db.execute(select(Broker).where(Broker.broker_name.ilike(broker_name.strip())))
        broker = result.scalars().first()
        if not broker:
            logger.error(f"Broker '{broker_name}' not found")
            raise Exception(f"Broker '{broker_name}' not found.")
        logger.debug(f"Found broker ID: {broker.broker_id}")
        return broker.broker_id

    @staticmethod
    @log_function_call
    async def get_broker_name(db: AsyncSession, broker_id: int) -> str:
        """Retrieve broker name by ID (case-insensitive)."""
        logger.info(f"Looking up broker name for broker_id: {broker_id}")
        result = await db.execute(select(Broker).where(Broker.broker_id.ilike(str(broker_id))))
        broker = result.scalars().first()
        if not broker:
            logger.error(f"Broker '{broker_id}' not found")
            raise Exception(f"Broker '{broker_id}' not found.")
        logger.debug(f"Found broker name: {broker.broker_name}")
        return broker.broker_name

    @staticmethod
    @log_function_call
    async def get_brokers(db: AsyncSession) -> List[Dict[str, Any]]:
        """Fetch all brokers from the database, ordered by broker_id."""
        logger.info("Fetching all brokers")
        q = await db.execute(select(Broker).order_by(Broker.broker_id))
        rows = q.scalars().all()
        data = [
            {
                "broker_id": r.broker_id,
                "broker_name": r.broker_name,
                "created_at": r.created_at
            }
            for r in rows
        ]
        logger.debug(f"Retrieved {len(data)} brokers")
        return data

    @staticmethod
    @log_function_call
    async def add_broker(db: AsyncSession, broker_name: str) -> Dict[str, Any]:
        """Add a new broker if it doesn't already exist."""
        logger.info(f"Attempting to add broker: '{broker_name}'")
        result = await db.execute(select(Broker).where(Broker.broker_name.ilike(broker_name)))
        if result.scalars().first():
            logger.warning(f"Broker '{broker_name}' already exists")
            raise Exception("Broker already exists")
        new_broker = Broker(broker_name=broker_name)
        db.add(new_broker)
        await db.commit()
        await db.refresh(new_broker)
        logger.info(f"Broker added with ID: {new_broker.broker_id}")
        return {
            "broker_id": new_broker.broker_id,
            "broker_name": new_broker.broker_name,
            "created_at": new_broker.created_at
        }

    @staticmethod
    @log_function_call
    async def update_broker(db: AsyncSession, old_value: str, new_value: str) -> Dict[str, Any]:
        """Update an existing broker's name."""
        logger.info(f"Updating broker from '{old_value}' to '{new_value}'")
        result = await db.execute(select(Broker).where(Broker.broker_name.ilike(old_value)))
        broker = result.scalars().first()
        if not broker:
            logger.error(f"Broker '{old_value}' not found")
            raise Exception("Broker not found")
        broker.broker_name = new_value
        await db.commit()
        await db.refresh(broker)
        logger.info(f"Broker updated to '{new_value}' with ID: {broker.broker_id}")
        return {
            "broker_id": broker.broker_id,
            "broker_name": broker.broker_name,
            "created_at": broker.created_at
        }

    @staticmethod
    @log_function_call
    async def delete_broker(db: AsyncSession, broker_name: str) -> Dict[str, Any]:
        """Delete a broker and update clients to a default broker (BROKER_0003)."""
        logger.info(f"Deleting broker: '{broker_name}'")
        result = await db.execute(select(Broker).where(Broker.broker_name.ilike(broker_name)))
        broker = result.scalars().first()
        if not broker:
            logger.error(f"Broker '{broker_name}' not found")
            raise Exception("Broker not found")
        
        logger.debug(f"Reassigning clients from broker '{broker.broker_id}' to 'BROKER_0003'")
        await db.execute(
            update(Client)
            .where(Client.broker_id == broker.broker_id)
            .values(broker_id='BROKER_0003')
        )
        await db.delete(broker)
        await db.commit()
        logger.info(f"Broker '{broker_name}' deleted successfully")
        return {"status": "ok"}

================================================================================
# File: app/services/accounts/joint_account_service.py
================================================================================

from typing import Optional
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, delete
from app.models.clients.broker_details import Broker
from app.models.clients.client_details import Client
from app.models.accounts.joint_account import JointAccount
from app.models.accounts.single_account import SingleAccount
from app.models.accounts.joint_account_mapping import JointAccountMapping
from app.schemas.accounts.joint_account import (
    JointAccountCreateRequest, JointAccountResponse, JointAccountUpdateRequest
)
from app.logger import logger, log_function_call
from typing import List, Dict


class JointAccountService:
    @staticmethod
    @log_function_call
    async def create_joint_account(db: AsyncSession, payload: JointAccountCreateRequest) -> Optional[JointAccountResponse]:
        """Create a new joint account and map it to single accounts."""
        try:
            new_joint_account = JointAccount(
                joint_account_name=payload.joint_account_name,
                portfolio_id=1  # Default portfolio_id; adjust as needed
            )
            db.add(new_joint_account)
            await db.flush()

            for acc_id in payload.single_account_ids:
                query_single = select(SingleAccount).where(SingleAccount.single_account_id == acc_id)
                single_result = await db.execute(query_single)
                single_obj = single_result.scalar_one_or_none()

                if not single_obj:
                    logger.warning(f"SingleAccount '{acc_id}' not found. Skipping mapping.")
                    continue

                mapping_query = select(JointAccountMapping).where(JointAccountMapping.account_id == single_obj.single_account_id)
                existing_mapping_result = await db.execute(mapping_query)
                existing_mapping = existing_mapping_result.scalar_one_or_none()

                if existing_mapping:
                    logger.error(
                        f"SingleAccount '{acc_id}' is already mapped to a different JointAccount "
                        f"('{existing_mapping.joint_account_id}'). Aborting creation."
                    )
                    raise ValueError(f"SingleAccount '{acc_id}' already belongs to another JointAccount.")

                mapping = JointAccountMapping(
                    joint_account_id=new_joint_account.joint_account_id,
                    account_id=single_obj.single_account_id
                )
                db.add(mapping)

            await db.commit()
            return JointAccountResponse(
                status="success",
                joint_account_id=new_joint_account.joint_account_id,
                joint_account_name=new_joint_account.joint_account_name,
                linked_single_accounts=payload.single_account_ids
            )
        except Exception as e:
            logger.error(f"Error in create_joint_account: {e}", exc_info=True)
            await db.rollback()
            return None

    @staticmethod
    @log_function_call
    async def update_joint_account(
        db: AsyncSession,
        joint_account_id: str,
        payload: JointAccountUpdateRequest
    ) -> Optional[JointAccountResponse]:
        """Update an existing joint account's name and/or linked single accounts."""
        try:
            query_joint = select(JointAccount).where(JointAccount.joint_account_id == joint_account_id)
            result_joint = await db.execute(query_joint)
            existing_joint: JointAccount = result_joint.scalar_one_or_none()

            if not existing_joint:
                logger.warning(f"JointAccount '{joint_account_id}' not found.")
                return None

            if payload.joint_account_name is not None:
                existing_joint.joint_account_name = payload.joint_account_name.strip()

            if payload.single_account_ids is not None:
                del_stmt = delete(JointAccountMapping).where(JointAccountMapping.joint_account_id == joint_account_id)
                await db.execute(del_stmt)

                for acc_id in payload.single_account_ids:
                    query_single = select(SingleAccount).where(SingleAccount.single_account_id == acc_id)
                    single_result = await db.execute(query_single)
                    single_obj = single_result.scalar_one_or_none()

                    if not single_obj:
                        logger.warning(f"SingleAccount '{acc_id}' not found. Skipping mapping.")
                        continue

                    mapping_query = select(JointAccountMapping).where(JointAccountMapping.account_id == single_obj.single_account_id)
                    existing_mapping_result = await db.execute(mapping_query)
                    existing_mapping = existing_mapping_result.scalar_one_or_none()

                    if existing_mapping and existing_mapping.joint_account_id != joint_account_id:
                        logger.error(
                            f"SingleAccount '{acc_id}' is already mapped to another JointAccount "
                            f"('{existing_mapping.joint_account_id}'). Aborting update."
                        )
                        raise ValueError(f"SingleAccount '{acc_id}' belongs to another JointAccount.")

                    new_map = JointAccountMapping(
                        joint_account_id=joint_account_id,
                        account_id=acc_id
                    )
                    db.add(new_map)

            await db.commit()
            refresh_query = select(JointAccountMapping.account_id).where(JointAccountMapping.joint_account_id == joint_account_id)
            refresh_result = await db.execute(refresh_query)
            mapped_ids = [row[0] for row in refresh_result.all()]

            return JointAccountResponse(
                status="success",
                joint_account_id=existing_joint.joint_account_id,
                joint_account_name=existing_joint.joint_account_name,
                linked_single_accounts=mapped_ids
            )
        except Exception as e:
            logger.error(f"Error in update_joint_account: {e}", exc_info=True)
            await db.rollback()
            return None

    @staticmethod
    @log_function_call
    async def delete_joint_account(db: AsyncSession, joint_account_id: str) -> Optional[JointAccountResponse]:
        """Delete the specified joint account and its mappings."""
        try:
            query_joint = select(JointAccount).where(JointAccount.joint_account_id == joint_account_id)
            result_joint = await db.execute(query_joint)
            existing_joint = result_joint.scalar_one_or_none()

            if not existing_joint:
                logger.warning(f"JointAccount '{joint_account_id}' not found.")
                return None

            joint_name = existing_joint.joint_account_name
            await db.delete(existing_joint)
            await db.commit()
            return JointAccountResponse(
                status="success",
                joint_account_id=joint_account_id,
                joint_account_name=joint_name,
                linked_single_accounts=[]
            )
        except Exception as e:
            logger.error(f"Error in delete_joint_account: {e}", exc_info=True)
            await db.rollback()
            return None
        
    @staticmethod
    @log_function_call
    async def get_joint_accounts_with_single_accounts(db: AsyncSession) -> List[Dict]:
        """
        Retrieve all joint accounts with their associated single accounts and start dates.

        Args:
            db (AsyncSession): The database session.

        Returns:
            List[Dict]: List of dictionaries containing joint account details and their single accounts.
        """
        try:
            query = (
                select(
                    JointAccount.joint_account_id,
                    SingleAccount.single_account_id.label("account_id"),
                    Client.acc_start_date,
                    Client.broker_code,
                    Broker.broker_name
                )
                .select_from(JointAccount)
                .outerjoin(JointAccountMapping, JointAccount.joint_account_id == JointAccountMapping.joint_account_id)
                .outerjoin(SingleAccount, JointAccountMapping.account_id == SingleAccount.single_account_id)
                .outerjoin(Client, SingleAccount.single_account_id == Client.account_id)
                .outerjoin(Broker, Client.broker_id == Broker.broker_id)
            )
            result = await db.execute(query)
            rows = result.all()

            joint_accounts_dict = {}
            for row in rows:
                joint_id = row.joint_account_id
                if joint_id not in joint_accounts_dict:
                    joint_accounts_dict[joint_id] = {
                        "joint_account_id": joint_id,
                        "single_accounts": []
                    }
                if row.account_id:
                    joint_accounts_dict[joint_id]["single_accounts"].append({
                        "account_id": row.account_id,
                        "acc_start_date": row.acc_start_date,
                        "broker_code": row.broker_code,
                        "broker_name": row.broker_name,
                    })

            joint_accounts = list(joint_accounts_dict.values())
            logger.info(f"Fetched {len(joint_accounts)} joint accounts with single account mappings.")
            return joint_accounts
        except Exception as e:
            logger.error(f"Error fetching joint accounts with single accounts: {e}")
            return []
        
    @staticmethod
    @log_function_call
    async def get_linked_single_accounts(db: AsyncSession, joint_account_id: str) -> list:
        """Fetch details of single accounts linked to a joint account."""
        query = (
            select(
                SingleAccount.single_account_id.label("account_id"),
                Client.acc_start_date,
                Client.broker_code,
                Broker.broker_name
            )
            .join(JointAccountMapping, JointAccountMapping.account_id == SingleAccount.single_account_id)
            .join(Client, Client.account_id == SingleAccount.single_account_id)
            .join(Broker, Broker.broker_id == Client.broker_id)
            .where(JointAccountMapping.joint_account_id == joint_account_id)
        )
        result = await db.execute(query)
        rows = result.all()
        return [
            {
                'account_id': row.account_id,
                'acc_start_date': row.acc_start_date,
                'broker_code': row.broker_code,
                'broker_name': row.broker_name,
            }
            for row in rows
        ]

================================================================================
# File: app/services/accounts/account_service.py
================================================================================

from typing import List, Dict
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, func
from sqlalchemy.orm import selectinload
from app.logger import logger, log_function_call
from app.models.clients.broker_details import Broker
from app.models.clients.client_details import Client
from app.models.accounts.single_account import SingleAccount
from app.models.accounts.joint_account import JointAccount
from app.models.accounts.joint_account_mapping import JointAccountMapping
from app.models.portfolio.bracket_details import Bracket
from app.models.portfolio.portfolio_template_details import PortfolioTemplate
from app.models.accounts.account_performance import AccountPerformance
from app.schemas.accounts.account import (
    ViewAccount, BulkAccountResult, AccountUpdateRequest, BulkAccountResponse
)

class AccountService:
    @staticmethod
    @log_function_call
    async def get_all_accounts_view(db: AsyncSession) -> List[ViewAccount]:
        """Fetch and combine single and joint accounts into a unified view."""
        logger.info("Starting consolidated fetch of SingleAccount objects...")
        single_query = (
            select(SingleAccount)
            .options(
                selectinload(SingleAccount.bracket),
                selectinload(SingleAccount.portfolio_template),
                selectinload(SingleAccount.performance)
            )
        )
        single_result = await db.execute(single_query)
        single_accounts = single_result.scalars().all()
        logger.info(f"Retrieved {len(single_accounts)} single accounts.")

        logger.info("Starting consolidated fetch of JointAccount objects...")
        joint_query = (
            select(JointAccount)
            .options(
                selectinload(JointAccount.bracket),
                selectinload(JointAccount.portfolio_template),
                selectinload(JointAccount.performance)
            )
        )
        joint_result = await db.execute(joint_query)
        joint_accounts = joint_result.scalars().all()
        logger.info(f"Retrieved {len(joint_accounts)} joint accounts.")

        logger.info("Combining single and joint accounts into a unified view...")
        unified_view: List[ViewAccount] = []

        for acc in single_accounts:
            bracket_name = acc.bracket.bracket_name if acc.bracket else None
            portfolio_name = acc.portfolio_template.portfolio_name if acc.portfolio_template else None
            total_twrr = acc.performance.total_twrr if acc.performance else None
            current_yr_twrr = acc.performance.current_yr_twrr if acc.performance else None
            cagr = acc.performance.cagr if acc.performance else None

            unified_view.append(
                ViewAccount(
                    account_type="single",
                    account_id=acc.single_account_id,
                    account_name=acc.account_name,
                    bracket_name=bracket_name,
                    portfolio_name=portfolio_name,
                    pf_value=acc.pf_value,
                    cash_value=acc.cash_value,
                    total_holdings=acc.total_holdings,
                    invested_amt=acc.invested_amt,
                    total_twrr=total_twrr,
                    current_yr_twrr=current_yr_twrr,
                    cagr=cagr,
                    created_at=acc.created_at.isoformat() if acc.created_at else None,
                )
            )

        for acc in joint_accounts:
            bracket_name = acc.bracket.bracket_name if acc.bracket else None
            portfolio_name = acc.portfolio_template.portfolio_name if acc.portfolio_template else None
            total_twrr = acc.performance.total_twrr if acc.performance else None
            current_yr_twrr = acc.performance.current_yr_twrr if acc.performance else None
            cagr = acc.performance.cagr if acc.performance else None

            unified_view.append(
                ViewAccount(
                    account_type="joint",
                    account_id=acc.joint_account_id,
                    account_name=acc.joint_account_name,
                    bracket_name=bracket_name,
                    portfolio_name=portfolio_name,
                    pf_value=acc.pf_value,
                    cash_value=acc.cash_value,
                    total_holdings=acc.total_holdings,
                    invested_amt=acc.invested_amt,
                    total_twrr=total_twrr,
                    current_yr_twrr=current_yr_twrr,
                    cagr=cagr,
                    created_at=acc.created_at.isoformat() if acc.created_at else None,
                )
            )

        logger.info(f"Final unified view contains {len(unified_view)} accounts.")
        return unified_view

    @staticmethod
    async def bulk_update_accounts(db: AsyncSession, updates: List[AccountUpdateRequest]) -> BulkAccountResponse:
        """
        Bulk update single and joint accounts with partial success:
        - Single: Updates pf_value, cash_value, invested_amt, total_holdings (calculated), and performance fields.
                  Then recalculates linked joint accounts.
        - Joint: Updates only performance fields; ignores pf_value, cash_value, invested_amt.
        """
        total = len(updates)
        results: List[BulkAccountResult] = []
        processed_count = 0

        for idx, req in enumerate(updates):
            row_index = idx + 1
            async with db.begin():
                try:
                    if req.account_type == "single":
                        single_acc = await db.get(SingleAccount, req.account_id)
                        if not single_acc:
                            msg = f"Single account '{req.account_id}' not found."
                            results.append(BulkAccountResult(
                                row_index=row_index,
                                status="failed",
                                detail=msg
                            ))
                            logger.warning(f"Row {row_index} - {msg}")
                            continue

                        if req.pf_value is not None:
                            single_acc.pf_value = req.pf_value
                        if req.cash_value is not None:
                            single_acc.cash_value = req.cash_value
                        if req.invested_amt is not None:
                            single_acc.invested_amt = req.invested_amt

                        single_acc.total_holdings = (single_acc.pf_value or 0) + (single_acc.cash_value or 0)

                        if req.total_twrr is not None or req.current_yr_twrr is not None or req.cagr is not None:
                            perf = single_acc.performance
                            if not perf:
                                perf = await AccountService._create_performance(db, req.account_id, "single")
                                single_acc.performance = perf

                            if req.total_twrr is not None:
                                perf.total_twrr = req.total_twrr
                            if req.current_yr_twrr is not None:
                                perf.current_yr_twrr = req.current_yr_twrr
                            if req.cagr is not None:
                                perf.cagr = req.cagr

                        await db.flush()
                        await AccountService._recalc_all_linked_joints(db, single_acc.single_account_id)

                        results.append(BulkAccountResult(
                            row_index=row_index,
                            status="success",
                            detail="Updated single account successfully",
                            account_id=req.account_id
                        ))
                        processed_count += 1

                    elif req.account_type == "joint":
                        joint_acc = await db.get(JointAccount, req.account_id)
                        if not joint_acc:
                            msg = f"Joint account '{req.account_id}' not found."
                            results.append(BulkAccountResult(
                                row_index=row_index,
                                status="failed",
                                detail=msg
                            ))
                            logger.warning(f"Row {row_index} - {msg}")
                            continue

                        if req.total_twrr is not None or req.current_yr_twrr is not None or req.cagr is not None:
                            perf = joint_acc.performance
                            if not perf:
                                perf = await AccountService._create_performance(db, req.account_id, "joint")
                                joint_acc.performance = perf

                            if req.total_twrr is not None:
                                perf.total_twrr = req.total_twrr
                            if req.current_yr_twrr is not None:
                                perf.current_yr_twrr = req.current_yr_twrr
                            if req.cagr is not None:
                                perf.cagr = req.cagr

                        await db.flush()

                        results.append(BulkAccountResult(
                            row_index=row_index,
                            status="success",
                            detail="Updated joint account successfully",
                            account_id=req.account_id
                        ))
                        processed_count += 1

                    else:
                        msg = f"Invalid account_type '{req.account_type}'"
                        results.append(BulkAccountResult(
                            row_index=row_index,
                            status="failed",
                            detail=msg
                        ))
                        logger.warning(f"Row {row_index} - {msg}")

                except Exception as exc:
                    logger.error(f"Row {row_index} update failed: {exc}", exc_info=True)
                    results.append(BulkAccountResult(
                        row_index=row_index,
                        status="failed",
                        detail=str(exc)
                    ))

        return BulkAccountResponse(
            total_rows=total,
            processed_rows=processed_count,
            results=results
        )

    @staticmethod
    async def _recalc_all_linked_joints(db: AsyncSession, single_id: str):
        """Recalculate joint account values based on linked single accounts."""
        stmt_joints = select(JointAccountMapping.joint_account_id).where(
            JointAccountMapping.account_id == single_id
        )
        res_joints = await db.execute(stmt_joints)
        joint_ids = [r[0] for r in res_joints.fetchall()]

        for j_id in joint_ids:
            sum_stmt = (
                select(
                    func.coalesce(func.sum(SingleAccount.pf_value), 0),
                    func.coalesce(func.sum(SingleAccount.cash_value), 0),
                    func.coalesce(func.sum(SingleAccount.invested_amt), 0),
                )
                .join(JointAccountMapping, SingleAccount.single_account_id == JointAccountMapping.account_id)
                .where(JointAccountMapping.joint_account_id == j_id)
            )
            sum_res = await db.execute(sum_stmt)
            pf_sum, cash_sum, invest_sum = sum_res.fetchone()
            total_sum = pf_sum + cash_sum

            joint_acc = await db.get(JointAccount, j_id)
            if joint_acc:
                joint_acc.pf_value = pf_sum
                joint_acc.cash_value = cash_sum
                joint_acc.invested_amt = invest_sum
                joint_acc.total_holdings = total_sum
                await db.flush()

    @staticmethod
    async def _create_performance(db: AsyncSession, owner_id: str, owner_type: str) -> AccountPerformance:
        """Create a new AccountPerformance record if none exists."""
        perf_id = f"PERF_{owner_id}"
        new_perf = AccountPerformance(
            performance_id=perf_id,
            owner_id=owner_id,
            owner_type=owner_type,
            total_twrr=0,
            current_yr_twrr=0,
            cagr=0
        )
        db.add(new_perf)
        await db.flush()
        return new_perf
    
    @staticmethod
    @log_function_call
    async def get_single_accounts_with_broker_info(db: AsyncSession):
        """
        Retrieve single accounts with their associated broker codes and names.

        This function joins SingleAccount, Client, and Broker tables to fetch account_id,
        broker_code, and broker_name for accounts with a non-null broker_code.

        Args:
            db (AsyncSession): The database session for executing the query.

        Returns:
            list[dict]: A list of dictionaries containing 'account_id', 'broker_code', and 'broker_name'.
        """
        query = (
            select(
                SingleAccount.single_account_id.label("account_id"),
                Client.broker_code,
                Broker.broker_name,
                Client.acc_start_date
            )
            .join(Client, SingleAccount.single_account_id == Client.account_id)
            .join(Broker, Client.broker_id == Broker.broker_id)
            .where(Client.broker_code.isnot(None))
        )

        result = await db.execute(query)
        rows = result.all()

        return [
            {
                "account_id": row.account_id,
                "broker_code": row.broker_code,
                "broker_name": row.broker_name,
                "acc_start_date": row.acc_start_date 
            }
            for row in rows
        ]

================================================================================
# File: app/services/accounts/accounts_data_service.py
================================================================================

from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select
from app.models.accounts.account_time_periods import AccountTimePeriods
from app.models.accounts.account_cashflow_progression import AccountCashflowProgression
from typing import List, Optional
from app.logger import logger, log_function_call 

class AccountTimePeriodsService:
    @staticmethod
    @log_function_call
    async def get_time_periods_by_owner_id(
        db: AsyncSession,
        owner_id: str,
        owner_type: Optional[str] = None
    ) -> List[AccountTimePeriods]:
        query = select(AccountTimePeriods).where(AccountTimePeriods.owner_id == owner_id)
        
        if owner_type:
            query = query.where(AccountTimePeriods.owner_type == owner_type)
        
        result = await db.execute(query)
        return result.scalars().all()
    
class AccountCashflowProgressionService:
    @staticmethod
    async def get_cashflow_progression_by_owner_id(
        db: AsyncSession,
        owner_id: str,
        owner_type: Optional[str] = None
    ) -> List[AccountCashflowProgression]:
        """
        Retrieve cashflow progression records for a given owner_id and optional owner_type.
        
        Args:
            db (AsyncSession): The asynchronous database session.
            owner_id (str): The ID of the account owner.
            owner_type (Optional[str]): The type of owner ("single" or "joint"), if specified.
        
        Returns:
            List[AccountCashflowProgression]: A list of matching cashflow progression records.
        """
        query = select(AccountCashflowProgression).where(AccountCashflowProgression.owner_id == owner_id)
        
        if owner_type:
            query = query.where(AccountCashflowProgression.owner_type == owner_type)
        
        result = await db.execute(query)
        return result.scalars().all()

================================================================================
# File: app/services/accounts/__init__.py
================================================================================



================================================================================
# File: app/frontend/__init__.py
================================================================================



================================================================================
# File: app/telegram/reports.py
================================================================================

from telegram import Update
from telegram.ext import ApplicationBuilder, CommandHandler, MessageHandler, filters, ConversationHandler, ContextTypes
import asyncio

BROKER, PAN = range(2)
TELEGRAM_BOT_TOKEN = "7716843660:AAGjNSCz69hwgIrLHV23kt1Dnatt1CnghKI"

user_inputs = {}

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text("👋 Welcome to Plus91 Reports Bot!\n\nPlease enter your Broker Code:")
    return BROKER

async def broker_code(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_id = update.message.from_user.id
    user_inputs[user_id] = {}
    user_inputs[user_id]['broker_code'] = update.message.text
    await update.message.reply_text("Got it!\n\nNow please enter your PAN No:")
    return PAN

async def pan_no(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_id = update.message.from_user.id
    user_inputs[user_id]['pan_no'] = update.message.text

    broker = user_inputs[user_id]['broker_code']
    pan = user_inputs[user_id]['pan_no']

    await update.message.reply_text(
        f"Thanks! We have received your details:\n\n"
        f"Broker Code: {broker}\n"
        f"PAN No: {pan}\n\n"
        f"Our system will now fetch and send your latest reports."
    )
    return ConversationHandler.END

async def cancel(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text("Process cancelled. Type /start to begin again.")
    return ConversationHandler.END

async def main():
    application = ApplicationBuilder().token(TELEGRAM_BOT_TOKEN).build()
    conv_handler = ConversationHandler(
        entry_points=[CommandHandler('start', start)],
        states={
            BROKER: [MessageHandler(filters.TEXT & ~filters.COMMAND, broker_code)],
            PAN: [MessageHandler(filters.TEXT & ~filters.COMMAND, pan_no)],
        },
        fallbacks=[CommandHandler('cancel', cancel)],
    )

    application.add_handler(conv_handler)

    print("🤖 Bot is running...")
    await application.run_polling()

if __name__ == '__main__':
    asyncio.run(main())


================================================================================
# File: app/telegram/main.py
================================================================================

import boto3
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession
from typing import List, Dict, Any, Union
import os
from app.database import get_db
from app.services.report_service import ReportService
from app.models.report import RequestData
from app.models.accounts.joint_account_mapping import JointAccountMapping
from app.models.clients.client_details import Client

async def download_reports(
    broker_code: str,
    pan_no: str,
    db: AsyncSession,
    download_dir: str = "./individual_reports"
) -> Dict[str, Union[str, List[str]]]:
    """
    Downloads client reports (single and joint accounts) to a local directory.
    
    Args:
        broker_code: Client's broker code
        pan_no: Client's PAN number
        db: Async database session
        download_dir: Directory to save reports (default: ./reports)
    
    Returns:
        Dictionary with status and list of downloaded file paths
    """
    service = ReportService(db, boto3.client('s3'))
    os.makedirs(download_dir, exist_ok=True)
    downloaded_files = []

    try:
     
        client = await service.verify_user(broker_code, pan_no)
        accounts = await service.get_accounts(client)
        
        
        single_report = service.get_latest_report([client.broker_code], is_joint=False)
        if single_report:
            single_filename = f"{download_dir}/{client.broker_code}_report.pdf"
            with open(single_filename, "wb") as f:
                f.write(single_report)
            downloaded_files.append(single_filename)

   
        for joint_account in accounts["joint"]:
            stmt_mappings = select(JointAccountMapping).where(
                JointAccountMapping.joint_account_id == joint_account.joint_account_id
            )
            result_mappings = await db.execute(stmt_mappings)
            joint_mappings = result_mappings.scalars().all()
            joint_single_account_ids = [m.account_id for m in joint_mappings]
            
            stmt_clients = select(Client).where(
                Client.account_id.in_(joint_single_account_ids)
            )
            result_clients = await db.execute(stmt_clients)
            joint_clients = result_clients.scalars().all()
            joint_broker_codes = [c.broker_code for c in joint_clients]
            
            joint_report = service.get_latest_report(joint_broker_codes, is_joint=True)
            if joint_report:
                joint_filename = f"{download_dir}/JointAccount_{joint_account.joint_account_id}_report.pdf"
                with open(joint_filename, "wb") as f:
                    f.write(joint_report)
                downloaded_files.append(joint_filename)

        if downloaded_files:
            return {
                "status": "success",
                "message": "Reports downloaded successfully",
                "downloaded_files": downloaded_files
            }
        else:
            return {
                "status": "error",
                "message": "No reports found for download",
                "downloaded_files": []
            }

    except ValueError as e:
        return {
            "status": "error",
            "message": str(e),
            "downloaded_files": []
        }
    except Exception as e:
        return {
            "status": "error",
            "message": f"Internal server error: {str(e)}",
            "downloaded_files": []
        }

================================================================================
# File: alembic/env.py
================================================================================

# alembic/env.py
from logging.config import fileConfig
from sqlalchemy import engine_from_config, pool
from alembic import context
import os
import sys

# Add the parent directory to sys.path to access the 'app' module
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from app.models.base import Base  # Import Base with all models

# This is the Alembic Config object, which provides access to the values within the .ini file in use.
config = context.config

# Interpret the config file for Python logging.
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# Set target_metadata to your models' metadata
target_metadata = Base.metadata

def run_migrations_offline() -> None:
    """Run migrations in 'offline' mode."""
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()

def run_migrations_online() -> None:
    """Run migrations in 'online' mode."""
    connectable = engine_from_config(
        config.get_section(config.config_ini_section, {}),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection,
            target_metadata=target_metadata
        )

        with context.begin_transaction():
            context.run_migrations()

if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()

================================================================================
# File: alembic/versions/4971cd171d3d_initial_migration.py
================================================================================

"""Initial migration

Revision ID: 4971cd171d3d
Revises: 
Create Date: 2025-02-26 02:57:52.198268

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = '4971cd171d3d'
down_revision: Union[str, None] = None
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('ltp_data',
    sa.Column('trading_symbol', sa.String(), nullable=False),
    sa.Column('ltp', sa.Float(), nullable=True),
    sa.Column('last_updated', sa.DateTime(), nullable=True),
    sa.PrimaryKeyConstraint('trading_symbol')
    )
    op.create_table('stock_exceptions',
    sa.Column('owner_id', sa.String(), nullable=False),
    sa.Column('owner_type', sa.String(), nullable=False),
    sa.Column('trading_symbol', sa.String(), nullable=False),
    sa.PrimaryKeyConstraint('owner_id', 'owner_type', 'trading_symbol')
    )
    op.create_table('account_bracket_basket_allocation',
    sa.Column('owner_id', sa.String(), nullable=False),
    sa.Column('owner_type', sa.String(), nullable=False),
    sa.Column('basket_id', sa.Integer(), nullable=False),
    sa.Column('allocation_pct', sa.Float(), nullable=True),
    sa.ForeignKeyConstraint(['basket_id'], ['basket_details.basket_id'], ),
    sa.PrimaryKeyConstraint('owner_id', 'owner_type', 'basket_id')
    )
    op.drop_column('account_actual_portfolio', 'month_end_date')
    op.alter_column('account_ideal_portfolio', 'trading_symbol',
               existing_type=sa.VARCHAR(),
               type_=sa.Text(),
               existing_nullable=False)
    op.alter_column('account_ideal_portfolio', 'basket',
               existing_type=sa.VARCHAR(),
               type_=sa.Text(),
               existing_nullable=False)
    op.alter_column('client_details', 'acc_start_date',
               existing_type=sa.TEXT(),
               type_=sa.String(),
               existing_nullable=True)
    op.drop_column('client_details', 'is_distributor')
    op.alter_column('joint_account', 'account_type',
               existing_type=sa.TEXT(),
               type_=sa.String(),
               existing_nullable=False,
               existing_server_default=sa.text("'joint'::text"))
    op.alter_column('pf_bracket_basket_allocation', 'bracket_id',
               existing_type=sa.INTEGER(),
               nullable=True)
    op.alter_column('pf_bracket_basket_allocation', 'basket_id',
               existing_type=sa.INTEGER(),
               nullable=True)
    op.alter_column('pf_bracket_basket_allocation', 'portfolio_id',
               existing_type=sa.INTEGER(),
               nullable=True)
    op.drop_constraint('portfolio_bracket_basket_allocation_bracket_id_fkey', 'pf_bracket_basket_allocation', type_='foreignkey')
    op.drop_constraint('portfolio_bracket_basket_allocation_basket_id_fkey', 'pf_bracket_basket_allocation', type_='foreignkey')
    op.drop_constraint('portfolio_bracket_basket_allocation_portfolio_id_fkey', 'pf_bracket_basket_allocation', type_='foreignkey')
    op.create_foreign_key(None, 'pf_bracket_basket_allocation', 'basket_details', ['basket_id'], ['basket_id'])
    op.create_foreign_key(None, 'pf_bracket_basket_allocation', 'portfolio_template_details', ['portfolio_id'], ['portfolio_id'])
    op.create_foreign_key(None, 'pf_bracket_basket_allocation', 'bracket_details', ['bracket_id'], ['bracket_id'])
    op.alter_column('portfolio_basket_mapping', 'allocation_pct',
               existing_type=sa.DOUBLE_PRECISION(precision=53),
               nullable=True)
    op.alter_column('single_account', 'account_type',
               existing_type=sa.TEXT(),
               type_=sa.String(),
               existing_nullable=False,
               existing_server_default=sa.text("'single'::text"))
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.alter_column('single_account', 'account_type',
               existing_type=sa.String(),
               type_=sa.TEXT(),
               existing_nullable=False,
               existing_server_default=sa.text("'single'::text"))
    op.alter_column('portfolio_basket_mapping', 'allocation_pct',
               existing_type=sa.DOUBLE_PRECISION(precision=53),
               nullable=False)
    op.drop_constraint(None, 'pf_bracket_basket_allocation', type_='foreignkey')
    op.drop_constraint(None, 'pf_bracket_basket_allocation', type_='foreignkey')
    op.drop_constraint(None, 'pf_bracket_basket_allocation', type_='foreignkey')
    op.create_foreign_key('portfolio_bracket_basket_allocation_portfolio_id_fkey', 'pf_bracket_basket_allocation', 'portfolio_template_details', ['portfolio_id'], ['portfolio_id'], ondelete='CASCADE')
    op.create_foreign_key('portfolio_bracket_basket_allocation_basket_id_fkey', 'pf_bracket_basket_allocation', 'basket_details', ['basket_id'], ['basket_id'], ondelete='CASCADE')
    op.create_foreign_key('portfolio_bracket_basket_allocation_bracket_id_fkey', 'pf_bracket_basket_allocation', 'bracket_details', ['bracket_id'], ['bracket_id'], ondelete='CASCADE')
    op.alter_column('pf_bracket_basket_allocation', 'portfolio_id',
               existing_type=sa.INTEGER(),
               nullable=False)
    op.alter_column('pf_bracket_basket_allocation', 'basket_id',
               existing_type=sa.INTEGER(),
               nullable=False)
    op.alter_column('pf_bracket_basket_allocation', 'bracket_id',
               existing_type=sa.INTEGER(),
               nullable=False)
    op.alter_column('joint_account', 'account_type',
               existing_type=sa.String(),
               type_=sa.TEXT(),
               existing_nullable=False,
               existing_server_default=sa.text("'joint'::text"))
    op.add_column('client_details', sa.Column('is_distributor', sa.BOOLEAN(), server_default=sa.text('false'), autoincrement=False, nullable=True))
    op.alter_column('client_details', 'acc_start_date',
               existing_type=sa.String(),
               type_=sa.TEXT(),
               existing_nullable=True)
    op.alter_column('account_ideal_portfolio', 'basket',
               existing_type=sa.Text(),
               type_=sa.VARCHAR(),
               existing_nullable=False)
    op.alter_column('account_ideal_portfolio', 'trading_symbol',
               existing_type=sa.Text(),
               type_=sa.VARCHAR(),
               existing_nullable=False)
    op.add_column('account_actual_portfolio', sa.Column('month_end_date', postgresql.TIMESTAMP(), server_default=sa.text('CURRENT_TIMESTAMP'), autoincrement=False, nullable=False))
    op.drop_table('account_bracket_basket_allocation')
    op.drop_table('stock_exceptions')
    op.drop_table('ltp_data')
    # ### end Alembic commands ###


================================================================================
# File: data/directory_structure.py
================================================================================

import os
import datetime

def print_directory_tree(start_path: str, output_file: str = None, exclude: list = None):
    """
    Print or write the directory structure starting from the given directory in a tree-like format.
    
    Args:
        start_path: The root directory to start from.
        output_file: Optional; if provided, write the output to this file instead of printing to console.
        exclude: Optional; list of directory names to exclude from the traversal.
    """
    # Validate the starting path
    if not os.path.exists(start_path):
        print(f"Error: Path '{start_path}' does not exist.")
        return
    if not os.path.isdir(start_path):
        print(f"Error: '{start_path}' is not a directory.")
        return
    
    # Set default exclude list if none provided
    if exclude is None:
        exclude = ['__pycache__', 'venv', '.venv', '.git']
    
    # Helper function to recursively print the directory tree
    def _print_tree(path, prefix, file):
        try:
            # Get directory contents, excluding specified directories
            contents = [entry for entry in os.scandir(path) if entry.name not in exclude]
        except Exception as e:
            print(f"{prefix}└── [Error: {str(e)}]", file=file)
            return
        
        # Sort contents alphabetically for consistent output
        contents.sort(key=lambda e: e.name)
        
        # Process each entry in the directory
        for i, entry in enumerate(contents):
            # Determine if this is the last item to choose the connector
            is_last = i == len(contents) - 1
            connector = '└── ' if is_last else '├── '
            
            # Print the current entry
            print(f"{prefix}{connector}{entry.name}", file=file)
            
            # If it's a directory, recurse into it with updated prefix
            if entry.is_dir():
                new_prefix = prefix + ('    ' if is_last else '│   ')
                _print_tree(entry.path, new_prefix, file)
    
    # Determine output method and write header
    header = (
        f"# Directory Structure\n"
        f"# Generated on: {datetime.datetime.now()}\n"
        f"# Source directory: {os.path.abspath(start_path)}\n\n"
    )
    
    if output_file:
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(header)
                _print_tree(start_path, '', f)
            print(f"Directory structure written to '{output_file}'")
        except Exception as e:
            print(f"Error writing to '{output_file}': {str(e)}")
    else:
        print(header.rstrip())  # Remove trailing newline for console output
        _print_tree(start_path, '', None)

if __name__ == "__main__":
    import sys
    if len(sys.argv) < 2:
        print("Usage: python script.py <directory> [output_file]")
        sys.exit(1)
    
    start_path = sys.argv[1]
    output_file = sys.argv[2] if len(sys.argv) > 2 else None
    print_directory_tree(start_path, output_file)

================================================================================
# File: test/queries.py
================================================================================

# app/scripts/fetch_accounts.py

import asyncio
from sqlalchemy.ext.asyncio import AsyncSession
from app.database import AsyncSessionLocal
from app.services.accounts.account_service import AccountService

async def run_fetch_accounts():
    """
    Execute the get_single_accounts_with_broker_codes function and print results.
    """
    async with AsyncSessionLocal() as db:
        try:
            accounts_data = await AccountService.get_single_accounts_with_broker_info(db)
            for account in accounts_data:
                print(f"Account ID: {account['account_id']}, Broker Code: {account['broker_code']}, Broker Name: {account['broker_name']}, Start Date: {account['acc_start_date']}")
                print(f"Account ID: {account['account_id']}, Broker Code: {account['broker_code']}, Broker Name: {account['broker_name']}, Start Date: {type(account['acc_start_date'])}")
            print(f"Total accounts retrieved: {len(accounts_data)}")
        except Exception as e:
            print(f"Error occurred: {str(e)}")

if __name__ == "__main__":
    asyncio.run(run_fetch_accounts())

================================================================================
# File: test/__init__.py
================================================================================



================================================================================
# Summary: Processed 106 Python files
# End of combined codebase
